{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjPTaRB4mpCd"
   },
   "source": [
    "# Colab FAQ\n",
    "\n",
    "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
    "\n",
    "You need to use the colab GPU for this assignment by selecting:\n",
    "\n",
    "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9IS9B9-yUU5"
   },
   "source": [
    "# Setup PyTorch\n",
    "\n",
    "All files will be stored at /content/csc421/a3/ folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-6MQhMOlHXD"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Setup python environment and change the current working directory\n",
    "######################################################################\n",
    "!pip install Pillow\n",
    "%mkdir -p ./content/csc421/a3/\n",
    "%cd ./content/csc421/a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DaTdRNuUra7"
   },
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BIpGwANoQOg"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "D-UJHBYZkh7f"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_file(\n",
    "    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n",
    "):\n",
    "    datadir = os.path.join(cache_dir)\n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "\n",
    "    if untar:\n",
    "        untar_fpath = os.path.join(datadir, fname)\n",
    "        fpath = untar_fpath + \".tar.gz\"\n",
    "    else:\n",
    "        fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    print(fpath)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(\"Downloading data from\", origin)\n",
    "\n",
    "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    if untar:\n",
    "        if not os.path.exists(untar_fpath):\n",
    "            print(\"Extracting file.\")\n",
    "            with tarfile.open(fpath) as archive:\n",
    "                archive.extractall(datadir)\n",
    "        return untar_fpath\n",
    "\n",
    "    if extract:\n",
    "        _extract_archive(fpath, datadir, archive_format)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "def to_var(tensor, cuda):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "    Arguments:\n",
    "        tensor: A Tensor object.\n",
    "        cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "    Returns:\n",
    "        A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "def create_dir_if_not_exists(directory):\n",
    "    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save_loss_plot(train_losses, val_losses, opts):\n",
    "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(val_losses)), val_losses)\n",
    "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
    "    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
    "    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
    "    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
    "    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
    "\n",
    "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
    "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
    "    ax[0].legend(loc=\"upper right\")\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    plt.legend()\n",
    "\n",
    "    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n",
    "    plt.savefig(plt_path)\n",
    "    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n",
    "\n",
    "\n",
    "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
    "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
    "\n",
    "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        l3: Tuple of lists containing training / val losses for model 3.\n",
    "        l4: Tuple of lists containing training / val losses for model 4.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        o3: Options for model 3.\n",
    "        o4: Options for model 4.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
    "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
    "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
    "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
    "    ax[0][0].title.set_text(\n",
    "        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n",
    "    )\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
    "    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
    "    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n",
    "\n",
    "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n",
    "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n",
    "    ax[1][0].title.set_text(\n",
    "        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n",
    "    )\n",
    "\n",
    "    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n",
    "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n",
    "    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][0].legend(loc=\"upper right\")\n",
    "        ax[i][1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
    "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
    "\n",
    "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        l3: Tuple of lists containing training / val losses for model 3.\n",
    "        l4: Tuple of lists containing training / val losses for model 4.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        o3: Options for model 3.\n",
    "        o4: Options for model 4.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
    "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
    "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n",
    "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n",
    "    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n",
    "    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n",
    "    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n",
    "\n",
    "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n",
    "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n",
    "    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n",
    "\n",
    "    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n",
    "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n",
    "    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][0].legend(loc=\"upper right\")\n",
    "        ax[i][1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def checkpoint(encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
    "    contains the char_to_index and index_to_char mappings, and the start_token\n",
    "    and end_token values.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n",
    "        torch.save(encoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
    "        torch.save(decoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
    "        pkl.dump(idx_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbvpn4MaV0I1"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "XVT4TNTOV3Eg"
   },
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    \"\"\"Read a file and split it into lines.\"\"\"\n",
    "    lines = open(filename).read().strip().lower().split(\"\\n\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def read_pairs(filename):\n",
    "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
    "\n",
    "    Returns:\n",
    "        source_words: A list of the first word in each line of the file.\n",
    "        target_words: A list of the second word in each line of the file.\n",
    "    \"\"\"\n",
    "    lines = read_lines(filename)\n",
    "    source_words, target_words = [], []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            source, target = line.split()\n",
    "            source_words.append(source)\n",
    "            target_words.append(target)\n",
    "    return source_words, target_words\n",
    "\n",
    "\n",
    "def all_alpha_or_dash(s):\n",
    "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n",
    "    return all(c.isalpha() or c == \"-\" for c in s)\n",
    "\n",
    "\n",
    "def filter_lines(lines):\n",
    "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n",
    "    return [line for line in lines if all_alpha_or_dash(line)]\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
    "    path = \"./data/{}.txt\".format(file_name)\n",
    "    source_lines, target_lines = read_pairs(path)\n",
    "\n",
    "    # Filter lines\n",
    "    source_lines = filter_lines(source_lines)\n",
    "    target_lines = filter_lines(target_lines)\n",
    "\n",
    "    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n",
    "\n",
    "    # Create a dictionary mapping each character to a unique index\n",
    "    char_to_index = {\n",
    "        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n",
    "    }\n",
    "\n",
    "    # Add start and end tokens to the dictionary\n",
    "    start_token = len(char_to_index)\n",
    "    end_token = len(char_to_index) + 1\n",
    "    char_to_index[\"SOS\"] = start_token\n",
    "    char_to_index[\"EOS\"] = end_token\n",
    "\n",
    "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
    "    index_to_char = {index: char for (char, index) in char_to_index.items()}\n",
    "\n",
    "    # Store the final size of the vocabulary\n",
    "    vocab_size = len(char_to_index)\n",
    "\n",
    "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
    "\n",
    "    idx_dict = {\n",
    "        \"char_to_index\": char_to_index,\n",
    "        \"index_to_char\": index_to_char,\n",
    "        \"start_token\": start_token,\n",
    "        \"end_token\": end_token,\n",
    "    }\n",
    "\n",
    "    return line_pairs, vocab_size, idx_dict\n",
    "\n",
    "\n",
    "def create_dict(pairs):\n",
    "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
    "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
    "    all source indexes and the other containing all corresponding target indexes.\n",
    "    Within a batch, all the source words are the same length, and all the target words are\n",
    "    the same length.\n",
    "    \"\"\"\n",
    "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
    "\n",
    "    d = defaultdict(list)\n",
    "    for (s, t) in unique_pairs:\n",
    "        d[(len(s), len(t))].append((s, t))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRWfRdmVVjUl"
   },
   "source": [
    "## Training and evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "wa5-onJhoSeM"
   },
   "outputs": [],
   "source": [
    "def string_to_index_list(s, char_to_index, end_token):\n",
    "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
    "    return [char_to_index[char] for char in s] + [\n",
    "        end_token\n",
    "    ]  # Adds the end token to each index list\n",
    "\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
    "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
    "    word independently, and then stitching the words back together with spaces between them.\n",
    "    \"\"\"\n",
    "    if idx_dict is None:\n",
    "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
    "    return \" \".join(\n",
    "        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n",
    "    )\n",
    "\n",
    "\n",
    "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
    "\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "    index_to_char = idx_dict[\"index_to_char\"]\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = \"\"\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(\n",
    "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
    "    )  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_last_hidden\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "        ## slow decoding, recompute everything at each time\n",
    "        decoder_outputs, attention_weights = decoder(\n",
    "            decoder_inputs, encoder_annotations, decoder_hidden\n",
    "        )\n",
    "\n",
    "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "        ni = ni[-1]  # latest output token\n",
    "\n",
    "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "\n",
    "        if ni == end_token:\n",
    "            break\n",
    "        else:\n",
    "            gen_string = \"\".join(\n",
    "                [\n",
    "                    index_to_char[int(item)]\n",
    "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
    "    if idx_dict is None:\n",
    "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "    index_to_char = idx_dict[\"index_to_char\"]\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = \"\"\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(\n",
    "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
    "    )  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    produced_end_token = False\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "        ## slow decoding, recompute everything at each time\n",
    "        decoder_outputs, attention_weights = decoder(\n",
    "            decoder_inputs, encoder_annotations, decoder_hidden\n",
    "        )\n",
    "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "        ni = ni[-1]  # latest output token\n",
    "\n",
    "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "\n",
    "        if ni == end_token:\n",
    "            break\n",
    "        else:\n",
    "            gen_string = \"\".join(\n",
    "                [\n",
    "                    index_to_char[int(item)]\n",
    "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    if isinstance(attention_weights, tuple):\n",
    "        ## transformer's attention mweights\n",
    "        attention_weights, self_attention_weights = attention_weights\n",
    "\n",
    "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
    "\n",
    "    for i in range(len(all_attention_weights)):\n",
    "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
    "        fig.colorbar(cax)\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
    "        ax.set_xticklabels(\n",
    "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
    "        )\n",
    "\n",
    "        # Show label at every tick\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        # Add title\n",
    "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
    "        plt.tight_layout()\n",
    "        plt.grid(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
    "    \"\"\"Train/Evaluate the model on a dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
    "        opts: The command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        mean_loss: The average loss over all batches from data_dict.\n",
    "    \"\"\"\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "\n",
    "    losses = []\n",
    "    for key in data_dict:\n",
    "        input_strings, target_strings = zip(*data_dict[key])\n",
    "        input_tensors = [\n",
    "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
    "            for s in input_strings\n",
    "        ]\n",
    "        target_tensors = [\n",
    "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
    "            for s in target_strings\n",
    "        ]\n",
    "\n",
    "        num_tensors = len(input_tensors)\n",
    "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start = i * opts.batch_size\n",
    "            end = start + opts.batch_size\n",
    "\n",
    "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
    "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
    "\n",
    "            # The batch size may be different in each epoch\n",
    "            BS = inputs.size(0)\n",
    "\n",
    "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
    "\n",
    "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            start_vector = (\n",
    "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
    "            )  # BS x 1 --> 16x1  CHECKED\n",
    "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
    "\n",
    "            decoder_inputs = torch.cat(\n",
    "                [decoder_input, targets[:, 0:-1]], dim=1\n",
    "            )  # Gets decoder inputs by shifting the targets to the right\n",
    "\n",
    "            decoder_outputs, attention_weights = decoder(\n",
    "                decoder_inputs, encoder_annotations, decoder_hidden\n",
    "            )\n",
    "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
    "            targets_flatten = targets.view(-1)\n",
    "\n",
    "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            ## training if an optimizer is provided\n",
    "            if optimizer:\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                # Update the parameters of the encoder and decoder\n",
    "                optimizer.step()\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
    "):\n",
    "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
    "        * Prints training and val loss each epoch.\n",
    "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
    "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
    "        * Returns loss curves for comparison\n",
    "\n",
    "    Arguments:\n",
    "        train_dict: The training word pairs, organized by source and target lengths.\n",
    "        val_dict: The validation word pairs, organized by source and target lengths.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
    "        opts: The command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        losses: Lists containing training and validation loss curves.\n",
    "    \"\"\"\n",
    "\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "\n",
    "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
    "\n",
    "    best_val_loss = 1e6\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    mean_train_losses = []\n",
    "    mean_val_losses = []\n",
    "\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(opts.nepochs):\n",
    "\n",
    "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
    "\n",
    "        train_loss = compute_loss(\n",
    "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
    "        )\n",
    "        val_loss = compute_loss(\n",
    "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
    "        )\n",
    "\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            checkpoint(encoder, decoder, idx_dict, opts)\n",
    "            best_val_loss = mean_val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > opts.early_stopping_patience:\n",
    "            print(\n",
    "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
    "                    opts.early_stopping_patience\n",
    "                )\n",
    "            )\n",
    "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
    "            return (train_losses, mean_val_losses)\n",
    "\n",
    "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
    "        print(\n",
    "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
    "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
    "            )\n",
    "        )\n",
    "\n",
    "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
    "        loss_log.flush()\n",
    "\n",
    "        train_losses += train_loss\n",
    "        val_losses += val_loss\n",
    "\n",
    "        mean_train_losses.append(mean_train_loss)\n",
    "        mean_val_losses.append(mean_val_loss)\n",
    "\n",
    "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
    "\n",
    "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
    "    return (train_losses, mean_val_losses)\n",
    "\n",
    "\n",
    "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
    "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Data Stats\".center(80))\n",
    "    print(\"-\" * 80)\n",
    "    for pair in line_pairs[:5]:\n",
    "        print(pair)\n",
    "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
    "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
    "    print(\"Vocab size: {}\".format(vocab_size))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def train(opts):\n",
    "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
    "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
    "\n",
    "    # Split the line pairs into an 80% train and 20% val split\n",
    "    num_lines = len(line_pairs)\n",
    "    num_train = int(0.8 * num_lines)\n",
    "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
    "\n",
    "    # Group the data by the lengths of the source and target words, to form batches\n",
    "    train_dict = create_dict(train_pairs)\n",
    "    val_dict = create_dict(val_pairs)\n",
    "\n",
    "    ##########################################################################\n",
    "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
    "    ##########################################################################\n",
    "    if opts.encoder_type == \"rnn\":\n",
    "        encoder = GRUEncoder(\n",
    "            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n",
    "        )\n",
    "    elif opts.encoder_type == \"transformer\":\n",
    "        encoder = TransformerEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            num_layers=opts.num_transformer_layers,\n",
    "            opts=opts,\n",
    "        )\n",
    "    elif opts.encoder_type == \"attention\":\n",
    "      encoder = AttentionEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            opts=opts,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if opts.decoder_type == \"rnn\":\n",
    "        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n",
    "    elif opts.decoder_type == \"rnn_attention\":\n",
    "        decoder = RNNAttentionDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            attention_type=opts.attention_type,\n",
    "        )\n",
    "    elif opts.decoder_type == \"transformer\":\n",
    "        decoder = TransformerDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            num_layers=opts.num_transformer_layers,\n",
    "        )\n",
    "    elif opts.encoder_type == \"attention\":\n",
    "      decoder = AttentionDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    #### setup checkpoint path\n",
    "    model_name = \"h{}-bs{}-{}-{}\".format(\n",
    "        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n",
    "    )\n",
    "    opts.checkpoint_path = model_name\n",
    "    create_dir_if_not_exists(opts.checkpoint_path)\n",
    "    ####\n",
    "\n",
    "    if opts.cuda:\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "        print(\"Moved models to GPU!\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        losses = training_loop(\n",
    "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting early from training.\")\n",
    "        return encoder, decoder, losses\n",
    "\n",
    "    return encoder, decoder, losses\n",
    "\n",
    "\n",
    "def print_opts(opts):\n",
    "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Opts\".center(80))\n",
    "    print(\"-\" * 80)\n",
    "    for key in opts.__dict__:\n",
    "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yh08KhgnA30"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "aROU2xZanDKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pig_latin_small.txt\n",
      "data/pig_latin_large.txt\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Download Translation datasets\n",
    "######################################################################\n",
    "data_fpath = get_file(\n",
    "    fname=\"pig_latin_small.txt\",\n",
    "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n",
    "    untar=False,\n",
    ")\n",
    "\n",
    "data_fpath = get_file(\n",
    "    fname=\"pig_latin_large.txt\",\n",
    "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n",
    "    untar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDYMr7NclZdw"
   },
   "source": [
    "# Part 1: Neural machine translation (NMT)\n",
    "\n",
    "In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "cOnALRQkkjDO"
   },
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Wif = nn.Linear(input_size, hidden_size)\n",
    "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wii = nn.Linear(input_size, hidden_size)\n",
    "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wic = nn.Linear(input_size, hidden_size)\n",
    "        self.Whc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wio = nn.Linear(input_size, hidden_size)\n",
    "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
    "\n",
    "        Arguments\n",
    "            x: batch_size x input_size\n",
    "            h_prev: batch_size x hidden_size\n",
    "            c_prev: batch_size x hidden_size\n",
    "\n",
    "        Returns:\n",
    "            h_new: batch_size x hidden_size\n",
    "            c_new: batch_size x hidden_size\n",
    "        \"\"\"\n",
    "\n",
    "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
    "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
    "\n",
    "        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n",
    "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
    "\n",
    "        c_new = f * c_prev + i * c\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCae1mOUlZrC"
   },
   "source": [
    "## Step 1: GRU Cell\n",
    "Please implement the `MyGRUCell` class defined in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "DGyxqZIQzTJH"
   },
   "outputs": [],
   "source": [
    "class MyGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyGRUCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        # Input linear layers\n",
    "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
    "        self.Wir = nn.Linear(input_size, hidden_size)\n",
    "        self.Wih = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "\n",
    "        # Hidden linear layers\n",
    "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Forward pass of the GRU computation for one time step.\n",
    "\n",
    "        Arguments\n",
    "            x: batch_size x input_size\n",
    "            h_prev: batch_size x hidden_size\n",
    "\n",
    "        Returns:\n",
    "            h_new: batch_size x hidden_size\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
    "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
    "        h = torch.tanh(self.Wih(x) + self.Whh(r*h_prev))\n",
    "        h_new = (1-z)*h_prev + z*h\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecEq4TP2lZ4Z"
   },
   "source": [
    "## Step 2: GRU Encoder\n",
    "\n",
    "The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "8jDNim2fmVJV"
   },
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, opts):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "        annotations = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n",
    "            hidden = self.gru(x, hidden)\n",
    "            annotations.append(hidden)\n",
    "\n",
    "        annotations = torch.stack(annotations, dim=1)\n",
    "        return annotations, hidden\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
    "        of a batch of sequences.\n",
    "\n",
    "        Arguments:\n",
    "            bs: The batch size for the initial hidden state.\n",
    "\n",
    "        Returns:\n",
    "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "HvwizYM9ma4p"
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
    "            annotations: This is not used here. It just maintains consistency with the\n",
    "                    interface used by the AttentionDecoder class.\n",
    "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        hiddens = []\n",
    "        h_prev = hidden_init\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x = embed[\n",
    "                :, i, :\n",
    "            ]  # Get the current time step input tokens, across the whole batch\n",
    "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
    "            hiddens.append(h_prev)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSDTbsydlaGI"
   },
   "source": [
    "## Step 3: Training and Analysis\n",
    "\n",
    "Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XmVuXTozTPF7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 20                                     \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn                                    \n",
      "                         attention_type:                                        \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('listening', 'isteninglay')\n",
      "('lest', 'estlay')\n",
      "('scruples', 'uplesscray')\n",
      "('pacified', 'acifiedpay')\n",
      "('canvassing', 'anvassingcay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.261 | Val loss: 1.974 | Gen: illway illway illway insay illway\n",
      "Epoch:   1 | Train loss: 1.858 | Val loss: 1.832 | Gen: illway illway ontay-ingay ingay-ingay oussay-ingay-ingay\n",
      "Epoch:   2 | Train loss: 1.699 | Val loss: 1.714 | Gen: ensay ay-ingsay-ingay ontingway ingay ortay-ingsay-ingay\n",
      "Epoch:   3 | Train loss: 1.578 | Val loss: 1.664 | Gen: ensay ay-ationday ontingsay issay ortay-ingsay-ationwa\n",
      "Epoch:   4 | Train loss: 1.487 | Val loss: 1.603 | Gen: essay ay-ay-ationway onday-ingentay-antio iway orday-orday-antionda\n",
      "Epoch:   5 | Train loss: 1.406 | Val loss: 1.537 | Gen: essay aissay onday-ingentsay iway orday-ingedway\n",
      "Epoch:   6 | Train loss: 1.336 | Val loss: 1.483 | Gen: essay ayway onday-intionway iway ordessay-andway-andw\n",
      "Epoch:   7 | Train loss: 1.258 | Val loss: 1.482 | Gen: estay ayway onday-ingedtay iway orday-ingedtay\n",
      "Epoch:   8 | Train loss: 1.202 | Val loss: 1.448 | Gen: estay aissay ondingingsay iway ortingway\n",
      "Epoch:   9 | Train loss: 1.149 | Val loss: 1.382 | Gen: etsay aissay ondingingway-andway- iway ortingway\n",
      "Epoch:  10 | Train loss: 1.095 | Val loss: 1.425 | Gen: etsay aisay-intentway-andw ondingway-intentway- iway ortificationsway\n",
      "Epoch:  11 | Train loss: 1.063 | Val loss: 1.442 | Gen: etay aifay-ingedtay ondingingway-ybay iway ortificationsay\n",
      "Epoch:  12 | Train loss: 1.037 | Val loss: 1.383 | Gen: etway aisay ondingway iway ortingway\n",
      "Epoch:  13 | Train loss: 0.999 | Val loss: 1.350 | Gen: etay aiflyway ondingingway iway ortificationsway\n",
      "Epoch:  14 | Train loss: 1.004 | Val loss: 1.361 | Gen: etsay aificationsway ondingingway iway ortingway-ybay\n",
      "Epoch:  15 | Train loss: 0.961 | Val loss: 1.300 | Gen: etay airay ondingingway iway ortificationsway\n",
      "Epoch:  16 | Train loss: 0.909 | Val loss: 1.295 | Gen: ethay airfay ondingingway iway ortificationsway\n",
      "Epoch:  17 | Train loss: 0.879 | Val loss: 1.276 | Gen: etay airfay ongingway iway ortificationsway\n",
      "Epoch:  18 | Train loss: 0.860 | Val loss: 1.339 | Gen: etay airfay ongingway iway ortificationsray\n",
      "Epoch:  19 | Train loss: 0.845 | Val loss: 1.328 | Gen: etay aringway ongingway iway ortifactionsway\n",
      "Epoch:  20 | Train loss: 0.841 | Val loss: 1.296 | Gen: ethay airfay ondilingway-ybay iway ortingway\n",
      "Epoch:  21 | Train loss: 0.815 | Val loss: 1.293 | Gen: ethay airfay ongingingway iway ortifactionsway\n",
      "Epoch:  22 | Train loss: 0.781 | Val loss: 1.295 | Gen: ethay airfay ondingway iway ortingway\n",
      "Epoch:  23 | Train loss: 0.767 | Val loss: 1.277 | Gen: etay airfay ongingingway iway ortificationsray\n",
      "Epoch:  24 | Train loss: 0.745 | Val loss: 1.249 | Gen: etay airfay ondingway-ybay isway ortitionsway\n",
      "Epoch:  25 | Train loss: 0.733 | Val loss: 1.280 | Gen: ethay airfay ondingingway iway ortingway\n",
      "Epoch:  26 | Train loss: 0.730 | Val loss: 1.267 | Gen: ethay airdway ondingway-imtay-ybay isway ortitionsway\n",
      "Epoch:  27 | Train loss: 0.722 | Val loss: 1.304 | Gen: ethay airfay ondingingway isway ortificationray\n",
      "Epoch:  28 | Train loss: 0.762 | Val loss: 1.295 | Gen: ethay ariday ondilityway isway orvenderay-axtay\n",
      "Epoch:  29 | Train loss: 0.739 | Val loss: 1.215 | Gen: ethay airway ondigingway-ixsay isway ortitionway\n",
      "Epoch:  30 | Train loss: 0.692 | Val loss: 1.228 | Gen: ethay airfay ondingday isway ortitway\n",
      "Epoch:  31 | Train loss: 0.674 | Val loss: 1.209 | Gen: ethay airfay ondingingway-ybay isway ortitionway\n",
      "Epoch:  32 | Train loss: 0.658 | Val loss: 1.229 | Gen: ethay airfay ondingingway isway ortitway\n",
      "Epoch:  33 | Train loss: 0.654 | Val loss: 1.252 | Gen: ethay airdray ondingpay-imedtay isway ortitway\n",
      "Epoch:  34 | Train loss: 0.654 | Val loss: 1.183 | Gen: ethay airfay ondingingway isway ortitway\n",
      "Epoch:  35 | Train loss: 0.633 | Val loss: 1.198 | Gen: ethay airdway ondidingway isway ortitway\n",
      "Epoch:  36 | Train loss: 0.622 | Val loss: 1.201 | Gen: ethedway airfay ondidientsway isway ortitway\n",
      "Epoch:  37 | Train loss: 0.612 | Val loss: 1.196 | Gen: ethay airdway ondidilyway isway ortitway\n",
      "Epoch:  38 | Train loss: 0.601 | Val loss: 1.185 | Gen: ethay airdway ondilitybay isway ortitway\n",
      "Epoch:  39 | Train loss: 0.608 | Val loss: 1.211 | Gen: eethay airdway ondingway-illyway iway ourtitway\n",
      "Epoch:  40 | Train loss: 0.609 | Val loss: 1.180 | Gen: ethay airdway ondingingbay isway ortitway\n",
      "Epoch:  41 | Train loss: 0.603 | Val loss: 1.171 | Gen: ethay airdray ondingitionsway isway ortitsway\n",
      "Epoch:  42 | Train loss: 0.593 | Val loss: 1.183 | Gen: ethay airfay ondilitybay isway ortitway\n",
      "Epoch:  43 | Train loss: 0.596 | Val loss: 1.263 | Gen: ethay airdway ondigingway isway ortitway\n",
      "Epoch:  44 | Train loss: 0.586 | Val loss: 1.219 | Gen: ethay airdray ondidionmay isway ortitionmay\n",
      "Epoch:  45 | Train loss: 0.604 | Val loss: 1.192 | Gen: ethay airfay ondidingstay isway ortitsway\n",
      "Epoch:  46 | Train loss: 0.589 | Val loss: 1.253 | Gen: ethay airway ondidingway isway ortithway\n",
      "Epoch:  47 | Train loss: 0.576 | Val loss: 1.212 | Gen: ehenday airway ondidilybay isway ortithway\n",
      "Epoch:  48 | Train loss: 0.554 | Val loss: 1.184 | Gen: ethay airway ondingingway isway ortithway\n",
      "Epoch:  49 | Train loss: 0.526 | Val loss: 1.186 | Gen: ehtway airdray ondidalnesway isway orvedthay\n",
      "Obtained lowest validation loss of: 1.1707285598323152\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tehtway airdray ondidalnesway isway orvedthay\n",
      "train time  175.50101137161255\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "rnn_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
    "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
    "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
    "}\n",
    "rnn_args_s.update(args_dict)\n",
    "\n",
    "print_opts(rnn_args_s)\n",
    "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"train time \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mR97V_NtER6"
   },
   "source": [
    "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
    "\n",
    "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H3YLrAjsmx_W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 10                                     \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn                                    \n",
      "                         attention_type:                                        \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('fracture', 'acturefray')\n",
      "('idled', 'idledway')\n",
      "('lever', 'everlay')\n",
      "('overnight', 'overnightway')\n",
      "('apollo', 'apolloway')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.319 | Val loss: 1.989 | Gen: ay ay-ay-ay onday-ay ontay-ay onday\n",
      "Epoch:   1 | Train loss: 1.883 | Val loss: 1.854 | Gen: ay-ay away-ayday onday-ayday ingay-ay-ay-ay onday-ayday\n",
      "Epoch:   2 | Train loss: 1.730 | Val loss: 1.772 | Gen: eray-ayday ayday-ayday ontay-away-ayday ingay-ayday ontay-ayday-ayday\n",
      "Epoch:   3 | Train loss: 1.626 | Val loss: 1.687 | Gen: eray ayday-ayday ontay-ingway-ayday incay-ayday otingray-away-ayday\n",
      "Epoch:   4 | Train loss: 1.531 | Val loss: 1.615 | Gen: eway arway ontingnedgay-ingway- incay otergrontay-ingronta\n",
      "Epoch:   5 | Train loss: 1.438 | Val loss: 1.568 | Gen: eway array ontednay-ingway-away isteday-away othergay-ontay-ontay\n",
      "Epoch:   6 | Train loss: 1.382 | Val loss: 1.497 | Gen: eway araypay ontergenconterway istay orgherghay-ontay-ont\n",
      "Epoch:   7 | Train loss: 1.345 | Val loss: 1.481 | Gen: eway array ontingnay-inway istay-inway ortiongray\n",
      "Epoch:   8 | Train loss: 1.267 | Val loss: 1.443 | Gen: eway arraypay ontingnay-inway-away istay ortingnay\n",
      "Epoch:   9 | Train loss: 1.210 | Val loss: 1.383 | Gen: eway ariray ontingnay-inway istway ommungray-ingway\n",
      "Epoch:  10 | Train loss: 1.153 | Val loss: 1.382 | Gen: eway array ontingcay istay ommunicontay\n",
      "Epoch:  11 | Train loss: 1.117 | Val loss: 1.346 | Gen: eway ariray onglay-incedway istway ortingnonay\n",
      "Epoch:  12 | Train loss: 1.088 | Val loss: 1.334 | Gen: eway array ongancontay istionsway orkingnay\n",
      "Epoch:  13 | Train loss: 1.070 | Val loss: 1.326 | Gen: eway array oninglay-inway-away istay ortingnay\n",
      "Epoch:  14 | Train loss: 1.035 | Val loss: 1.330 | Gen: eway array ontingconway isway orkingnenay\n",
      "Epoch:  15 | Train loss: 1.015 | Val loss: 1.288 | Gen: eway array ondinglay-inway-awla isway orkingnay\n",
      "Epoch:  16 | Train loss: 0.980 | Val loss: 1.243 | Gen: eway iray ondingledway-awlay isway orkingnay\n",
      "Epoch:  17 | Train loss: 0.946 | Val loss: 1.199 | Gen: eway arriteway ondinglay isway orkingnay\n",
      "Epoch:  18 | Train loss: 0.910 | Val loss: 1.229 | Gen: ethay ariway ontincoway isway orkingnay\n",
      "Epoch:  19 | Train loss: 0.889 | Val loss: 1.196 | Gen: etay arireway oningledneway isway orkingnay\n",
      "Epoch:  20 | Train loss: 0.881 | Val loss: 1.234 | Gen: eway ayray ondingcay isway orkingnay\n",
      "Epoch:  21 | Train loss: 0.890 | Val loss: 1.200 | Gen: etway imay oningniccationstay issway orkingnay\n",
      "Epoch:  22 | Train loss: 0.864 | Val loss: 1.218 | Gen: ethay ayray oningcay issway orkingnay\n",
      "Epoch:  23 | Train loss: 0.850 | Val loss: 1.147 | Gen: etway arireway ondingscay isway orkingnay\n",
      "Epoch:  24 | Train loss: 0.808 | Val loss: 1.147 | Gen: ethay airway ontinconway isway orkingnenay\n",
      "Epoch:  25 | Train loss: 0.789 | Val loss: 1.134 | Gen: ethay ariray ontincechay isway orkingnay\n",
      "Epoch:  26 | Train loss: 0.789 | Val loss: 1.156 | Gen: ehay ariray oningcay-ompationtay isway orkingnay\n",
      "Epoch:  27 | Train loss: 0.780 | Val loss: 1.132 | Gen: etay arireway onningcay isway orkingnetray\n",
      "Epoch:  28 | Train loss: 0.758 | Val loss: 1.148 | Gen: eheway airway ontingconway isway orkingnay\n",
      "Epoch:  29 | Train loss: 0.762 | Val loss: 1.164 | Gen: eway arireway onningctay isway orkingnay\n",
      "Epoch:  30 | Train loss: 0.760 | Val loss: 1.143 | Gen: ethway arieway ondingway issway orkingnay\n",
      "Epoch:  31 | Train loss: 0.744 | Val loss: 1.148 | Gen: ethway ayray ontingctonesway isway orkingnay\n",
      "Epoch:  32 | Train loss: 0.717 | Val loss: 1.128 | Gen: ethay airway ontingectingnay isway orkingnay\n",
      "Epoch:  33 | Train loss: 0.691 | Val loss: 1.068 | Gen: ethay arireway onnintectioncay isway orkingnetray\n",
      "Epoch:  34 | Train loss: 0.662 | Val loss: 1.046 | Gen: ethway airway ondiniectionsway isway orkingnay\n",
      "Epoch:  35 | Train loss: 0.647 | Val loss: 1.044 | Gen: ethay aireway ontinconway isway orkingnetway\n",
      "Epoch:  36 | Train loss: 0.640 | Val loss: 1.142 | Gen: ethway airway ontincemay issway orkingnay\n",
      "Epoch:  37 | Train loss: 0.660 | Val loss: 1.094 | Gen: ethway aireway onningedscay isway orkingnay\n",
      "Epoch:  38 | Train loss: 0.656 | Val loss: 1.135 | Gen: ethway airway ondinienceway isway orkingnenway\n",
      "Epoch:  39 | Train loss: 0.649 | Val loss: 1.171 | Gen: ethway arireway ontincenessway isway orkingnay\n",
      "Epoch:  40 | Train loss: 0.653 | Val loss: 1.117 | Gen: ethay airerway onintectionedway isway orkingnay\n",
      "Epoch:  41 | Train loss: 0.655 | Val loss: 1.139 | Gen: etway airway ontingeddatingway isway orkingway\n",
      "Epoch:  42 | Train loss: 0.641 | Val loss: 1.040 | Gen: ethay airway ontinmencay isway orkingnay-itlyway\n",
      "Epoch:  43 | Train loss: 0.611 | Val loss: 1.130 | Gen: ethway arireway ontinmonscay isway orkingnay\n",
      "Epoch:  44 | Train loss: 0.613 | Val loss: 1.073 | Gen: etway airway ontingctionedway isway orkingnay\n",
      "Epoch:  45 | Train loss: 0.584 | Val loss: 1.054 | Gen: ethway airerway ontinmonday isway orkingnay\n",
      "Epoch:  46 | Train loss: 0.569 | Val loss: 1.012 | Gen: ethway aireway ontinmenctionway isway orkingnay-otuinway\n",
      "Epoch:  47 | Train loss: 0.567 | Val loss: 1.027 | Gen: ethway airway ondinionedway isway orkingnay\n",
      "Epoch:  48 | Train loss: 0.557 | Val loss: 1.029 | Gen: ethay iarway oninticechay isway orkingnay\n",
      "Epoch:  49 | Train loss: 0.553 | Val loss: 0.998 | Gen: etay airway ondinionedway isway orkingnay\n",
      "Obtained lowest validation loss of: 0.9981780432018579\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tetay airway ondinionedway isway orkingnay\n",
      "train time  213.0550720691681\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "rnn_args_l = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_large\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"batch_size\": 512,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
    "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
    "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
    "}\n",
    "rnn_args_l.update(args_dict)\n",
    "\n",
    "print_opts(rnn_args_l)\n",
    "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"train time \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01HsZ6EItc56"
   },
   "source": [
    "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Qyk_9-Fwtekj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to: /home/maryamebr/anaconda3/envs/new-tf-gpu/CSC2516_PA3/loss_plot_gru.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE4ijaCzneAt"
   },
   "source": [
    "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WrNnz8W1nULf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tetay airway ondinionedway isway orkingnay\n",
      "source:\t\texceptional flavor \n",
      "translated:\texpecotingpay avorlechay\n",
      "source:\t\tonomatopoeia is hard \n",
      "translated:\tooopationallyway isway ardfay\n"
     ]
    }
   ],
   "source": [
    "best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n",
    "best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n",
    "best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n",
    "\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "\n",
    "\n",
    "### long and rare words\n",
    "\n",
    "TEST_SENTENCE = \"exceptional flavor\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "\n",
    "\n",
    "TEST_SENTENCE = \"onomatopoeia is hard\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWwA6OGqlaTq"
   },
   "source": [
    "# Part 2: Attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJSafHSAmu_w"
   },
   "source": [
    "## Step 1: Additive attention\n",
    "\n",
    "In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "AdewEVSMo5jJ"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # A two layer fully-connected network\n",
    "        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n",
    "        self.attention_network = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the additive attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "        batch_size = keys.size(0)\n",
    "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n",
    "            keys\n",
    "        )\n",
    "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
    "        unnormalized_attention = self.attention_network(concat_inputs)\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(2, 1), values)\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73_p8d5EmvOJ"
   },
   "source": [
    "## Step 2: RNN + additive attention\n",
    "\n",
    "In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "RJaABkXrpJSw"
   },
   "outputs": [],
   "source": [
    "class RNNAttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n",
    "        super(RNNAttentionDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n",
    "        if attention_type == \"additive\":\n",
    "            self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
    "        elif attention_type == \"scaled_dot\":\n",
    "            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        hiddens = []\n",
    "        attentions = []\n",
    "        h_prev = hidden_init\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            embed_current = embed[\n",
    "                :, i, :\n",
    "            ]  # Get the current time step, across the whole batch\n",
    "            context, attention_weights = self.attention(\n",
    "                h_prev, annotations, annotations\n",
    "            )  # batch_size x 1 x hidden_size\n",
    "            embed_and_context = torch.cat(\n",
    "                [embed_current, context.squeeze(1)], dim=1\n",
    "            )  # batch_size x (2*hidden_size)\n",
    "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n",
    "\n",
    "            hiddens.append(h_prev)\n",
    "            attentions.append(attention_weights)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
    "        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n",
    "\n",
    "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
    "        return output, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYPae08Io1Fi"
   },
   "source": [
    "## Step 3: Training and analysis (with additive attention)\n",
    "\n",
    "Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ke6t6rCezpZV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 10                                     \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn_attention                          \n",
      "                         attention_type: additive                               \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('abridgement', 'abridgementway')\n",
      "('delicacies', 'elicaciesday')\n",
      "('served', 'ervedsay')\n",
      "('us', 'usway')\n",
      "('simpson', 'impsonsay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 1.958 | Val loss: 1.775 | Gen: etay aray ongay-incay isay orgay-orday\n",
      "Epoch:   1 | Train loss: 1.404 | Val loss: 1.593 | Gen: etetay away oonay-otinay isway orway\n",
      "Epoch:   2 | Train loss: 1.133 | Val loss: 1.427 | Gen: etehay ay-arway oncay-ingcay-inconca isway ingway\n",
      "Epoch:   3 | Train loss: 0.953 | Val loss: 1.480 | Gen: ethay airway-arway-arway-a olcingway issesway ortingway\n",
      "Epoch:   4 | Train loss: 0.810 | Val loss: 1.070 | Gen: ethay airway ondingcay-odingcay isway orugway\n",
      "Epoch:   5 | Train loss: 0.576 | Val loss: 1.001 | Gen: epay array onditingcay isway ortingway\n",
      "Epoch:   6 | Train loss: 0.451 | Val loss: 0.833 | Gen: ethay arway ondingcay iway oringway\n",
      "Epoch:   7 | Train loss: 0.338 | Val loss: 0.801 | Gen: ethay airway onditingcay-oningcay isway ortingway\n",
      "Epoch:   8 | Train loss: 0.295 | Val loss: 0.665 | Gen: ethay airway ondindingcay isway orkingway\n",
      "Epoch:   9 | Train loss: 0.216 | Val loss: 0.730 | Gen: ethay airway ondingcay-ionicingca isway oringway\n",
      "Epoch:  10 | Train loss: 0.205 | Val loss: 0.696 | Gen: ehay arway ondiningcay-oningcay isway orkingway\n",
      "Epoch:  11 | Train loss: 0.218 | Val loss: 0.772 | Gen: ethay arway onditingcay isway ougingway\n",
      "Epoch:  12 | Train loss: 0.198 | Val loss: 0.552 | Gen: eway arway onditingcay isway orkingway\n",
      "Epoch:  13 | Train loss: 0.136 | Val loss: 0.645 | Gen: ethay arway ondingcay isway orkingway\n",
      "Epoch:  14 | Train loss: 0.140 | Val loss: 0.850 | Gen: ethay ariway ondingcay isway ordgway\n",
      "Epoch:  15 | Train loss: 0.143 | Val loss: 0.371 | Gen: etay airway onditingcay isway orkingway\n",
      "Epoch:  16 | Train loss: 0.063 | Val loss: 0.330 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  17 | Train loss: 0.037 | Val loss: 0.315 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  18 | Train loss: 0.032 | Val loss: 0.294 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  19 | Train loss: 0.019 | Val loss: 0.279 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  20 | Train loss: 0.012 | Val loss: 0.274 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  21 | Train loss: 0.009 | Val loss: 0.265 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  22 | Train loss: 0.007 | Val loss: 0.268 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  23 | Train loss: 0.005 | Val loss: 0.266 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  24 | Train loss: 0.005 | Val loss: 0.266 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  25 | Train loss: 0.004 | Val loss: 0.266 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  26 | Train loss: 0.004 | Val loss: 0.267 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  27 | Train loss: 0.003 | Val loss: 0.268 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  28 | Train loss: 0.003 | Val loss: 0.269 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  29 | Train loss: 0.002 | Val loss: 0.270 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  30 | Train loss: 0.002 | Val loss: 0.272 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  31 | Train loss: 0.002 | Val loss: 0.274 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Validation loss has not improved in 10 epochs, stopping early\n",
      "Obtained lowest validation loss of: 0.265358589754297\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditioningcay isway orkingway\n",
      "train time  144.27908396720886\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "rnn_attn_args = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n",
    "    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n",
    "    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n",
    "}\n",
    "rnn_attn_args.update(args_dict)\n",
    "\n",
    "print_opts(rnn_attn_args)\n",
    "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"train time \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "VNVKbLc0ACj_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditioningcay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xq7nhsEio1w-"
   },
   "source": [
    "## Step 4: Implement scaled dot-product attention\n",
    "\n",
    "In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d_j3oY3hqsJQ"
   },
   "outputs": [],
   "source": [
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(\n",
    "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        batch_size = keys.size(0)\n",
    "        \n",
    "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
    "        k = self.K(keys).view(batch_size, -1, self.hidden_size)\n",
    "        v = self.V(values).view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        unnormalized_attention = torch.bmm(k, q.transpose(2,1))*self.scaling_factor\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
    "        return context, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unReAOrjo113"
   },
   "source": [
    "## Step 5: Implement causal dot-product Attention\n",
    "\n",
    "\n",
    "Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ovigzQffrKqj"
   },
   "outputs": [],
   "source": [
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7)\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(\n",
    "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        batch_size = keys.size(0)\n",
    "        \n",
    "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
    "        k = self.K(keys).view(batch_size, -1, self.hidden_size)\n",
    "        v = self.V(values).view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n",
    "        mask = torch.tril(torch.ones_like(unnormalized_attention) * self.neg_inf, diagonal=-1)\n",
    "        attention_weights = self.softmax(unnormalized_attention + mask)\n",
    "\n",
    "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
    "        return context, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkjHbtvT6Qxs"
   },
   "source": [
    "## Step 6: Attention encoder and decoder\n",
    "\n",
    "The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "yKGNqUaX6RLO"
   },
   "outputs": [],
   "source": [
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, opts):\n",
    "        super(AttentionEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.self_attention = ScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "               \n",
    "        self.attention_mlp = nn.Sequential(\n",
    "                                nn.Linear(hidden_size, hidden_size),\n",
    "                                nn.ReLU(),\n",
    "                              )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder scaled dot attention.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            None: Used to conform to standard encoder return signature.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.size()\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        annotations = encoded\n",
    "        new_annotations, self_attention_weights = self.self_attention(\n",
    "            annotations, annotations, annotations\n",
    "        )  # batch_size x seq_len x hidden_size\n",
    "        residual_annotations = annotations + new_annotations\n",
    "        new_annotations = self.attention_mlp(residual_annotations)\n",
    "        annotations = residual_annotations + new_annotations\n",
    "\n",
    "        return annotations, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vDUvtOee7cMy"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.self_attention = CausalScaledDotAttention(\n",
    "                                hidden_size=hidden_size,\n",
    "                                )\n",
    "                \n",
    "        self.decoder_attention = ScaledDotAttention(\n",
    "                                  hidden_size=hidden_size,\n",
    "                                  )\n",
    "                \n",
    "        self.attention_mlp = nn.Sequential(\n",
    "                                nn.Linear(hidden_size, hidden_size),\n",
    "                                nn.ReLU(),\n",
    "                              )\n",
    "                \n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: Not used in the transformer decoder\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        new_contexts, self_attention_weights = self.self_attention(\n",
    "            contexts, contexts, contexts\n",
    "        )  # batch_size x seq_len x hidden_size\n",
    "        residual_contexts = contexts + new_contexts\n",
    "        new_contexts, encoder_attention_weights = self.decoder_attention(\n",
    "            residual_contexts, annotations, annotations\n",
    "        )  # batch_size x seq_len x hidden_size\n",
    "        residual_contexts = residual_contexts + new_contexts\n",
    "        new_contexts = self.attention_mlp(residual_contexts)\n",
    "        contexts = residual_contexts + new_contexts\n",
    "\n",
    "        encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "        self_attention_weights_list.append(self_attention_weights)\n",
    "\n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "\n",
    "        return output, (encoder_attention_weights, self_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7gJLw5t_rnW"
   },
   "source": [
    "## Step 7: Training and analysis (single scaled dot-product attention block)\n",
    "\n",
    "Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7MOkZonC8T3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 100                                    \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: attention                              \n",
      "                           decoder_type: attention                              \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('abridgement', 'abridgementway')\n",
      "('delicacies', 'elicaciesday')\n",
      "('served', 'ervedsay')\n",
      "('us', 'usway')\n",
      "('simpson', 'impsonsay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.967 | Val loss: 2.450 | Gen: iayay ay ay say ay  \n",
      "Epoch:   1 | Train loss: 2.271 | Val loss: 2.168 | Gen: eeay ay ingay issay ingay\n",
      "Epoch:   2 | Train loss: 2.058 | Val loss: 2.059 | Gen: eay ay ongay issay ongay\n",
      "Epoch:   3 | Train loss: 1.950 | Val loss: 1.987 | Gen: eay ay ongay issay ongay\n",
      "Epoch:   4 | Train loss: 1.867 | Val loss: 1.924 | Gen: eay ay ongay isay ongay\n",
      "Epoch:   5 | Train loss: 1.797 | Val loss: 1.869 | Gen: eay iay ongay isay ongay\n",
      "Epoch:   6 | Train loss: 1.733 | Val loss: 1.822 | Gen: eteay iay ongay isisay ongay\n",
      "Epoch:   7 | Train loss: 1.675 | Val loss: 1.781 | Gen: eteteay iay ongay isisay ongay\n",
      "Epoch:   8 | Train loss: 1.623 | Val loss: 1.746 | Gen: eteteay iay ongay isisay ongay\n",
      "Epoch:   9 | Train loss: 1.578 | Val loss: 1.708 | Gen: etetay away ondgay isisay ongay\n",
      "Epoch:  10 | Train loss: 1.537 | Val loss: 1.689 | Gen: etetay away onday isisay ongay\n",
      "Epoch:  11 | Train loss: 1.503 | Val loss: 1.637 | Gen: etetay aiay inday isisay ongway\n",
      "Epoch:  12 | Train loss: 1.464 | Val loss: 1.600 | Gen: etay away indeday isisay ongway\n",
      "Epoch:  13 | Train loss: 1.427 | Val loss: 1.567 | Gen: etetay away indeday isisay oray\n",
      "Epoch:  14 | Train loss: 1.394 | Val loss: 1.537 | Gen: etetay away incingway isisay oray\n",
      "Epoch:  15 | Train loss: 1.365 | Val loss: 1.515 | Gen: etetay away incingway isisay oray\n",
      "Epoch:  16 | Train loss: 1.338 | Val loss: 1.491 | Gen: etetay away incingintingway isisay origway\n",
      "Epoch:  17 | Train loss: 1.313 | Val loss: 1.473 | Gen: etetay away incingintingway isisay origway\n",
      "Epoch:  18 | Train loss: 1.289 | Val loss: 1.454 | Gen: etetay away inmincay isisay ingway\n",
      "Epoch:  19 | Train loss: 1.265 | Val loss: 1.439 | Gen: etay away inmingintingintingwa isway ingway\n",
      "Epoch:  20 | Train loss: 1.241 | Val loss: 1.422 | Gen: etay away ingintinginay isway ingway\n",
      "Epoch:  21 | Train loss: 1.220 | Val loss: 1.408 | Gen: etay away inmingintingintingwa isway ingway\n",
      "Epoch:  22 | Train loss: 1.204 | Val loss: 1.393 | Gen: etay away ingintingday isway ingway\n",
      "Epoch:  23 | Train loss: 1.189 | Val loss: 1.381 | Gen: etay away inmingintingday isway ingway\n",
      "Epoch:  24 | Train loss: 1.174 | Val loss: 1.369 | Gen: etay away inmingintingday isway ingway\n",
      "Epoch:  25 | Train loss: 1.161 | Val loss: 1.359 | Gen: ehtay away inmingintingday isway ingway\n",
      "Epoch:  26 | Train loss: 1.148 | Val loss: 1.350 | Gen: ehtay away inmingday isway ingway\n",
      "Epoch:  27 | Train loss: 1.136 | Val loss: 1.341 | Gen: ehtay away inmingday isway ingway\n",
      "Epoch:  28 | Train loss: 1.124 | Val loss: 1.332 | Gen: ehtay away inmingdway isway ingway\n",
      "Epoch:  29 | Train loss: 1.113 | Val loss: 1.325 | Gen: ehtay away inmingday isway ingway\n",
      "Epoch:  30 | Train loss: 1.103 | Val loss: 1.318 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  31 | Train loss: 1.092 | Val loss: 1.311 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  32 | Train loss: 1.082 | Val loss: 1.304 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  33 | Train loss: 1.073 | Val loss: 1.298 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  34 | Train loss: 1.064 | Val loss: 1.291 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  35 | Train loss: 1.056 | Val loss: 1.286 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  36 | Train loss: 1.047 | Val loss: 1.280 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  37 | Train loss: 1.039 | Val loss: 1.275 | Gen: ehtay away inmiongday isway ingway\n",
      "Epoch:  38 | Train loss: 1.032 | Val loss: 1.269 | Gen: ehtehay away inmiongday isway ingway\n",
      "Epoch:  39 | Train loss: 1.024 | Val loss: 1.264 | Gen: ehtehay away inmiongday isway ingway\n",
      "Epoch:  40 | Train loss: 1.017 | Val loss: 1.259 | Gen: ehtehay away inmiongday isway ingway\n",
      "Epoch:  41 | Train loss: 1.009 | Val loss: 1.255 | Gen: ehtehay away inmiongday isway ingway\n",
      "Epoch:  42 | Train loss: 1.003 | Val loss: 1.250 | Gen: ehtehay away inmiongday isway ingway\n",
      "Epoch:  43 | Train loss: 0.996 | Val loss: 1.245 | Gen: ehtehay away inmiongday isway ingway\n",
      "Epoch:  44 | Train loss: 0.989 | Val loss: 1.240 | Gen: ehtehay arway inmiongday isway ingway\n",
      "Epoch:  45 | Train loss: 0.983 | Val loss: 1.237 | Gen: ehtehay arway inciongday isway ingway\n",
      "Epoch:  46 | Train loss: 0.977 | Val loss: 1.232 | Gen: ehtehay arway inciongday isway ingway\n",
      "Epoch:  47 | Train loss: 0.971 | Val loss: 1.227 | Gen: ehtehay arway inciongday isway ingway\n",
      "Epoch:  48 | Train loss: 0.965 | Val loss: 1.223 | Gen: ehtehay arway inciongday isway ingway\n",
      "Epoch:  49 | Train loss: 0.959 | Val loss: 1.218 | Gen: ehtehay arway inciongday isway ingway\n",
      "Epoch:  50 | Train loss: 0.953 | Val loss: 1.214 | Gen: ehtehay arway inciongday isway ingway\n",
      "Epoch:  51 | Train loss: 0.947 | Val loss: 1.210 | Gen: ehtehay arway inciongday isway ingway\n",
      "Epoch:  52 | Train loss: 0.942 | Val loss: 1.205 | Gen: ehtetay arway inciongday isway ingway\n",
      "Epoch:  53 | Train loss: 0.937 | Val loss: 1.202 | Gen: ehtetay arway inciongday isway ingway\n",
      "Epoch:  54 | Train loss: 0.932 | Val loss: 1.198 | Gen: ehtetay arway inciongday isway ingway\n",
      "Epoch:  55 | Train loss: 0.927 | Val loss: 1.195 | Gen: ehtetay arway incionmay isway ingway\n",
      "Epoch:  56 | Train loss: 0.922 | Val loss: 1.191 | Gen: ehtetay arway incionmay isway ingway\n",
      "Epoch:  57 | Train loss: 0.917 | Val loss: 1.187 | Gen: ehtetay arway incionmay isway ingway\n",
      "Epoch:  58 | Train loss: 0.912 | Val loss: 1.183 | Gen: ehtetay arway incionmay isway ingway\n",
      "Epoch:  59 | Train loss: 0.908 | Val loss: 1.181 | Gen: ehtetay arway incionmay isway ingway\n",
      "Epoch:  60 | Train loss: 0.903 | Val loss: 1.177 | Gen: ehtetay arway incionciongday isway ingway\n",
      "Epoch:  61 | Train loss: 0.899 | Val loss: 1.174 | Gen: ehtetay arway incionciongday isway ingway\n",
      "Epoch:  62 | Train loss: 0.894 | Val loss: 1.171 | Gen: ehtetay arway incionciongday isway ingway\n",
      "Epoch:  63 | Train loss: 0.890 | Val loss: 1.168 | Gen: ehtetay arway incionciongday isway ingway\n",
      "Epoch:  64 | Train loss: 0.886 | Val loss: 1.165 | Gen: ethay arway incionciongday isway ingway\n",
      "Epoch:  65 | Train loss: 0.882 | Val loss: 1.162 | Gen: ethay arway incionciongday isway ingway\n",
      "Epoch:  66 | Train loss: 0.878 | Val loss: 1.160 | Gen: ethay arway incionciongday isway orkwaay\n",
      "Epoch:  67 | Train loss: 0.874 | Val loss: 1.156 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  68 | Train loss: 0.870 | Val loss: 1.154 | Gen: ethay arway incionciontingway isway orkingway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  69 | Train loss: 0.866 | Val loss: 1.151 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  70 | Train loss: 0.863 | Val loss: 1.149 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  71 | Train loss: 0.859 | Val loss: 1.147 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  72 | Train loss: 0.856 | Val loss: 1.145 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  73 | Train loss: 0.852 | Val loss: 1.143 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  74 | Train loss: 0.849 | Val loss: 1.140 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  75 | Train loss: 0.846 | Val loss: 1.138 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  76 | Train loss: 0.843 | Val loss: 1.136 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  77 | Train loss: 0.839 | Val loss: 1.134 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  78 | Train loss: 0.836 | Val loss: 1.133 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  79 | Train loss: 0.833 | Val loss: 1.130 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  80 | Train loss: 0.830 | Val loss: 1.129 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  81 | Train loss: 0.827 | Val loss: 1.126 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  82 | Train loss: 0.824 | Val loss: 1.125 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  83 | Train loss: 0.822 | Val loss: 1.122 | Gen: ethay arway incionciontingway isway orkingway\n",
      "Epoch:  84 | Train loss: 0.819 | Val loss: 1.121 | Gen: ethay arway incionciontway isway orkingway\n",
      "Epoch:  85 | Train loss: 0.816 | Val loss: 1.120 | Gen: ethay arway incionciontway isway orkingway\n",
      "Epoch:  86 | Train loss: 0.814 | Val loss: 1.118 | Gen: ethay arway incionciontway isway orkingway\n",
      "Epoch:  87 | Train loss: 0.811 | Val loss: 1.117 | Gen: ethay arway incionciontway isway orkingway\n",
      "Epoch:  88 | Train loss: 0.809 | Val loss: 1.115 | Gen: ethay arway inciontingday isway orkingway\n",
      "Epoch:  89 | Train loss: 0.806 | Val loss: 1.114 | Gen: ethay arway inciontingday isway orkingway\n",
      "Epoch:  90 | Train loss: 0.804 | Val loss: 1.113 | Gen: ethay arway inciontingday isway orkingway\n",
      "Epoch:  91 | Train loss: 0.801 | Val loss: 1.113 | Gen: ethay arway inciontingday isway orkingway\n",
      "Epoch:  92 | Train loss: 0.799 | Val loss: 1.112 | Gen: ethay arway inciontingday isway orkingway\n",
      "Epoch:  93 | Train loss: 0.797 | Val loss: 1.112 | Gen: ethay arway inciontingday isway orkingway\n",
      "Epoch:  94 | Train loss: 0.794 | Val loss: 1.111 | Gen: ethay arway inciontingday isway orkingray\n",
      "Epoch:  95 | Train loss: 0.792 | Val loss: 1.111 | Gen: ethay arway inciontingday isway orkingray\n",
      "Epoch:  96 | Train loss: 0.790 | Val loss: 1.110 | Gen: ethay arway inciontingday isway orkingray\n",
      "Epoch:  97 | Train loss: 0.788 | Val loss: 1.110 | Gen: ethay arway inciontingday isway orkingray\n",
      "Epoch:  98 | Train loss: 0.786 | Val loss: 1.110 | Gen: ethay arway inciontingday isway orkingray\n",
      "Epoch:  99 | Train loss: 0.783 | Val loss: 1.109 | Gen: ethay arway inciontingday isway orkingray\n",
      "Obtained lowest validation loss of: 1.1092625361951916\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay arway inciontingday isway orkingray\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "attention_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 100,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 100,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"attention\",\n",
    "    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n",
    "}\n",
    "attention_args_s.update(args_dict)\n",
    "print_opts(attention_args_s)\n",
    "\n",
    "attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tcpUFKqo2Oi"
   },
   "source": [
    "## Step 8: Transformer encoder and decoder\n",
    "\n",
    "The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "N3B-fWsarlVk"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.self_attentions = nn.ModuleList(\n",
    "            [\n",
    "                ScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_mlps = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            None: Used to conform to standard encoder return signature.\n",
    "            None: Used to conform to standard encoder return signature.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.size()\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
    "        encoded = encoded + self.positional_encodings[:seq_len]\n",
    "\n",
    "        annotations = encoded\n",
    "        for i in range(self.num_layers):\n",
    "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
    "                annotations, annotations, annotations\n",
    "            )  # batch_size x seq_len x hidden_size\n",
    "            residual_annotations = annotations + new_annotations\n",
    "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
    "            annotations = residual_annotations + new_annotations\n",
    "\n",
    "        # Transformer encoder does not have a last hidden or cell layer.\n",
    "        return annotations, None\n",
    "        # return annotations, None, None\n",
    "    def create_positional_encodings(self, max_seq_len=1000):\n",
    "        \"\"\"Creates positional encodings for the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
    "\n",
    "        Returns:\n",
    "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
    "        \"\"\"\n",
    "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
    "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
    "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
    "        trig_args = pos_indices / (10000**exponents)\n",
    "        sin_terms = torch.sin(trig_args)\n",
    "        cos_terms = torch.cos(trig_args)\n",
    "\n",
    "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
    "        pos_encodings[:, 0::2] = sin_terms\n",
    "        pos_encodings[:, 1::2] = cos_terms\n",
    "\n",
    "        if self.opts.cuda:\n",
    "            pos_encodings = pos_encodings.cuda()\n",
    "\n",
    "        return pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "nyvTZFxtrvc6"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.self_attentions = nn.ModuleList(\n",
    "            [\n",
    "                CausalScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.encoder_attentions = nn.ModuleList(\n",
    "            [\n",
    "                ScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_mlps = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: Not used in the transformer decoder\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        embed = embed + self.positional_encodings[:seq_len]\n",
    "\n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        for i in range(self.num_layers):\n",
    "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
    "                contexts, contexts, contexts\n",
    "            )  # batch_size x seq_len x hidden_size\n",
    "            residual_contexts = contexts + new_contexts\n",
    "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
    "                residual_contexts, annotations, annotations\n",
    "            )  # batch_size x seq_len x hidden_size\n",
    "            residual_contexts = residual_contexts + new_contexts\n",
    "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
    "            contexts = residual_contexts + new_contexts\n",
    "\n",
    "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "            self_attention_weights_list.append(self_attention_weights)\n",
    "\n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "\n",
    "        return output, (encoder_attention_weights, self_attention_weights)\n",
    "\n",
    "    def create_positional_encodings(self, max_seq_len=1000):\n",
    "        \"\"\"Creates positional encodings for the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
    "\n",
    "        Returns:\n",
    "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
    "        \"\"\"\n",
    "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
    "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
    "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
    "        trig_args = pos_indices / (10000**exponents)\n",
    "        sin_terms = torch.sin(trig_args)\n",
    "        cos_terms = torch.cos(trig_args)\n",
    "\n",
    "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
    "        pos_encodings[:, 0::2] = sin_terms\n",
    "        pos_encodings[:, 1::2] = cos_terms\n",
    "\n",
    "        pos_encodings = pos_encodings.cuda()\n",
    "\n",
    "        return pos_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29ZjkXTNrUKb"
   },
   "source": [
    "\n",
    "## Step 9: Training and analysis (with scaled dot-product attention)\n",
    "\n",
    "Now we will train a (simplified) transformer encoder-decoder model.\n",
    "\n",
    "First, we train our smaller model on the small dataset. Use this model to answer Question 5 in the handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "mk8e4KSnuZ8N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 100                                    \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 4                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('abridgement', 'abridgementway')\n",
      "('delicacies', 'elicaciesday')\n",
      "('served', 'ervedsay')\n",
      "('us', 'usway')\n",
      "('simpson', 'impsonsay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.703 | Val loss: 2.360 | Gen: eay aay otllllllllllllllllll issay ay-ay-ay-ay\n",
      "Epoch:   1 | Train loss: 2.004 | Val loss: 2.077 | Gen: eay aarrrrrrray ongndilllllay-ay-ay isssay aray\n",
      "Epoch:   2 | Train loss: 1.767 | Val loss: 1.926 | Gen: eay arrrrrrray ondgngngngay isay aray\n",
      "Epoch:   3 | Train loss: 1.604 | Val loss: 1.792 | Gen: eay arraray ondndndndndgay isay oray\n",
      "Epoch:   4 | Train loss: 1.472 | Val loss: 1.658 | Gen: ehhhhhhhhhhhhhhhhhhh array ongngngngtay isay oray\n",
      "Epoch:   5 | Train loss: 1.363 | Val loss: 1.617 | Gen: eay arrarrrray ongngngngtay isay ongray\n",
      "Epoch:   6 | Train loss: 1.276 | Val loss: 1.607 | Gen: ehhhhhhhhhhhhhhhhhhh arrray ongay-ongay isay oray\n",
      "Epoch:   7 | Train loss: 1.207 | Val loss: 1.663 | Gen: eaaay arararay ongay-ingay isay oay\n",
      "Epoch:   8 | Train loss: 1.139 | Val loss: 1.594 | Gen: ethahhhhhhaaahhhhhah arrararray ongdidingay isay oray\n",
      "Epoch:   9 | Train loss: 1.104 | Val loss: 1.545 | Gen: etay arrray ongdingway isay oray\n",
      "Epoch:  10 | Train loss: 1.063 | Val loss: 1.490 | Gen: ethay arrrrway ondingway isay orgay\n",
      "Epoch:  11 | Train loss: 0.994 | Val loss: 1.598 | Gen: ethay arrrrrrray ongngtingway isay oray\n",
      "Epoch:  12 | Train loss: 0.949 | Val loss: 1.581 | Gen: etay arrrway ondingway isway oray\n",
      "Epoch:  13 | Train loss: 0.923 | Val loss: 1.413 | Gen: ethay arrrrray ondingay isay oray\n",
      "Epoch:  14 | Train loss: 0.903 | Val loss: 1.351 | Gen: eththay arrway ondongtitingway isway orkringwyway\n",
      "Epoch:  15 | Train loss: 0.840 | Val loss: 1.365 | Gen: ethay arrrriay ondontingway isway orray\n",
      "Epoch:  16 | Train loss: 0.800 | Val loss: 1.351 | Gen: ethay arrrrway ondidingway isway orringway\n",
      "Epoch:  17 | Train loss: 0.767 | Val loss: 1.263 | Gen: ethay arrrray ondoningway isway orrway\n",
      "Epoch:  18 | Train loss: 0.732 | Val loss: 1.256 | Gen: ethay arrrway ondoningwyiowyicay isway orkray\n",
      "Epoch:  19 | Train loss: 0.719 | Val loss: 1.275 | Gen: eththay arriway oondiniingway isway orkrigay\n",
      "Epoch:  20 | Train loss: 0.699 | Val loss: 1.244 | Gen: ethay arrriway ondoniotiongway isway orkrkray\n",
      "Epoch:  21 | Train loss: 0.661 | Val loss: 1.262 | Gen: ethay arirarway oondilingway isysway orkringay\n",
      "Epoch:  22 | Train loss: 0.641 | Val loss: 1.319 | Gen: eththay ariway oondinioingway isway orkway\n",
      "Epoch:  23 | Train loss: 0.626 | Val loss: 1.220 | Gen: ethay irarrway ondioinioicay isysway orkingay\n",
      "Epoch:  24 | Train loss: 0.592 | Val loss: 1.316 | Gen: eththay irarway ondoinioingway isway orkingway\n",
      "Epoch:  25 | Train loss: 0.584 | Val loss: 1.159 | Gen: ethay irrarway ondinioingicay isway orkingay\n",
      "Epoch:  26 | Train loss: 0.552 | Val loss: 1.179 | Gen: ethay ariway ondinioicay isway orkway\n",
      "Epoch:  27 | Train loss: 0.529 | Val loss: 1.178 | Gen: ethay irrarway ondidingway isway okrkiway\n",
      "Epoch:  28 | Train loss: 0.506 | Val loss: 1.169 | Gen: ethay arriway ondidingway isway okrkway\n",
      "Epoch:  29 | Train loss: 0.488 | Val loss: 1.136 | Gen: ethay arrraway ondidingway isway okrkray\n",
      "Epoch:  30 | Train loss: 0.469 | Val loss: 1.158 | Gen: ethay arraway ondidingway isway orkway\n",
      "Epoch:  31 | Train loss: 0.457 | Val loss: 1.124 | Gen: ethay arrraway ondidingway isway okrkrkway\n",
      "Epoch:  32 | Train loss: 0.442 | Val loss: 1.175 | Gen: etthay arraway ondidiongway isway okrkway\n",
      "Epoch:  33 | Train loss: 0.434 | Val loss: 1.110 | Gen: ethay arrraway ondidinicay isway okrkrkway\n",
      "Epoch:  34 | Train loss: 0.428 | Val loss: 1.168 | Gen: ethay arraway ondioingway isway orkwrkway\n",
      "Epoch:  35 | Train loss: 0.405 | Val loss: 1.109 | Gen: ethay arrraway ondidioingway isway orkrkway\n",
      "Epoch:  36 | Train loss: 0.389 | Val loss: 1.201 | Gen: ethay arrraway ondietingway isway orkrkway\n",
      "Epoch:  37 | Train loss: 0.379 | Val loss: 1.170 | Gen: ethay arrraway ondioingway isway owrkrkway\n",
      "Epoch:  38 | Train loss: 0.371 | Val loss: 1.230 | Gen: ethay arrrawawawawawawawaw ondidingway isway orrkwrkway\n",
      "Epoch:  39 | Train loss: 0.365 | Val loss: 1.241 | Gen: ethay arrrway ondidiongway isway okrkrkrkway\n",
      "Epoch:  40 | Train loss: 0.379 | Val loss: 1.249 | Gen: eway arrraway ondidingway isway orkrkrkway\n",
      "Epoch:  41 | Train loss: 0.359 | Val loss: 1.236 | Gen: ethay arrraway otindiongway isway orrkrkray\n",
      "Epoch:  42 | Train loss: 0.374 | Val loss: 1.045 | Gen: ethay arrway ondidingncay isway orkrkway\n",
      "Epoch:  43 | Train loss: 0.376 | Val loss: 1.173 | Gen: thay irarway ondiditingcay iway yrkray\n",
      "Epoch:  44 | Train loss: 0.404 | Val loss: 1.072 | Gen: ethay irarway ondidionticay isway orkwrkway\n",
      "Epoch:  45 | Train loss: 0.369 | Val loss: 1.042 | Gen: ethay irraway onditingicay isway orkrkingway\n",
      "Epoch:  46 | Train loss: 0.347 | Val loss: 1.153 | Gen: ethay arraway ondiniotay isway orrkway\n",
      "Epoch:  47 | Train loss: 0.367 | Val loss: 1.091 | Gen: ethay iraway ondiniingway isway orrkway\n",
      "Epoch:  48 | Train loss: 0.338 | Val loss: 1.041 | Gen: ethay irraway ondiniitingcay isway orrkway\n",
      "Epoch:  49 | Train loss: 0.302 | Val loss: 0.993 | Gen: ethay irraway ondidingincay isway orrkrkingway\n",
      "Epoch:  50 | Train loss: 0.280 | Val loss: 1.068 | Gen: ethay arriway ondinitingway isway orrkway\n",
      "Epoch:  51 | Train loss: 0.266 | Val loss: 0.988 | Gen: ethay irraway ondidingncay isway orrkwingway\n",
      "Epoch:  52 | Train loss: 0.248 | Val loss: 0.961 | Gen: ethay irraway ondidingcay isway orrkwingway\n",
      "Epoch:  53 | Train loss: 0.236 | Val loss: 0.998 | Gen: ethay irraway ondidingncay isway orrkwingway\n",
      "Epoch:  54 | Train loss: 0.229 | Val loss: 1.003 | Gen: ethay irraway ondidingcay isway orrkrkway\n",
      "Epoch:  55 | Train loss: 0.221 | Val loss: 1.002 | Gen: ethay irraway ondidingctingway isway orrkrkway\n",
      "Epoch:  56 | Train loss: 0.216 | Val loss: 1.018 | Gen: ethay arriway ondidingcay isway orrkrkway\n",
      "Epoch:  57 | Train loss: 0.208 | Val loss: 1.018 | Gen: ethay irraway ondidingctingway isway orrkrkway\n",
      "Epoch:  58 | Train loss: 0.207 | Val loss: 1.101 | Gen: ethay arriway ondidingcay isway orrkwingway\n",
      "Epoch:  59 | Train loss: 0.206 | Val loss: 1.033 | Gen: ethay arriway ondidingcay isway orrkrkway\n",
      "Epoch:  60 | Train loss: 0.205 | Val loss: 1.053 | Gen: ethay irraway ondiditingcay isway orrkrkway\n",
      "Epoch:  61 | Train loss: 0.211 | Val loss: 1.026 | Gen: ethay arriway onditingngcay isway orrkwingway\n",
      "Epoch:  62 | Train loss: 0.198 | Val loss: 0.995 | Gen: ethay irraway onditingcay isway orkrkrkingway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  63 | Train loss: 0.222 | Val loss: 1.018 | Gen: ethay arrwiway onditingcay isway orrkrkway\n",
      "Epoch:  64 | Train loss: 0.207 | Val loss: 1.235 | Gen: ethay arrraway onditingwincay isway orrkrkingway\n",
      "Epoch:  65 | Train loss: 0.285 | Val loss: 1.105 | Gen: ehay ariway oonitingway isway orkrkwingway\n",
      "Epoch:  66 | Train loss: 0.309 | Val loss: 1.218 | Gen: ethay irarway onditingtingcay isway orkrkrkway\n",
      "Epoch:  67 | Train loss: 0.304 | Val loss: 0.890 | Gen: ethay arriway onditiongngcay isway orkrkrkingway\n",
      "Epoch:  68 | Train loss: 0.260 | Val loss: 0.923 | Gen: ethay irarway onditingncay isway orkrkwingway\n",
      "Epoch:  69 | Train loss: 0.226 | Val loss: 1.279 | Gen: etway arwiway onditingcay isway orkrkway\n",
      "Epoch:  70 | Train loss: 0.246 | Val loss: 0.944 | Gen: ethay arrwiway onditiongcay isway orkrkwingway\n",
      "Epoch:  71 | Train loss: 0.192 | Val loss: 0.910 | Gen: ethay arriway onditingcay isway orkrkway\n",
      "Epoch:  72 | Train loss: 0.165 | Val loss: 0.870 | Gen: ethay arriway onditiongcay isway orkrkwingway\n",
      "Epoch:  73 | Train loss: 0.152 | Val loss: 0.875 | Gen: ethay arriway onditingcay isway orkrkwingway\n",
      "Epoch:  74 | Train loss: 0.144 | Val loss: 0.877 | Gen: ethay arriway onditingcay isway orkrkwingway\n",
      "Epoch:  75 | Train loss: 0.138 | Val loss: 0.878 | Gen: ethay arriway onditingcay isway orkrkwingway\n",
      "Epoch:  76 | Train loss: 0.133 | Val loss: 0.882 | Gen: ethay arriway onditiongcay isway orkrkwingway\n",
      "Epoch:  77 | Train loss: 0.128 | Val loss: 0.884 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  78 | Train loss: 0.124 | Val loss: 0.887 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  79 | Train loss: 0.120 | Val loss: 0.893 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  80 | Train loss: 0.116 | Val loss: 0.897 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  81 | Train loss: 0.112 | Val loss: 0.900 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  82 | Train loss: 0.109 | Val loss: 0.905 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  83 | Train loss: 0.106 | Val loss: 0.909 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  84 | Train loss: 0.103 | Val loss: 0.915 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  85 | Train loss: 0.100 | Val loss: 0.915 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  86 | Train loss: 0.096 | Val loss: 0.924 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  87 | Train loss: 0.093 | Val loss: 0.921 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  88 | Train loss: 0.090 | Val loss: 0.931 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  89 | Train loss: 0.087 | Val loss: 0.938 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  90 | Train loss: 0.084 | Val loss: 0.943 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  91 | Train loss: 0.082 | Val loss: 0.954 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  92 | Train loss: 0.080 | Val loss: 0.979 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  93 | Train loss: 0.081 | Val loss: 0.969 | Gen: ethay arriway onditiongcay isway orkrkingway\n",
      "Epoch:  94 | Train loss: 0.075 | Val loss: 0.969 | Gen: ethay arriway onditiongcay isway orkwingway\n",
      "Epoch:  95 | Train loss: 0.072 | Val loss: 0.974 | Gen: ethay arriway onditiongcay isway orkwingway\n",
      "Epoch:  96 | Train loss: 0.069 | Val loss: 0.980 | Gen: ethay arriway onditiongcay isway orkwingway\n",
      "Epoch:  97 | Train loss: 0.067 | Val loss: 0.992 | Gen: ethay arriway onditiongcay isway orkwingway\n",
      "Epoch:  98 | Train loss: 0.064 | Val loss: 1.000 | Gen: ethay arriway onditiongcay isway orkwingway\n",
      "Epoch:  99 | Train loss: 0.062 | Val loss: 1.015 | Gen: ethay arriway onditiongcay isway orkwingway\n",
      "Obtained lowest validation loss of: 0.8699115142226219\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay arriway onditiongcay isway orkwingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans32_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 100,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 100,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 4,\n",
    "}\n",
    "trans32_args_s.update(args_dict)\n",
    "print_opts(trans32_args_s)\n",
    "\n",
    "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "l28mKuZxvaRT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay arriway onditiongcay isway orkwingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L8EqLYFu48H"
   },
   "source": [
    "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "FdZO69DozuUu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 10                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('universally', 'universallyway')\n",
      "('earnings', 'earningsway')\n",
      "('ardour', 'ardourway')\n",
      "('scissors', 'issorsscay')\n",
      "('damaged', 'amagedday')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.771 | Val loss: 2.318 | Gen: ay ay iinay-ay ay ogay-gay\n",
      "Epoch:   1 | Train loss: 2.061 | Val loss: 2.102 | Gen: eththththththwththth ay intintintintintintin sististiswtisway ogeway-ay\n",
      "Epoch:   2 | Train loss: 1.849 | Val loss: 1.983 | Gen: ehthhhththththththth inay intintintintintintin iststististisy odingeway\n",
      "Epoch:   3 | Train loss: 1.703 | Val loss: 1.905 | Gen: ethththththththththt iay ontintintintintintin isstisway odingeway-gay\n",
      "Epoch:   4 | Train loss: 1.585 | Val loss: 1.868 | Gen: ethththththththththt iay ontintintintintintin isssway odingeway-ingay\n",
      "Epoch:   5 | Train loss: 1.486 | Val loss: 1.829 | Gen: eththhththwhway iay ontintintintingtingt issway odingeway-ingay\n",
      "Epoch:   6 | Train loss: 1.410 | Val loss: 1.841 | Gen: eththay aiay ontintintintingay issway oingeway-oday-ingay\n",
      "Epoch:   7 | Train loss: 1.374 | Val loss: 1.723 | Gen: etthay iatay ontindindtingay isway odgay-ingway\n",
      "Epoch:   8 | Train loss: 1.284 | Val loss: 1.722 | Gen: eththay iaray intindndintingay issway odingeway-ogay\n",
      "Epoch:   9 | Train loss: 1.211 | Val loss: 1.714 | Gen: eththay iaray ontintintiongtingtay issway oingeway-ingway\n",
      "Epoch:  10 | Train loss: 1.163 | Val loss: 1.678 | Gen: ethay iaray ontindintingay-ingay issway oingeway\n",
      "Epoch:  11 | Train loss: 1.103 | Val loss: 1.598 | Gen: ethay ariay ontindindtnay-indind isway oodingay\n",
      "Epoch:  12 | Train loss: 1.030 | Val loss: 1.610 | Gen: ethay ariay ontindintionay-ingay isay oingrgay-ay\n",
      "Epoch:  13 | Train loss: 0.985 | Val loss: 1.583 | Gen: ethay iaray ontintiontnay-iongay isway oingrgay-ay\n",
      "Epoch:  14 | Train loss: 0.939 | Val loss: 1.603 | Gen: ethay iaray onintiontiongay isway oingringay\n",
      "Epoch:  15 | Train loss: 0.931 | Val loss: 1.569 | Gen: etay ariay ontindiontiongay isway oodingway\n",
      "Epoch:  16 | Train loss: 0.877 | Val loss: 1.608 | Gen: ethay aiaway ondintiontiongay isway oingrigway\n",
      "Epoch:  17 | Train loss: 0.829 | Val loss: 1.584 | Gen: ethay iaray ontionmtintiongay isway oirdingay\n",
      "Epoch:  18 | Train loss: 0.792 | Val loss: 1.611 | Gen: ethay iariay ontionitiongray isway oiriggway\n",
      "Epoch:  19 | Train loss: 0.754 | Val loss: 1.604 | Gen: ethay iaray onditiongcay isway oiriggway\n",
      "Epoch:  20 | Train loss: 0.724 | Val loss: 1.638 | Gen: ethay iraiay onditiontiongray isway oiriggway\n",
      "Epoch:  21 | Train loss: 0.697 | Val loss: 1.607 | Gen: ethay iraiay onditiongcay isway oirigway\n",
      "Epoch:  22 | Train loss: 0.672 | Val loss: 1.629 | Gen: ethay iraiway onditiongcay isway oirigway\n",
      "Epoch:  23 | Train loss: 0.654 | Val loss: 1.680 | Gen: etheway iraiway onditiongcay isway oiringway\n",
      "Epoch:  24 | Train loss: 0.637 | Val loss: 1.664 | Gen: ethay iraiway onditiongcay isway oiringway\n",
      "Epoch:  25 | Train loss: 0.634 | Val loss: 1.606 | Gen: ethay iraiway onditiongcay isway oiringgway\n",
      "Validation loss has not improved in 10 epochs, stopping early\n",
      "Obtained lowest validation loss of: 1.5691886745669223\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay iraiway onditingcay isway oiriggway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans32_args_l = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 100,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 512,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 3,\n",
    "}\n",
    "trans32_args_l.update(args_dict)\n",
    "print_opts(trans32_args_l)\n",
    "\n",
    "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "SmoTgrDcr_dw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 20                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('abridgement', 'abridgementway')\n",
      "('delicacies', 'elicaciesday')\n",
      "('served', 'ervedsay')\n",
      "('us', 'usway')\n",
      "('simpson', 'impsonsay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.476 | Val loss: 2.002 | Gen: etay iingin inday isssssssssssssssssss ingay\n",
      "Epoch:   1 | Train loss: 1.737 | Val loss: 1.809 | Gen: etay-ay ingay onday-onday-onday isssssssssay oray-ong-ay\n",
      "Epoch:   2 | Train loss: 1.477 | Val loss: 1.588 | Gen: athay iway onday-onay issay ooay-ongray\n",
      "Epoch:   3 | Train loss: 1.294 | Val loss: 1.714 | Gen: eay iway inday-inday isay oray-ingway\n",
      "Epoch:   4 | Train loss: 1.216 | Val loss: 1.477 | Gen: thay iway ondainay-onay issay oaray-ingway\n",
      "Epoch:   5 | Train loss: 1.034 | Val loss: 1.349 | Gen: ethay iray ondintiningingingngn isay oringway\n",
      "Epoch:   6 | Train loss: 0.887 | Val loss: 1.279 | Gen: etay iray ontiningingay isway oriray-ingway\n",
      "Epoch:   7 | Train loss: 0.770 | Val loss: 1.147 | Gen: ethay aray ontiningay-intingnga isway orkingway\n",
      "Epoch:   8 | Train loss: 0.671 | Val loss: 1.152 | Gen: hay iray ondintingingay isway orkingway\n",
      "Epoch:   9 | Train loss: 0.639 | Val loss: 1.101 | Gen: ethay away ontiniiongingay isway-ay orkingray\n",
      "Epoch:  10 | Train loss: 0.587 | Val loss: 1.216 | Gen: ehay aray ondioniongay-ingay isway orkingray-oway\n",
      "Epoch:  11 | Train loss: 0.529 | Val loss: 1.208 | Gen: ehay irway andingionginay iway orkingray\n",
      "Epoch:  12 | Train loss: 0.455 | Val loss: 1.075 | Gen: ethay aray ondingingnay isway orkingray\n",
      "Epoch:  13 | Train loss: 0.400 | Val loss: 1.039 | Gen: ehay airway ondiniingay isway orkinggway\n",
      "Epoch:  14 | Train loss: 0.358 | Val loss: 1.090 | Gen: ehay airway ondidiongay isway orkinggway\n",
      "Epoch:  15 | Train loss: 0.346 | Val loss: 0.976 | Gen: ethay iawaray ondinitingnay isway orkingway\n",
      "Epoch:  16 | Train loss: 0.288 | Val loss: 0.908 | Gen: ethay irway ondidiondingway isway orkingway\n",
      "Epoch:  17 | Train loss: 0.249 | Val loss: 0.905 | Gen: ehtay awarway ondiniongcay isway owrkingway\n",
      "Epoch:  18 | Train loss: 0.216 | Val loss: 0.952 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  19 | Train loss: 0.198 | Val loss: 0.868 | Gen: ethay airway onditioniongcay isway orkingway\n",
      "Epoch:  20 | Train loss: 0.170 | Val loss: 0.967 | Gen: ethay airway onditiionginingway isway orkingway\n",
      "Epoch:  21 | Train loss: 0.161 | Val loss: 0.874 | Gen: ethay aiwrwwwway onditioningcay isway orkingway\n",
      "Epoch:  22 | Train loss: 0.137 | Val loss: 0.881 | Gen: ethay awrwwwwwway onditioningcay isway orkingway\n",
      "Epoch:  23 | Train loss: 0.120 | Val loss: 0.852 | Gen: ethay iaway ondiniongcay issway orkingway\n",
      "Epoch:  24 | Train loss: 0.124 | Val loss: 1.062 | Gen: ethay awarway onditioningingway isway orkingway\n",
      "Epoch:  25 | Train loss: 0.142 | Val loss: 0.968 | Gen: ethay iway onditionctingcay isway owkigngway\n",
      "Epoch:  26 | Train loss: 0.189 | Val loss: 0.903 | Gen: ethay awrwwwway onditiningway isway orkingway\n",
      "Epoch:  27 | Train loss: 0.154 | Val loss: 0.976 | Gen: ethay aiwarwway ondiniongcay isway orkingway\n",
      "Epoch:  28 | Train loss: 0.148 | Val loss: 0.898 | Gen: ethay awrwwwway onditiongingcay isway orkingwbway\n",
      "Epoch:  29 | Train loss: 0.119 | Val loss: 0.822 | Gen: ethay aiway onditionnngnay isway orkingway\n",
      "Epoch:  30 | Train loss: 0.094 | Val loss: 0.720 | Gen: ethay awrway onditiningtingcay isway orkingwray\n",
      "Epoch:  31 | Train loss: 0.083 | Val loss: 0.777 | Gen: ethay arway onditiongnay isway orkingway\n",
      "Epoch:  32 | Train loss: 0.088 | Val loss: 0.757 | Gen: ethay awrwwway onditiongcay isway orkingwrway\n",
      "Epoch:  33 | Train loss: 0.072 | Val loss: 0.826 | Gen: ethay ariway onditiongnay isway orkingway\n",
      "Epoch:  34 | Train loss: 0.064 | Val loss: 0.747 | Gen: ethay awarway onditiongcay isway orkingway\n",
      "Epoch:  35 | Train loss: 0.059 | Val loss: 0.826 | Gen: ethay arway onditiongingcay isway orkingwray\n",
      "Epoch:  36 | Train loss: 0.051 | Val loss: 0.707 | Gen: ethay aiwrwwway onditiongcay isway orkingway\n",
      "Epoch:  37 | Train loss: 0.034 | Val loss: 0.791 | Gen: ethay aiwrwway onditiongcay isway orkingwray\n",
      "Epoch:  38 | Train loss: 0.033 | Val loss: 0.717 | Gen: ethay airway ondidiongcay isway orkingway\n",
      "Epoch:  39 | Train loss: 0.028 | Val loss: 0.798 | Gen: ethay arway ondidiongingcay isway orkingway\n",
      "Epoch:  40 | Train loss: 0.029 | Val loss: 0.788 | Gen: ethay arway onditiongcay isway orkigwray\n",
      "Epoch:  41 | Train loss: 0.031 | Val loss: 0.842 | Gen: ethay arway onditiongingcay isway orkingway\n",
      "Epoch:  42 | Train loss: 0.037 | Val loss: 0.771 | Gen: ethay airwway onditiongcay isway orkingway\n",
      "Epoch:  43 | Train loss: 0.034 | Val loss: 0.879 | Gen: ethay aiwrway onditiongingcay isway orkingway\n",
      "Epoch:  44 | Train loss: 0.048 | Val loss: 0.814 | Gen: ethay arway onditiongcay isway orkingway\n",
      "Epoch:  45 | Train loss: 0.060 | Val loss: 0.910 | Gen: ethay aiwarway onditiongingway isway orkingway\n",
      "Epoch:  46 | Train loss: 0.088 | Val loss: 0.936 | Gen: ethay aiway onditiongcay isway orrkigway\n",
      "Epoch:  47 | Train loss: 0.072 | Val loss: 0.831 | Gen: ethay aiway onditioningday isway orkingway\n",
      "Epoch:  48 | Train loss: 0.080 | Val loss: 0.977 | Gen: ethay aiway onditioingingingway isway orkingway\n",
      "Epoch:  49 | Train loss: 0.096 | Val loss: 0.756 | Gen: ethay aiwayway onditionncay isway okingpay\n",
      "Obtained lowest validation loss of: 0.707227385179563\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay aiwayway onditionncay isway okingpay\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans64_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,  # Increased model size\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 3,\n",
    "}\n",
    "trans64_args_s.update(args_dict)\n",
    "print_opts(trans64_args_s)\n",
    "\n",
    "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dardK4RWvUWV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 20                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('universally', 'universallyway')\n",
      "('earnings', 'earningsway')\n",
      "('ardour', 'ardourway')\n",
      "('scissors', 'issorsscay')\n",
      "('damaged', 'amagedday')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.432 | Val loss: 2.035 | Gen: eteay iay ingingnway iway ongingway-inway-ingw\n",
      "Epoch:   1 | Train loss: 1.731 | Val loss: 1.813 | Gen: eteteay iway ingway-ingway iway oway-igway\n",
      "Epoch:   2 | Train loss: 1.502 | Val loss: 1.716 | Gen: etetetay ay ingdgdgdgdgdwndway isway orgway-igway\n",
      "Epoch:   3 | Train loss: 1.335 | Val loss: 1.680 | Gen: eththay ay intintingntinay isway orgrgay\n",
      "Epoch:   4 | Train loss: 1.230 | Val loss: 1.668 | Gen: eway ay ongingndongnday isway orgray\n",
      "Epoch:   5 | Train loss: 1.130 | Val loss: 1.514 | Gen: ethay ay otindintintinay isway orkingray\n",
      "Epoch:   6 | Train loss: 1.015 | Val loss: 1.444 | Gen: ethay ayway ingdingay isway orgay\n",
      "Epoch:   7 | Train loss: 0.936 | Val loss: 1.479 | Gen: ethay arway otintiningngnay isway orkway\n",
      "Epoch:   8 | Train loss: 0.878 | Val loss: 1.288 | Gen: ethay irway otingay isway orkway\n",
      "Epoch:   9 | Train loss: 0.768 | Val loss: 1.257 | Gen: ethay airay onditingay isway orkigray\n",
      "Epoch:  10 | Train loss: 0.689 | Val loss: 1.274 | Gen: ethay arway onditingngcay isway orkway\n",
      "Epoch:  11 | Train loss: 0.624 | Val loss: 1.153 | Gen: ethay arway ondintingngway isway orkingray\n",
      "Epoch:  12 | Train loss: 0.552 | Val loss: 1.066 | Gen: ethay airway onditingngcay isway orkingray\n",
      "Epoch:  13 | Train loss: 0.516 | Val loss: 0.987 | Gen: ethay airway onditingctingway isway orkingway\n",
      "Epoch:  14 | Train loss: 0.451 | Val loss: 0.981 | Gen: ethay aiway onditingningway isway orkingway\n",
      "Epoch:  15 | Train loss: 0.409 | Val loss: 1.052 | Gen: ethay airwayway onditiniongway isway orkingway\n",
      "Epoch:  16 | Train loss: 0.352 | Val loss: 0.981 | Gen: ehtay ariwayway onditingingway isway orkingway\n",
      "Epoch:  17 | Train loss: 0.324 | Val loss: 0.934 | Gen: ethay airway onditiningnay isway orkingway\n",
      "Epoch:  18 | Train loss: 0.334 | Val loss: 1.009 | Gen: ehtay airway onditingingsingway isway orkingway\n",
      "Epoch:  19 | Train loss: 0.420 | Val loss: 1.056 | Gen: ethay irwiwayway onditiningtingway isway orkingway\n",
      "Epoch:  20 | Train loss: 0.368 | Val loss: 0.832 | Gen: ethay arwayway onditingingway isway orkingway\n",
      "Epoch:  21 | Train loss: 0.275 | Val loss: 0.807 | Gen: ehtay arway onditingcingcay isway orkingway\n",
      "Epoch:  22 | Train loss: 0.245 | Val loss: 0.802 | Gen: ethay ariway ondititongngway isway orkingway\n",
      "Epoch:  23 | Train loss: 0.222 | Val loss: 0.846 | Gen: ethay airwayway onditiongcingcay isway orkingway\n",
      "Epoch:  24 | Train loss: 0.182 | Val loss: 0.794 | Gen: ethay airway onditiongingngcay isway orkingway\n",
      "Epoch:  25 | Train loss: 0.149 | Val loss: 0.778 | Gen: ethay airway onditioniongscay isway orkingway\n",
      "Epoch:  26 | Train loss: 0.132 | Val loss: 0.794 | Gen: ethay airway onditiningcingway isway orkingway\n",
      "Epoch:  27 | Train loss: 0.121 | Val loss: 0.769 | Gen: ethay airway onditiontiongcay isway orkingway\n",
      "Epoch:  28 | Train loss: 0.109 | Val loss: 0.784 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  29 | Train loss: 0.095 | Val loss: 0.773 | Gen: ethay airway onditiontingcay isway orkingway\n",
      "Epoch:  30 | Train loss: 0.085 | Val loss: 0.758 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  31 | Train loss: 0.073 | Val loss: 0.769 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  32 | Train loss: 0.068 | Val loss: 0.782 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  33 | Train loss: 0.092 | Val loss: 1.493 | Gen: ethay airway onditioningcay issway orkingway\n",
      "Epoch:  34 | Train loss: 0.459 | Val loss: 1.235 | Gen: ehthay irwdraEOSay onditioningcincay issswy okingkinwy\n",
      "Epoch:  35 | Train loss: 0.404 | Val loss: 0.827 | Gen: ethay airway ondititingcay issway orkingway\n",
      "Epoch:  36 | Train loss: 0.247 | Val loss: 0.657 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  37 | Train loss: 0.132 | Val loss: 0.614 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  38 | Train loss: 0.091 | Val loss: 0.604 | Gen: ethay airway onditioningcay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans64_args_l = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 512,\n",
    "    \"hidden_size\": 64,  # Increased model size\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 3,\n",
    "}\n",
    "trans64_args_l.update(args_dict)\n",
    "print_opts(trans64_args_l)\n",
    "\n",
    "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSSyiG39vVlN"
   },
   "source": [
    "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ql0pxrEvVP6"
   },
   "outputs": [],
   "source": [
    "save_loss_comparison_by_dataset(\n",
    "    trans32_losses_s,\n",
    "    trans32_losses_l,\n",
    "    trans64_losses_s,\n",
    "    trans64_losses_l,\n",
    "    trans32_args_s,\n",
    "    trans32_args_l,\n",
    "    trans64_args_s,\n",
    "    trans64_args_l,\n",
    "    \"trans_by_dataset\",\n",
    ")\n",
    "save_loss_comparison_by_hidden(\n",
    "    trans32_losses_s,\n",
    "    trans32_losses_l,\n",
    "    trans64_losses_s,\n",
    "    trans64_losses_l,\n",
    "    trans32_args_s,\n",
    "    trans32_args_l,\n",
    "    trans64_args_s,\n",
    "    trans64_args_l,\n",
    "    \"trans_by_hidden\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "TjPTaRB4mpCd",
    "s9IS9B9-yUU5",
    "9DaTdRNuUra7",
    "4BIpGwANoQOg",
    "pbvpn4MaV0I1",
    "bRWfRdmVVjUl",
    "0yh08KhgnA30",
    "YDYMr7NclZdw",
    "dCae1mOUlZrC",
    "ecEq4TP2lZ4Z",
    "TSDTbsydlaGI",
    "RWwA6OGqlaTq",
    "AJSafHSAmu_w",
    "73_p8d5EmvOJ",
    "vYPae08Io1Fi",
    "xq7nhsEio1w-",
    "unReAOrjo113",
    "ZkjHbtvT6Qxs",
    "B7gJLw5t_rnW",
    "9tcpUFKqo2Oi",
    "29ZjkXTNrUKb"
   ],
   "name": "nmt.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
