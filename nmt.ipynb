{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjPTaRB4mpCd"
   },
   "source": [
    "# Colab FAQ\n",
    "\n",
    "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
    "\n",
    "You need to use the colab GPU for this assignment by selecting:\n",
    "\n",
    "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9IS9B9-yUU5"
   },
   "source": [
    "# Setup PyTorch\n",
    "\n",
    "All files will be stored at /content/csc421/a3/ folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-6MQhMOlHXD"
   },
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Setup python environment and change the current working directory\n",
    "######################################################################\n",
    "!pip install Pillow\n",
    "%mkdir -p ./content/csc421/a3/\n",
    "%cd ./content/csc421/a3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DaTdRNuUra7"
   },
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BIpGwANoQOg"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D-UJHBYZkh7f"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_file(\n",
    "    fname, origin, untar=False, extract=False, archive_format=\"auto\", cache_dir=\"data\"\n",
    "):\n",
    "    datadir = os.path.join(cache_dir)\n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "\n",
    "    if untar:\n",
    "        untar_fpath = os.path.join(datadir, fname)\n",
    "        fpath = untar_fpath + \".tar.gz\"\n",
    "    else:\n",
    "        fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    print(fpath)\n",
    "    if not os.path.exists(fpath):\n",
    "        print(\"Downloading data from\", origin)\n",
    "\n",
    "        error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    if untar:\n",
    "        if not os.path.exists(untar_fpath):\n",
    "            print(\"Extracting file.\")\n",
    "            with tarfile.open(fpath) as archive:\n",
    "                archive.extractall(datadir)\n",
    "        return untar_fpath\n",
    "\n",
    "    if extract:\n",
    "        _extract_archive(fpath, datadir, archive_format)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "def to_var(tensor, cuda):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "    Arguments:\n",
    "        tensor: A Tensor object.\n",
    "        cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "    Returns:\n",
    "        A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "def create_dir_if_not_exists(directory):\n",
    "    \"\"\"Creates a directory if it doesn't already exist.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save_loss_plot(train_losses, val_losses, opts):\n",
    "    \"\"\"Saves a plot of the training and validation loss curves.\"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(val_losses)), val_losses)\n",
    "    plt.title(\"BS={}, nhid={}\".format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
    "    plt.xlabel(\"Epochs\", fontsize=16)\n",
    "    plt.ylabel(\"Loss\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(opts.checkpoint_path, \"loss_plot.pdf\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_loss_comparison_gru(l1, l2, o1, o2, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and val loss curves from GRU runs.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    ax[0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
    "    ax[0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
    "    ax[0].title.set_text(\"Train Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
    "    ax[1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
    "    ax[1].title.set_text(\"Val Loss | GRU Hidden Size = {}\".format(o2.hidden_size))\n",
    "\n",
    "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
    "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
    "    ax[0].legend(loc=\"upper right\")\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"GRU Performance by Dataset\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    plt.legend()\n",
    "\n",
    "    plt_path = \"./loss_plot_{}.pdf\".format(fn)\n",
    "    plt.savefig(plt_path)\n",
    "    print(f\"Plot saved to: {Path(plt_path).resolve()}\")\n",
    "\n",
    "\n",
    "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
    "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
    "\n",
    "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        l3: Tuple of lists containing training / val losses for model 3.\n",
    "        l4: Tuple of lists containing training / val losses for model 4.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        o3: Options for model 3.\n",
    "        o4: Options for model 4.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
    "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
    "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"ds=\" + o1.data_file_name)\n",
    "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label=\"ds=\" + o2.data_file_name)\n",
    "    ax[0][0].title.set_text(\n",
    "        \"Train Loss | Model Hidden Size = {}\".format(o1.hidden_size)\n",
    "    )\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"ds=\" + o1.data_file_name)\n",
    "    ax[0][1].plot(range(len(l2[1])), l2[1], label=\"ds=\" + o2.data_file_name)\n",
    "    ax[0][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o1.hidden_size))\n",
    "\n",
    "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label=\"ds=\" + o3.data_file_name)\n",
    "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"ds=\" + o4.data_file_name)\n",
    "    ax[1][0].title.set_text(\n",
    "        \"Train Loss | Model Hidden Size = {}\".format(o3.hidden_size)\n",
    "    )\n",
    "\n",
    "    ax[1][1].plot(range(len(l3[1])), l3[1], label=\"ds=\" + o3.data_file_name)\n",
    "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"ds=\" + o4.data_file_name)\n",
    "    ax[1][1].title.set_text(\"Val Loss | Model Hidden Size = {}\".format(o4.hidden_size))\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][0].legend(loc=\"upper right\")\n",
    "        ax[i][1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
    "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
    "\n",
    "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        l3: Tuple of lists containing training / val losses for model 3.\n",
    "        l4: Tuple of lists containing training / val losses for model 4.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        o3: Options for model 3.\n",
    "        o4: Options for model 4.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i * s : (i + 1) * s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i * s : (i + 1) * s]) for i in range(len(l2[0]) // s)]\n",
    "    mean_l3 = [np.mean(l3[0][i * s : (i + 1) * s]) for i in range(len(l3[0]) // s)]\n",
    "    mean_l4 = [np.mean(l4[0][i * s : (i + 1) * s]) for i in range(len(l4[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label=\"hid_size=\" + str(o1.hidden_size))\n",
    "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label=\"hid_size=\" + str(o3.hidden_size))\n",
    "    ax[0][0].title.set_text(\"Train Loss | Dataset = \" + o1.data_file_name)\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[0][1].plot(range(len(l1[1])), l1[1], label=\"hid_size=\" + str(o1.hidden_size))\n",
    "    ax[0][1].plot(range(len(l3[1])), l3[1], label=\"hid_size=\" + str(o3.hidden_size))\n",
    "    ax[0][1].title.set_text(\"Val Loss | Dataset = \" + o1.data_file_name)\n",
    "\n",
    "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label=\"hid_size=\" + str(o2.hidden_size))\n",
    "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label=\"hid_size=\" + str(o4.hidden_size))\n",
    "    ax[1][0].title.set_text(\"Train Loss | Dataset = \" + o3.data_file_name)\n",
    "\n",
    "    ax[1][1].plot(range(len(l2[1])), l2[1], label=\"hid_size=\" + str(o2.hidden_size))\n",
    "    ax[1][1].plot(range(len(l4[1])), l4[1], label=\"hid_size=\" + str(o4.hidden_size))\n",
    "    ax[1][1].title.set_text(\"Val Loss | Dataset = \" + o4.data_file_name)\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][0].legend(loc=\"upper right\")\n",
    "        ax[i][1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./loss_plot_{}.pdf\".format(fn))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def checkpoint(encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
    "    contains the char_to_index and index_to_char mappings, and the start_token\n",
    "    and end_token values.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(opts.checkpoint_path, \"encoder.pt\"), \"wb\") as f:\n",
    "        torch.save(encoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, \"decoder.pt\"), \"wb\") as f:\n",
    "        torch.save(decoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, \"idx_dict.pkl\"), \"wb\") as f:\n",
    "        pkl.dump(idx_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbvpn4MaV0I1"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XVT4TNTOV3Eg"
   },
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    \"\"\"Read a file and split it into lines.\"\"\"\n",
    "    lines = open(filename).read().strip().lower().split(\"\\n\")\n",
    "    return lines\n",
    "\n",
    "\n",
    "def read_pairs(filename):\n",
    "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
    "\n",
    "    Returns:\n",
    "        source_words: A list of the first word in each line of the file.\n",
    "        target_words: A list of the second word in each line of the file.\n",
    "    \"\"\"\n",
    "    lines = read_lines(filename)\n",
    "    source_words, target_words = [], []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            source, target = line.split()\n",
    "            source_words.append(source)\n",
    "            target_words.append(target)\n",
    "    return source_words, target_words\n",
    "\n",
    "\n",
    "def all_alpha_or_dash(s):\n",
    "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\"\"\"\n",
    "    return all(c.isalpha() or c == \"-\" for c in s)\n",
    "\n",
    "\n",
    "def filter_lines(lines):\n",
    "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\"\"\"\n",
    "    return [line for line in lines if all_alpha_or_dash(line)]\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
    "    path = \"./data/{}.txt\".format(file_name)\n",
    "    source_lines, target_lines = read_pairs(path)\n",
    "\n",
    "    # Filter lines\n",
    "    source_lines = filter_lines(source_lines)\n",
    "    target_lines = filter_lines(target_lines)\n",
    "\n",
    "    all_characters = set(\"\".join(source_lines)) | set(\"\".join(target_lines))\n",
    "\n",
    "    # Create a dictionary mapping each character to a unique index\n",
    "    char_to_index = {\n",
    "        char: index for (index, char) in enumerate(sorted(list(all_characters)))\n",
    "    }\n",
    "\n",
    "    # Add start and end tokens to the dictionary\n",
    "    start_token = len(char_to_index)\n",
    "    end_token = len(char_to_index) + 1\n",
    "    char_to_index[\"SOS\"] = start_token\n",
    "    char_to_index[\"EOS\"] = end_token\n",
    "\n",
    "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
    "    index_to_char = {index: char for (char, index) in char_to_index.items()}\n",
    "\n",
    "    # Store the final size of the vocabulary\n",
    "    vocab_size = len(char_to_index)\n",
    "\n",
    "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
    "\n",
    "    idx_dict = {\n",
    "        \"char_to_index\": char_to_index,\n",
    "        \"index_to_char\": index_to_char,\n",
    "        \"start_token\": start_token,\n",
    "        \"end_token\": end_token,\n",
    "    }\n",
    "\n",
    "    return line_pairs, vocab_size, idx_dict\n",
    "\n",
    "\n",
    "def create_dict(pairs):\n",
    "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
    "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
    "    all source indexes and the other containing all corresponding target indexes.\n",
    "    Within a batch, all the source words are the same length, and all the target words are\n",
    "    the same length.\n",
    "    \"\"\"\n",
    "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
    "\n",
    "    d = defaultdict(list)\n",
    "    for (s, t) in unique_pairs:\n",
    "        d[(len(s), len(t))].append((s, t))\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRWfRdmVVjUl"
   },
   "source": [
    "## Training and evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wa5-onJhoSeM"
   },
   "outputs": [],
   "source": [
    "def string_to_index_list(s, char_to_index, end_token):\n",
    "    \"\"\"Converts a sentence into a list of indexes (for each character).\"\"\"\n",
    "    return [char_to_index[char] for char in s] + [\n",
    "        end_token\n",
    "    ]  # Adds the end token to each index list\n",
    "\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
    "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
    "    word independently, and then stitching the words back together with spaces between them.\n",
    "    \"\"\"\n",
    "    if idx_dict is None:\n",
    "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
    "    return \" \".join(\n",
    "        [translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()]\n",
    "    )\n",
    "\n",
    "\n",
    "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a given string from English to Pig-Latin.\"\"\"\n",
    "\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "    index_to_char = idx_dict[\"index_to_char\"]\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = \"\"\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(\n",
    "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
    "    )  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_last_hidden\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "        ## slow decoding, recompute everything at each time\n",
    "        decoder_outputs, attention_weights = decoder(\n",
    "            decoder_inputs, encoder_annotations, decoder_hidden\n",
    "        )\n",
    "\n",
    "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "        ni = ni[-1]  # latest output token\n",
    "\n",
    "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "\n",
    "        if ni == end_token:\n",
    "            break\n",
    "        else:\n",
    "            gen_string = \"\".join(\n",
    "                [\n",
    "                    index_to_char[int(item)]\n",
    "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\"\"\"\n",
    "    if idx_dict is None:\n",
    "        line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "    index_to_char = idx_dict[\"index_to_char\"]\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = \"\"\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(\n",
    "        torch.LongTensor(indexes).unsqueeze(0), opts.cuda\n",
    "    )  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_hidden = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    produced_end_token = False\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "        ## slow decoding, recompute everything at each time\n",
    "        decoder_outputs, attention_weights = decoder(\n",
    "            decoder_inputs, encoder_annotations, decoder_hidden\n",
    "        )\n",
    "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "        ni = ni[-1]  # latest output token\n",
    "\n",
    "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "\n",
    "        if ni == end_token:\n",
    "            break\n",
    "        else:\n",
    "            gen_string = \"\".join(\n",
    "                [\n",
    "                    index_to_char[int(item)]\n",
    "                    for item in generated_words.cpu().numpy().reshape(-1)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    if isinstance(attention_weights, tuple):\n",
    "        ## transformer's attention mweights\n",
    "        attention_weights, self_attention_weights = attention_weights\n",
    "\n",
    "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
    "\n",
    "    for i in range(len(all_attention_weights)):\n",
    "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(attention_weights_matrix, cmap=\"bone\")\n",
    "        fig.colorbar(cax)\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_yticklabels([\"\"] + list(input_string) + [\"EOS\"], rotation=90)\n",
    "        ax.set_xticklabels(\n",
    "            [\"\"] + list(gen_string) + ([\"EOS\"] if produced_end_token else [])\n",
    "        )\n",
    "\n",
    "        # Show label at every tick\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        # Add title\n",
    "        plt.xlabel(\"Attention weights to the source sentence in layer {}\".format(i + 1))\n",
    "        plt.tight_layout()\n",
    "        plt.grid(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
    "    \"\"\"Train/Evaluate the model on a dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model.\n",
    "        opts: The command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        mean_loss: The average loss over all batches from data_dict.\n",
    "    \"\"\"\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "\n",
    "    losses = []\n",
    "    for key in data_dict:\n",
    "        input_strings, target_strings = zip(*data_dict[key])\n",
    "        input_tensors = [\n",
    "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
    "            for s in input_strings\n",
    "        ]\n",
    "        target_tensors = [\n",
    "            torch.LongTensor(string_to_index_list(s, char_to_index, end_token))\n",
    "            for s in target_strings\n",
    "        ]\n",
    "\n",
    "        num_tensors = len(input_tensors)\n",
    "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start = i * opts.batch_size\n",
    "            end = start + opts.batch_size\n",
    "\n",
    "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
    "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
    "\n",
    "            # The batch size may be different in each epoch\n",
    "            BS = inputs.size(0)\n",
    "\n",
    "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
    "\n",
    "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "\n",
    "            start_vector = (\n",
    "                torch.ones(BS).long().unsqueeze(1) * start_token\n",
    "            )  # BS x 1 --> 16x1  CHECKED\n",
    "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
    "\n",
    "            decoder_inputs = torch.cat(\n",
    "                [decoder_input, targets[:, 0:-1]], dim=1\n",
    "            )  # Gets decoder inputs by shifting the targets to the right\n",
    "\n",
    "            decoder_outputs, attention_weights = decoder(\n",
    "                decoder_inputs, encoder_annotations, decoder_hidden\n",
    "            )\n",
    "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
    "            targets_flatten = targets.view(-1)\n",
    "\n",
    "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            ## training if an optimizer is provided\n",
    "            if optimizer:\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                # Update the parameters of the encoder and decoder\n",
    "                optimizer.step()\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
    "):\n",
    "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
    "        * Prints training and val loss each epoch.\n",
    "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
    "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
    "        * Returns loss curves for comparison\n",
    "\n",
    "    Arguments:\n",
    "        train_dict: The training word pairs, organized by source and target lengths.\n",
    "        val_dict: The validation word pairs, organized by source and target lengths.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
    "        opts: The command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        losses: Lists containing training and validation loss curves.\n",
    "    \"\"\"\n",
    "\n",
    "    start_token = idx_dict[\"start_token\"]\n",
    "    end_token = idx_dict[\"end_token\"]\n",
    "    char_to_index = idx_dict[\"char_to_index\"]\n",
    "\n",
    "    loss_log = open(os.path.join(opts.checkpoint_path, \"loss_log.txt\"), \"w\")\n",
    "\n",
    "    best_val_loss = 1e6\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    mean_train_losses = []\n",
    "    mean_val_losses = []\n",
    "\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(opts.nepochs):\n",
    "\n",
    "        optimizer.param_groups[0][\"lr\"] *= opts.lr_decay\n",
    "\n",
    "        train_loss = compute_loss(\n",
    "            train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts\n",
    "        )\n",
    "        val_loss = compute_loss(\n",
    "            val_dict, encoder, decoder, idx_dict, criterion, None, opts\n",
    "        )\n",
    "\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            checkpoint(encoder, decoder, idx_dict, opts)\n",
    "            best_val_loss = mean_val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter > opts.early_stopping_patience:\n",
    "            print(\n",
    "                \"Validation loss has not improved in {} epochs, stopping early\".format(\n",
    "                    opts.early_stopping_patience\n",
    "                )\n",
    "            )\n",
    "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
    "            return (train_losses, mean_val_losses)\n",
    "\n",
    "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
    "        print(\n",
    "            \"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(\n",
    "                epoch, mean_train_loss, mean_val_loss, gen_string\n",
    "            )\n",
    "        )\n",
    "\n",
    "        loss_log.write(\"{} {} {}\\n\".format(epoch, train_loss, val_loss))\n",
    "        loss_log.flush()\n",
    "\n",
    "        train_losses += train_loss\n",
    "        val_losses += val_loss\n",
    "\n",
    "        mean_train_losses.append(mean_train_loss)\n",
    "        mean_val_losses.append(mean_val_loss)\n",
    "\n",
    "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
    "\n",
    "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
    "    return (train_losses, mean_val_losses)\n",
    "\n",
    "\n",
    "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
    "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Data Stats\".center(80))\n",
    "    print(\"-\" * 80)\n",
    "    for pair in line_pairs[:5]:\n",
    "        print(pair)\n",
    "    print(\"Num unique word pairs: {}\".format(len(line_pairs)))\n",
    "    print(\"Vocabulary: {}\".format(idx_dict[\"char_to_index\"].keys()))\n",
    "    print(\"Vocab size: {}\".format(vocab_size))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "def train(opts):\n",
    "    line_pairs, vocab_size, idx_dict = load_data(opts[\"data_file_name\"])\n",
    "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
    "\n",
    "    # Split the line pairs into an 80% train and 20% val split\n",
    "    num_lines = len(line_pairs)\n",
    "    num_train = int(0.8 * num_lines)\n",
    "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
    "\n",
    "    # Group the data by the lengths of the source and target words, to form batches\n",
    "    train_dict = create_dict(train_pairs)\n",
    "    val_dict = create_dict(val_pairs)\n",
    "\n",
    "    ##########################################################################\n",
    "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
    "    ##########################################################################\n",
    "    if opts.encoder_type == \"rnn\":\n",
    "        encoder = GRUEncoder(\n",
    "            vocab_size=vocab_size, hidden_size=opts.hidden_size, opts=opts\n",
    "        )\n",
    "    elif opts.encoder_type == \"transformer\":\n",
    "        encoder = TransformerEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            num_layers=opts.num_transformer_layers,\n",
    "            opts=opts,\n",
    "        )\n",
    "    elif opts.encoder_type == \"attention\":\n",
    "      encoder = AttentionEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            opts=opts,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if opts.decoder_type == \"rnn\":\n",
    "        decoder = RNNDecoder(vocab_size=vocab_size, hidden_size=opts.hidden_size)\n",
    "    elif opts.decoder_type == \"rnn_attention\":\n",
    "        decoder = RNNAttentionDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            attention_type=opts.attention_type,\n",
    "        )\n",
    "    elif opts.decoder_type == \"transformer\":\n",
    "        decoder = TransformerDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "            num_layers=opts.num_transformer_layers,\n",
    "        )\n",
    "    elif opts.encoder_type == \"attention\":\n",
    "      decoder = AttentionDecoder(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=opts.hidden_size,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    #### setup checkpoint path\n",
    "    model_name = \"h{}-bs{}-{}-{}\".format(\n",
    "        opts.hidden_size, opts.batch_size, opts.decoder_type, opts.data_file_name\n",
    "    )\n",
    "    opts.checkpoint_path = model_name\n",
    "    create_dir_if_not_exists(opts.checkpoint_path)\n",
    "    ####\n",
    "\n",
    "    if opts.cuda:\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "        print(\"Moved models to GPU!\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        losses = training_loop(\n",
    "            train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts\n",
    "        )\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting early from training.\")\n",
    "        return encoder, decoder, losses\n",
    "\n",
    "    return encoder, decoder, losses\n",
    "\n",
    "\n",
    "def print_opts(opts):\n",
    "    \"\"\"Prints the values of all command-line arguments.\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Opts\".center(80))\n",
    "    print(\"-\" * 80)\n",
    "    for key in opts.__dict__:\n",
    "        print(\"{:>30}: {:<30}\".format(key, opts.__dict__[key]).center(80))\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yh08KhgnA30"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aROU2xZanDKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pig_latin_small.txt\n",
      "data/pig_latin_large.txt\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Download Translation datasets\n",
    "######################################################################\n",
    "data_fpath = get_file(\n",
    "    fname=\"pig_latin_small.txt\",\n",
    "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_small.txt\",\n",
    "    untar=False,\n",
    ")\n",
    "\n",
    "data_fpath = get_file(\n",
    "    fname=\"pig_latin_large.txt\",\n",
    "    origin=\"http://www.cs.toronto.edu/~jba/pig_latin_large.txt\",\n",
    "    untar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDYMr7NclZdw"
   },
   "source": [
    "# Part 1: Neural machine translation (NMT)\n",
    "\n",
    "In this section, you will implement a Gated Recurrent Unit (GRU) cell, a common type of recurrent neural network (RNN). The GRU cell is a simplification of the Long Short-Term Memory cell. Therefore, we have provided you with an implemented LSTM cell (`MyLSTMCell`), which you can reference when completing `MyGRUCell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cOnALRQkkjDO"
   },
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Wif = nn.Linear(input_size, hidden_size)\n",
    "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wii = nn.Linear(input_size, hidden_size)\n",
    "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wic = nn.Linear(input_size, hidden_size)\n",
    "        self.Whc = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wio = nn.Linear(input_size, hidden_size)\n",
    "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
    "\n",
    "        Arguments\n",
    "            x: batch_size x input_size\n",
    "            h_prev: batch_size x hidden_size\n",
    "            c_prev: batch_size x hidden_size\n",
    "\n",
    "        Returns:\n",
    "            h_new: batch_size x hidden_size\n",
    "            c_new: batch_size x hidden_size\n",
    "        \"\"\"\n",
    "\n",
    "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
    "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
    "\n",
    "        c = torch.tanh(self.Wic(x) + self.Whc(h_prev))\n",
    "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
    "\n",
    "        c_new = f * c_prev + i * c\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCae1mOUlZrC"
   },
   "source": [
    "## Step 1: GRU Cell\n",
    "Please implement the `MyGRUCell` class defined in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DGyxqZIQzTJH"
   },
   "outputs": [],
   "source": [
    "class MyGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyGRUCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        # Input linear layers\n",
    "        self.Wiz = nn.Linear(input_size, hidden_size)\n",
    "        self.Wir = nn.Linear(input_size, hidden_size)\n",
    "        self.Wih = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "\n",
    "        # Hidden linear layers\n",
    "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"Forward pass of the GRU computation for one time step.\n",
    "\n",
    "        Arguments\n",
    "            x: batch_size x input_size\n",
    "            h_prev: batch_size x hidden_size\n",
    "\n",
    "        Returns:\n",
    "            h_new: batch_size x hidden_size\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        z = torch.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
    "        r = torch.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
    "        h = torch.tanh(self.Wih(x) + self.Whh(r*h_prev))\n",
    "        h_new = (1-z)*h_prev + z*h\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecEq4TP2lZ4Z"
   },
   "source": [
    "## Step 2: GRU Encoder\n",
    "\n",
    "The following cells use your `MyGRUCell` implementation to build a recurrent encoder and decoder. Please read the implementations to understand what they do and run the cells before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8jDNim2fmVJV"
   },
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, opts):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = MyGRUCell(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "        annotations = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x = encoded[:, i, :]  # Get the current time step, across the whole batch\n",
    "            hidden = self.gru(x, hidden)\n",
    "            annotations.append(hidden)\n",
    "\n",
    "        annotations = torch.stack(annotations, dim=1)\n",
    "        return annotations, hidden\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
    "        of a batch of sequences.\n",
    "\n",
    "        Arguments:\n",
    "            bs: The batch size for the initial hidden state.\n",
    "\n",
    "        Returns:\n",
    "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HvwizYM9ma4p"
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = MyGRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
    "            annotations: This is not used here. It just maintains consistency with the\n",
    "                    interface used by the AttentionDecoder class.\n",
    "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            None\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        hiddens = []\n",
    "        h_prev = hidden_init\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x = embed[\n",
    "                :, i, :\n",
    "            ]  # Get the current time step input tokens, across the whole batch\n",
    "            h_prev = self.rnn(x, h_prev)  # batch_size x hidden_size\n",
    "            hiddens.append(h_prev)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSDTbsydlaGI"
   },
   "source": [
    "## Step 3: Training and Analysis\n",
    "\n",
    "Train the encoder-decoder model to perform English --> Pig Latin translation. We will start by training on the smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XmVuXTozTPF7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 20                                     \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn                                    \n",
      "                         attention_type:                                        \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('destination', 'estinationday')\n",
      "('surveying', 'urveyingsay')\n",
      "('declaring', 'eclaringday')\n",
      "('conditioned', 'onditionedcay')\n",
      "('vindication', 'indicationvay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.273 | Val loss: 2.064 | Gen: alay-onsay-onsay-ons ay onsay-onsay-onsay-on alay-onsay-onsay-ons onsay-onsay-onsay-on\n",
      "Epoch:   1 | Train loss: 1.899 | Val loss: 1.915 | Gen: alay-onsay alay onsay-onsay insay-onsay onsay-onsay\n",
      "Epoch:   2 | Train loss: 1.749 | Val loss: 1.840 | Gen: atay alay onday-onday inway onday\n",
      "Epoch:   3 | Train loss: 1.643 | Val loss: 1.761 | Gen: eway atay oondway-onday-ortay iway-ortay oodgay-ontay\n",
      "Epoch:   4 | Train loss: 1.555 | Val loss: 1.702 | Gen: eway alay oondsay-onday-ontay- iway-ortay oodgay-ontay-otay\n",
      "Epoch:   5 | Train loss: 1.488 | Val loss: 1.696 | Gen: eway aray-otay-otay-otay- oondsay-ondendway-or iway-ationsay oondway-ortay-ontay-\n",
      "Epoch:   6 | Train loss: 1.438 | Val loss: 1.672 | Gen: eway areday-otay-otay ontingpray-ondenday- iway-otay-otay ondedgay-ortay-otay\n",
      "Epoch:   7 | Train loss: 1.386 | Val loss: 1.639 | Gen: eway areday-atedway ondillway-otedway inway-ationsay oondgay-orsay-ontay-\n",
      "Epoch:   8 | Train loss: 1.344 | Val loss: 1.683 | Gen: eway array-ortedway ontingpedsay-ondedga inway oongrendgay-ondedgay\n",
      "Epoch:   9 | Train loss: 1.312 | Val loss: 1.586 | Gen: eway array-ortay-oday-ate ondingssay-athray-ot inway overway-iedway\n",
      "Epoch:  10 | Train loss: 1.259 | Val loss: 1.538 | Gen: eway aray-othingsay ondingsway inway ongray-indway-atured\n",
      "Epoch:  11 | Train loss: 1.213 | Val loss: 1.539 | Gen: eway away-etay onsiblingway insway overway-away-etay\n",
      "Epoch:  12 | Train loss: 1.174 | Val loss: 1.526 | Gen: eway away-etay onsibleway insway overway-away-etay\n",
      "Epoch:  13 | Train loss: 1.151 | Val loss: 1.575 | Gen: eway away-etay ondingway-axtay insay overway\n",
      "Epoch:  14 | Train loss: 1.144 | Val loss: 1.507 | Gen: eway aitedway oncondilingway insay overway-iedway\n",
      "Epoch:  15 | Train loss: 1.103 | Val loss: 1.462 | Gen: eway aintray onsingnomay-inway-aw inway overway-awlay\n",
      "Epoch:  16 | Train loss: 1.067 | Val loss: 1.458 | Gen: ecepay aintray onsicepay-away-eteta iway overway-ieday\n",
      "Epoch:  17 | Train loss: 1.043 | Val loss: 1.454 | Gen: eway aitedway onconcinciway-ieday iway overway-ieday\n",
      "Epoch:  18 | Train loss: 1.032 | Val loss: 1.456 | Gen: eceplay aitredway onsioushay inedway overway-iecepay\n",
      "Epoch:  19 | Train loss: 1.011 | Val loss: 1.432 | Gen: ecay aintray oncimplaintedsay inecay overway-iedway\n",
      "Epoch:  20 | Train loss: 0.972 | Val loss: 1.410 | Gen: eway aiterway onsicecingway iway overway-iedway\n",
      "Epoch:  21 | Train loss: 0.946 | Val loss: 1.440 | Gen: ecay aitredway onciwingway iway overionsay\n",
      "Epoch:  22 | Train loss: 0.930 | Val loss: 1.413 | Gen: ecepay aitredway onsicompay-away-etet iway overway-ientway\n",
      "Epoch:  23 | Train loss: 0.923 | Val loss: 1.444 | Gen: ectedway aintray onsicomplationsday inesway oromprerionsway\n",
      "Epoch:  24 | Train loss: 0.920 | Val loss: 1.451 | Gen: ecepay aritedway onsicomplationcay iway overway-ientway\n",
      "Epoch:  25 | Train loss: 0.923 | Val loss: 1.441 | Gen: ectay away-eteray-away-ete onconcompleableway inesway ormonsway-atedray\n",
      "Epoch:  26 | Train loss: 0.907 | Val loss: 1.427 | Gen: ecepay aingray onsiconcepay-away-et insway oversionsway\n",
      "Epoch:  27 | Train loss: 0.894 | Val loss: 1.429 | Gen: ecepay aingray oncompomplepay-oday inesway overway-ientway\n",
      "Epoch:  28 | Train loss: 0.879 | Val loss: 1.423 | Gen: ectay avorway oncompliway ienway ormonway-aturedway\n",
      "Epoch:  29 | Train loss: 0.860 | Val loss: 1.410 | Gen: ectedway aintray onconcompay-away-ete inedsay ounguringway\n",
      "Epoch:  30 | Train loss: 0.835 | Val loss: 1.408 | Gen: ecepay avorway-ybay onconcomplearway-yba inedway orvenigingway\n",
      "Epoch:  31 | Train loss: 0.839 | Val loss: 1.465 | Gen: ectay aintray oncioustlay iway orviringway\n",
      "Epoch:  32 | Train loss: 0.834 | Val loss: 1.476 | Gen: eceplay aintray onsictioncay inesway orveringway\n",
      "Epoch:  33 | Train loss: 0.825 | Val loss: 1.430 | Gen: ectay avourway oncinciecay-away-ete inesway ounguringway\n",
      "Epoch:  34 | Train loss: 0.815 | Val loss: 1.455 | Gen: ecepay avorway-oteday onconcompay-abletay inedway ourmonway-aturednay\n",
      "Epoch:  35 | Train loss: 0.814 | Val loss: 1.422 | Gen: ectedway avourway onconcoullay inedway orverway-indway-inwa\n",
      "Epoch:  36 | Train loss: 0.794 | Val loss: 1.413 | Gen: ectycay aideray oncioncepay-away-ete inedway orvirway-aturednay\n",
      "Epoch:  37 | Train loss: 0.778 | Val loss: 1.422 | Gen: ectedway avoway-etray-abletay oncompoumentway ientway orviringway\n",
      "Epoch:  38 | Train loss: 0.762 | Val loss: 1.396 | Gen: ecepay aideray oncioncepay-away-ete inedway orvisonay\n",
      "Epoch:  39 | Train loss: 0.762 | Val loss: 1.542 | Gen: ectay avorway-ybay oncioncepationcay inedsay orrfuringway\n",
      "Epoch:  40 | Train loss: 0.780 | Val loss: 1.393 | Gen: ectay aideray oncinciway-iecepay inesway orommonedway\n",
      "Epoch:  41 | Train loss: 0.752 | Val loss: 1.488 | Gen: ectedway avoway-etray-abletay onciway-away-etetay iway orviringway\n",
      "Epoch:  42 | Train loss: 0.764 | Val loss: 1.446 | Gen: ececay aindray-ybay oncioncientway inesway ormonway-aturednay\n",
      "Epoch:  43 | Train loss: 0.753 | Val loss: 1.426 | Gen: eceplay avoway-etrationcay oncioncepalitionsway inessway orvingway-awlay-away\n",
      "Epoch:  44 | Train loss: 0.746 | Val loss: 1.458 | Gen: ececay avoway-etray inciousthoway inedway oursibleway\n",
      "Epoch:  45 | Train loss: 0.739 | Val loss: 1.508 | Gen: epray abidray oncioncepay-atedway inesssay orkingroachway\n",
      "Epoch:  46 | Train loss: 0.770 | Val loss: 1.490 | Gen: ecepay aingray-ybay inconcondingway inessway oversonableway\n",
      "Epoch:  47 | Train loss: 0.730 | Val loss: 1.426 | Gen: eceplay aiderway oncioncepay-away-ete insway ormonway-aturednay\n",
      "Epoch:  48 | Train loss: 0.703 | Val loss: 1.461 | Gen: ectedcay aideray onciontemplay inesway orkingray-oulhay\n",
      "Epoch:  49 | Train loss: 0.692 | Val loss: 1.443 | Gen: ecepay aideray oncioncepalicay inesway orvigeningway\n",
      "Obtained lowest validation loss of: 1.3932414062139464\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tecepay aideray oncioncepalicay inesway orvigeningway\n",
      "train time  187.7792046070099\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "rnn_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
    "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
    "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
    "}\n",
    "rnn_args_s.update(args_dict)\n",
    "\n",
    "print_opts(rnn_args_s)\n",
    "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"train time \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mR97V_NtER6"
   },
   "source": [
    "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
    "\n",
    "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a quick and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "H3YLrAjsmx_W"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 10                                     \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn                                    \n",
      "                         attention_type:                                        \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('exotic', 'exoticway')\n",
      "('wrung', 'ungwray')\n",
      "('fiat', 'iatfay')\n",
      "('declaring', 'eclaringday')\n",
      "('development', 'evelopmentday')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.315 | Val loss: 2.042 | Gen: ertay-ay ay-ay ongway-ay intay-ay ongway-ay\n",
      "Epoch:   1 | Train loss: 1.884 | Val loss: 1.922 | Gen: enteray-ay-ay-ay-ay- ay-ay-ay-ay-ay-ay-ay ongway-ay-ay-ay-ay-a intertay-ay-ay-ay-ay ongway-ay-ay-ay-ay-a\n",
      "Epoch:   2 | Train loss: 1.741 | Val loss: 1.842 | Gen: enteray-away away ongray-onteray-away intertay-away ongray-onteray-away\n",
      "Epoch:   3 | Train loss: 1.628 | Val loss: 1.772 | Gen: eteray away onglay-ayday interay-away ongray-ayday\n",
      "Epoch:   4 | Train loss: 1.528 | Val loss: 1.713 | Gen: eteray away ongray-away-away illway-away ongray-ayday\n",
      "Epoch:   5 | Train loss: 1.439 | Val loss: 1.664 | Gen: etay away ongray-orteray-away itay-ayday ortay-ortay-away-awa\n",
      "Epoch:   6 | Train loss: 1.376 | Val loss: 1.619 | Gen: eteray away ongrousergray-intera istay orgray-awlay\n",
      "Epoch:   7 | Train loss: 1.316 | Val loss: 1.575 | Gen: eteray away ontionaningway istay ortingray-interay-aw\n",
      "Epoch:   8 | Train loss: 1.255 | Val loss: 1.535 | Gen: eway away onglay-interay istay ortingway-inway-ayda\n",
      "Epoch:   9 | Train loss: 1.206 | Val loss: 1.514 | Gen: etay away onsingleteray istay ortingray-awlay\n",
      "Epoch:  10 | Train loss: 1.158 | Val loss: 1.506 | Gen: eway aiway onsingleway-inway-ac istay ortingray-inway-awla\n",
      "Epoch:  11 | Train loss: 1.129 | Val loss: 1.507 | Gen: eway aiway intergray-inway-awla isway ortingway-awlay\n",
      "Epoch:  12 | Train loss: 1.096 | Val loss: 1.479 | Gen: eway aiway ondingsay-inway-awla isway ortingray-awlay\n",
      "Epoch:  13 | Train loss: 1.068 | Val loss: 1.453 | Gen: eway iarway ontingray-inway isway ongray-away\n",
      "Epoch:  14 | Train loss: 1.040 | Val loss: 1.400 | Gen: eway aiway ontingray-inway-ache isay ongray-awlay\n",
      "Epoch:  15 | Train loss: 0.992 | Val loss: 1.373 | Gen: eway aiway onilingsay-inway-awl isay ongray-awlay\n",
      "Epoch:  16 | Train loss: 0.966 | Val loss: 1.383 | Gen: eway aiway ontingsay-inway-awla isay ongray\n",
      "Epoch:  17 | Train loss: 0.958 | Val loss: 1.348 | Gen: eway aiway onsingsay-inway-awla isway ongray-ybay\n",
      "Epoch:  18 | Train loss: 0.943 | Val loss: 1.380 | Gen: eway aiway inoweray-inway-ached isay ongray\n",
      "Epoch:  19 | Train loss: 0.928 | Val loss: 1.351 | Gen: eway aiway ontingsay-inway-awla issay ongray\n",
      "Epoch:  20 | Train loss: 0.901 | Val loss: 1.306 | Gen: ethay aiway onsingsay isway ongray-awlay\n",
      "Epoch:  21 | Train loss: 0.873 | Val loss: 1.289 | Gen: eway aiway ontingsay-inway-awla istay ongray-awlay\n",
      "Epoch:  22 | Train loss: 0.884 | Val loss: 1.404 | Gen: eway aiway ontingay-inway istay ongray-yearay\n",
      "Epoch:  23 | Train loss: 0.904 | Val loss: 1.309 | Gen: eway aiway inonsingnay istay ongray\n",
      "Epoch:  24 | Train loss: 0.848 | Val loss: 1.329 | Gen: eway airay ingroirsay-inway-awl isway origfray\n",
      "Epoch:  25 | Train loss: 0.818 | Val loss: 1.240 | Gen: eway aiway ontingsay isway ortingbay\n",
      "Epoch:  26 | Train loss: 0.786 | Val loss: 1.228 | Gen: eway aiway ontingscay isway ortingbay\n",
      "Epoch:  27 | Train loss: 0.778 | Val loss: 1.251 | Gen: eway airway inotingsay-othingnay istay orkingnay\n",
      "Epoch:  28 | Train loss: 0.773 | Val loss: 1.226 | Gen: eway aiway inosingsray isway orignetay\n",
      "Epoch:  29 | Train loss: 0.767 | Val loss: 1.221 | Gen: eway aiway oningsray-inway-arss isway orkingnay\n",
      "Epoch:  30 | Train loss: 0.758 | Val loss: 1.252 | Gen: eway airway introunationgray istay orkingnay\n",
      "Epoch:  31 | Train loss: 0.772 | Val loss: 1.271 | Gen: eway aiway onsingsay-inway-awla istay orbingnay\n",
      "Epoch:  32 | Train loss: 0.760 | Val loss: 1.172 | Gen: ehway aiway ontingscay isway orkingnay\n",
      "Epoch:  33 | Train loss: 0.725 | Val loss: 1.180 | Gen: ehway airway onsingscay isway oringnay\n",
      "Epoch:  34 | Train loss: 0.696 | Val loss: 1.174 | Gen: eway aiway ontingscay istay orkingnay\n",
      "Epoch:  35 | Train loss: 0.683 | Val loss: 1.148 | Gen: eway aiway onsingscay istay orkingnay\n",
      "Epoch:  36 | Train loss: 0.676 | Val loss: 1.183 | Gen: eway aiway onsicingshay isway orignetway\n",
      "Epoch:  37 | Train loss: 0.673 | Val loss: 1.161 | Gen: eway aiway onsingsray isway orkingnay\n",
      "Epoch:  38 | Train loss: 0.672 | Val loss: 1.190 | Gen: eway airway ontifingcay isway orkingnay\n",
      "Epoch:  39 | Train loss: 0.665 | Val loss: 1.191 | Gen: ehay aiway onsilingnay isway ongingway\n",
      "Epoch:  40 | Train loss: 0.656 | Val loss: 1.157 | Gen: ehay airway onintificationsway isway ongingway\n",
      "Epoch:  41 | Train loss: 0.639 | Val loss: 1.164 | Gen: ehay aiway inoningscay-ousehay isway ongingway\n",
      "Epoch:  42 | Train loss: 0.630 | Val loss: 1.152 | Gen: ehay airway oningingsmay isway ongignetway\n",
      "Epoch:  43 | Train loss: 0.620 | Val loss: 1.209 | Gen: eway airway onitificationsway isway orkingway\n",
      "Epoch:  44 | Train loss: 0.634 | Val loss: 1.194 | Gen: ehay aiway oningingmay isway ongingway\n",
      "Epoch:  45 | Train loss: 0.632 | Val loss: 1.212 | Gen: eway airway ontingsay isway orkingnay\n",
      "Validation loss has not improved in 10 epochs, stopping early\n",
      "Obtained lowest validation loss of: 1.1484168402428896\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tehay aiway ontingscay isway orkingway\n",
      "train time  206.1423110961914\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "rnn_args_l = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_large\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"batch_size\": 512,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"rnn\",  # options: rnn / transformer\n",
    "    \"decoder_type\": \"rnn\",  # options: rnn / rnn_attention / transformer\n",
    "    \"attention_type\": \"\",   # options: additive / scaled_dot\n",
    "}\n",
    "rnn_args_l.update(args_dict)\n",
    "\n",
    "print_opts(rnn_args_l)\n",
    "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"train time \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01HsZ6EItc56"
   },
   "source": [
    "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Are there significant differences in the validation performance of each model? (see follow-up questions in handout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qyk_9-Fwtekj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to: /home/maryamebr/anaconda3/envs/new-tf-gpu/CSC2516_PA3/loss_plot_gru.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_loss_comparison_gru(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, \"gru\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE4ijaCzneAt"
   },
   "source": [
    "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it (see follow-up questions in handout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WrNnz8W1nULf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tehay aiway ontingscay isway orkingway\n",
      "source:\t\texceptional flavor \n",
      "translated:\texpcerpay-achesway avoway\n",
      "source:\t\tonomatopoeia is hard \n",
      "translated:\tomponationcay isway adway\n"
     ]
    }
   ],
   "source": [
    "best_encoder = rnn_encode_l  # Replace with rnn_encode_s or rnn_encode_l\n",
    "best_decoder = rnn_decoder_l  # Replace with rnn_decoder_s or rnn_decoder_l\n",
    "best_args = rnn_args_l     # Replace with rnn_args_s or rnn_args_l\n",
    "\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "\n",
    "\n",
    "### long and rare words\n",
    "\n",
    "TEST_SENTENCE = \"exceptional flavor\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "\n",
    "\n",
    "TEST_SENTENCE = \"onomatopoeia is hard\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, best_encoder, best_decoder, None, best_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWwA6OGqlaTq"
   },
   "source": [
    "# Part 2: Attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJSafHSAmu_w"
   },
   "source": [
    "## Step 1: Additive attention\n",
    "\n",
    "In the next cell, the [additive attention](https://paperswithcode.com/method/additive-attention) mechanism has been implemented for you. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AdewEVSMo5jJ"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # A two layer fully-connected network\n",
    "        # hidden_size * 2 --> hidden_size, ReLU, hidden_size --> 1\n",
    "        self.attention_network = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the additive attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "        batch_size = keys.size(0)\n",
    "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(\n",
    "            keys\n",
    "        )\n",
    "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
    "        unnormalized_attention = self.attention_network(concat_inputs)\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(2, 1), values)\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73_p8d5EmvOJ"
   },
   "source": [
    "## Step 2: RNN + additive attention\n",
    "\n",
    "In the next cell, a modification of our `RNNDecoder` that makes use of an additive attention mechanism as been implemented for your. Please take a momement to read through it and understand what it is doing. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RJaABkXrpJSw"
   },
   "outputs": [],
   "source": [
    "class RNNAttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, attention_type=\"scaled_dot\"):\n",
    "        super(RNNAttentionDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.rnn = MyGRUCell(input_size=hidden_size * 2, hidden_size=hidden_size)\n",
    "        if attention_type == \"additive\":\n",
    "            self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
    "        elif attention_type == \"scaled_dot\":\n",
    "            self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        hiddens = []\n",
    "        attentions = []\n",
    "        h_prev = hidden_init\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            embed_current = embed[\n",
    "                :, i, :\n",
    "            ]  # Get the current time step, across the whole batch\n",
    "            context, attention_weights = self.attention(\n",
    "                h_prev, annotations, annotations\n",
    "            )  # batch_size x 1 x hidden_size\n",
    "            embed_and_context = torch.cat(\n",
    "                [embed_current, context.squeeze(1)], dim=1\n",
    "            )  # batch_size x (2*hidden_size)\n",
    "            h_prev = self.rnn(embed_and_context, h_prev)  # batch_size x hidden_size\n",
    "\n",
    "            hiddens.append(h_prev)\n",
    "            attentions.append(attention_weights)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1)  # batch_size x seq_len x hidden_size\n",
    "        attentions = torch.cat(attentions, dim=2)  # batch_size x seq_len x seq_len\n",
    "\n",
    "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
    "        return output, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYPae08Io1Fi"
   },
   "source": [
    "## Step 3: Training and analysis (with additive attention)\n",
    "\n",
    "Now, run the following cell to train our recurrent encoder-decoder model with additive attention. How does it perform compared to the recurrent encoder-decoder model without attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ke6t6rCezpZV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 10                                     \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn_attention                          \n",
      "                         attention_type: additive                               \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('destination', 'estinationday')\n",
      "('surveying', 'urveyingsay')\n",
      "('declaring', 'eclaringday')\n",
      "('conditioned', 'onditionedcay')\n",
      "('vindication', 'indicationvay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 1.970 | Val loss: 1.800 | Gen: ay-otay ay ondingay isay onday\n",
      "Epoch:   1 | Train loss: 1.464 | Val loss: 1.550 | Gen: etetay away otionday issay odgay-odgay-odgay-od\n",
      "Epoch:   2 | Train loss: 1.155 | Val loss: 1.432 | Gen: etay away-arway otay-otingsay issay orway-ourway-ingway\n",
      "Epoch:   3 | Train loss: 1.000 | Val loss: 1.513 | Gen: tay-otay-otay ariray onfay issay orray\n",
      "Epoch:   4 | Train loss: 0.883 | Val loss: 1.143 | Gen: ethay away-arway ondiondiondiondingsa isway orkngway\n",
      "Epoch:   5 | Train loss: 0.712 | Val loss: 1.024 | Gen: etay away-iway-away-away- onditiongcay isway ownay-angway\n",
      "Epoch:   6 | Train loss: 0.565 | Val loss: 1.006 | Gen: etay aiway-away onditingcay isway owngway-ingway-ab\n",
      "Epoch:   7 | Train loss: 0.483 | Val loss: 0.835 | Gen: eethay away onditiongcay issay orkingway\n",
      "Epoch:   8 | Train loss: 0.402 | Val loss: 0.940 | Gen: ecay airway onditionmay issway orkingway-ingway-ing\n",
      "Epoch:   9 | Train loss: 0.353 | Val loss: 0.889 | Gen: ethay away onditnationgcay issay orkngway\n",
      "Epoch:  10 | Train loss: 0.258 | Val loss: 0.622 | Gen: ethay airway onditiongcay issay orkingway-agngway-ag\n",
      "Epoch:  11 | Train loss: 0.202 | Val loss: 0.719 | Gen: ehthay away ondiongay isway owngway\n",
      "Epoch:  12 | Train loss: 0.189 | Val loss: 0.794 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  13 | Train loss: 0.174 | Val loss: 0.559 | Gen: ehay airway onditioncay issay orkingway\n",
      "Epoch:  14 | Train loss: 0.139 | Val loss: 0.513 | Gen: ethay airway onditingcay issay orkingway\n",
      "Epoch:  15 | Train loss: 0.115 | Val loss: 0.565 | Gen: ethay airway onditingway isway orkingway\n",
      "Epoch:  16 | Train loss: 0.122 | Val loss: 0.619 | Gen: ethay airway onditioncay isway orkingway-ingway-ing\n",
      "Epoch:  17 | Train loss: 0.103 | Val loss: 0.876 | Gen: ethay airway onditingcay isway okingway\n",
      "Epoch:  18 | Train loss: 0.124 | Val loss: 0.471 | Gen: ethay airway onditioncay issay orkingway\n",
      "Epoch:  19 | Train loss: 0.088 | Val loss: 0.474 | Gen: ethay airway ondiotiongcay isway orkingway\n",
      "Epoch:  20 | Train loss: 0.062 | Val loss: 0.709 | Gen: ethay airay ondititiongay isway orkingway\n",
      "Epoch:  21 | Train loss: 0.064 | Val loss: 0.444 | Gen: ethay airway onditiongay issway orkingway\n",
      "Epoch:  22 | Train loss: 0.039 | Val loss: 0.409 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  23 | Train loss: 0.029 | Val loss: 0.452 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  24 | Train loss: 0.021 | Val loss: 0.374 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  25 | Train loss: 0.019 | Val loss: 0.346 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  26 | Train loss: 0.013 | Val loss: 0.348 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  27 | Train loss: 0.009 | Val loss: 0.356 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  28 | Train loss: 0.007 | Val loss: 0.370 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  29 | Train loss: 0.006 | Val loss: 0.358 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  30 | Train loss: 0.005 | Val loss: 0.359 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  31 | Train loss: 0.004 | Val loss: 0.344 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  32 | Train loss: 0.003 | Val loss: 0.347 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  33 | Train loss: 0.003 | Val loss: 0.349 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  34 | Train loss: 0.002 | Val loss: 0.346 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  35 | Train loss: 0.002 | Val loss: 0.345 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  36 | Train loss: 0.002 | Val loss: 0.343 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  37 | Train loss: 0.002 | Val loss: 0.342 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  38 | Train loss: 0.002 | Val loss: 0.342 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  39 | Train loss: 0.001 | Val loss: 0.342 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  40 | Train loss: 0.001 | Val loss: 0.342 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  41 | Train loss: 0.001 | Val loss: 0.342 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  42 | Train loss: 0.001 | Val loss: 0.342 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  43 | Train loss: 0.001 | Val loss: 0.343 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  44 | Train loss: 0.001 | Val loss: 0.343 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  45 | Train loss: 0.001 | Val loss: 0.344 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  46 | Train loss: 0.001 | Val loss: 0.344 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  47 | Train loss: 0.001 | Val loss: 0.344 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  48 | Train loss: 0.001 | Val loss: 0.345 | Gen: ethay airway onditiongay isway orkingway\n",
      "Epoch:  49 | Train loss: 0.001 | Val loss: 0.345 | Gen: ethay airway onditiongay isway orkingway\n",
      "Obtained lowest validation loss of: 0.341950689686644\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditiongay isway orkingway\n",
      "train time  223.85989665985107\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "rnn_attn_args = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,\n",
    "    \"encoder_type\": \"rnn\",            # options: rnn / transformer\n",
    "    \"decoder_type\": \"rnn_attention\",  # options: rnn / rnn_attention / transformer\n",
    "    \"attention_type\": \"additive\",     # options: additive / scaled_dot\n",
    "}\n",
    "rnn_attn_args.update(args_dict)\n",
    "\n",
    "print_opts(rnn_attn_args)\n",
    "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n",
    "print(\"train time \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VNVKbLc0ACj_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditiongay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xq7nhsEio1w-"
   },
   "source": [
    "## Step 4: Implement scaled dot-product attention\n",
    "\n",
    "In the next cell, you will implement the [scaled dot-product attention](https://paperswithcode.com/method/scaled) mechanism. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d_j3oY3hqsJQ"
   },
   "outputs": [],
   "source": [
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(\n",
    "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        batch_size = keys.size(0)\n",
    "        \n",
    "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
    "        k = self.K(keys).view(batch_size, -1, self.hidden_size)\n",
    "        v = self.V(values).view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
    "        return context, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unReAOrjo113"
   },
   "source": [
    "## Step 5: Implement causal dot-product Attention\n",
    "\n",
    "\n",
    "Now, implement the casual scaled dot-product attention mechanism. It will be very similar to your implementation for `ScaledDotAttention`. The additional step is to mask out the attention to future timesteps so this attention mechanism can be used in a decoder. See the assignment handouts for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ovigzQffrKqj"
   },
   "outputs": [],
   "source": [
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7)\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(\n",
    "            torch.tensor(self.hidden_size, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x k)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        batch_size = keys.size(0)\n",
    "        \n",
    "        q = self.Q(queries).view(batch_size, -1, self.hidden_size)\n",
    "        k = self.K(keys).view(batch_size, -1, self.hidden_size)\n",
    "        v = self.V(values).view(batch_size, -1, self.hidden_size)\n",
    "\n",
    "        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) * self.scaling_factor\n",
    "        mask = torch.tril(torch.ones_like(unnormalized_attention) * self.neg_inf, diagonal=-1)\n",
    "        attention_weights = self.softmax(unnormalized_attention + mask)\n",
    "\n",
    "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
    "        return context, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkjHbtvT6Qxs"
   },
   "source": [
    "## Step 6: Attention encoder and decoder\n",
    "\n",
    "The following cells provide an implementation of an encoder and decoder that use a single `ScaledDotAttention` block. Please read through them to understand what they are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yKGNqUaX6RLO"
   },
   "outputs": [],
   "source": [
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, opts):\n",
    "        super(AttentionEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.self_attention = ScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "               \n",
    "        self.attention_mlp = nn.Sequential(\n",
    "                                nn.Linear(hidden_size, hidden_size),\n",
    "                                nn.ReLU(),\n",
    "                              )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder scaled dot attention.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            None: Used to conform to standard encoder return signature.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.size()\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        annotations = encoded\n",
    "        new_annotations, self_attention_weights = self.self_attention(\n",
    "            annotations, annotations, annotations\n",
    "        )  # batch_size x seq_len x hidden_size\n",
    "        residual_annotations = annotations + new_annotations\n",
    "        new_annotations = self.attention_mlp(residual_annotations)\n",
    "        annotations = residual_annotations + new_annotations\n",
    "\n",
    "        return annotations, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vDUvtOee7cMy"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.self_attention = CausalScaledDotAttention(\n",
    "                                hidden_size=hidden_size,\n",
    "                                )\n",
    "                \n",
    "        self.decoder_attention = ScaledDotAttention(\n",
    "                                  hidden_size=hidden_size,\n",
    "                                  )\n",
    "                \n",
    "        self.attention_mlp = nn.Sequential(\n",
    "                                nn.Linear(hidden_size, hidden_size),\n",
    "                                nn.ReLU(),\n",
    "                              )\n",
    "                \n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: Not used in the transformer decoder\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        new_contexts, self_attention_weights = self.self_attention(\n",
    "            contexts, contexts, contexts\n",
    "        )  # batch_size x seq_len x hidden_size\n",
    "        residual_contexts = contexts + new_contexts\n",
    "        new_contexts, encoder_attention_weights = self.decoder_attention(\n",
    "            residual_contexts, annotations, annotations\n",
    "        )  # batch_size x seq_len x hidden_size\n",
    "        residual_contexts = residual_contexts + new_contexts\n",
    "        new_contexts = self.attention_mlp(residual_contexts)\n",
    "        contexts = residual_contexts + new_contexts\n",
    "\n",
    "        encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "        self_attention_weights_list.append(self_attention_weights)\n",
    "\n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "\n",
    "        return output, (encoder_attention_weights, self_attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7gJLw5t_rnW"
   },
   "source": [
    "## Step 7: Training and analysis (single scaled dot-product attention block)\n",
    "\n",
    "Now, train the following model, with an encoder and decoder each composed a single `ScaledDotAttention` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7MOkZonC8T3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 100                                    \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: attention                              \n",
      "                           decoder_type: attention                              \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('destination', 'estinationday')\n",
      "('surveying', 'urveyingsay')\n",
      "('declaring', 'eclaringday')\n",
      "('conditioned', 'onditionedcay')\n",
      "('vindication', 'indicationvay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 3.013 | Val loss: 2.526 | Gen: itay ayayayayayayayayayay itay itay itay\n",
      "Epoch:   1 | Train loss: 2.325 | Val loss: 2.204 | Gen: eway ay incincincincincincin inay oay\n",
      "Epoch:   2 | Train loss: 2.100 | Val loss: 2.070 | Gen: eway ay oncincincincincincin inay oay\n",
      "Epoch:   3 | Train loss: 1.971 | Val loss: 1.970 | Gen: eway ay oncincincincincay isay ongway\n",
      "Epoch:   4 | Train loss: 1.870 | Val loss: 1.891 | Gen: eway ay oncincincincay isay ongway\n",
      "Epoch:   5 | Train loss: 1.789 | Val loss: 1.832 | Gen: etetatway ay oncincincincay isay ongway\n",
      "Epoch:   6 | Train loss: 1.719 | Val loss: 1.782 | Gen: etethathathathathath ay oncincincincincay isay ongway\n",
      "Epoch:   7 | Train loss: 1.657 | Val loss: 1.742 | Gen: ethathathathathathat ay ingay isisisay ongway\n",
      "Epoch:   8 | Train loss: 1.605 | Val loss: 1.710 | Gen: ethathathathathathat ay ingay isisisisisisisisisis ongway\n",
      "Epoch:   9 | Train loss: 1.560 | Val loss: 1.681 | Gen: ethathathathathathat ay ingay isisisisisisisisisis ongway\n",
      "Epoch:  10 | Train loss: 1.522 | Val loss: 1.656 | Gen: ethathathathathathhh ararrrrarrrway indiondgway isisisisisisisisisis oray\n",
      "Epoch:  11 | Train loss: 1.488 | Val loss: 1.634 | Gen: ethathhhhhhhathhhhhh ararrrrarrrrarrrrway indiondgay isisisisisisisisisis oray\n",
      "Epoch:  12 | Train loss: 1.458 | Val loss: 1.614 | Gen: ethhhhhhhhhhhhhhhhhh ararrrrarrrrarrrrarr indiondgay isisisisisisisisisis oray\n",
      "Epoch:  13 | Train loss: 1.430 | Val loss: 1.593 | Gen: ethhhhhhhhhhhhhhhhhh arirarrrarrrrarrrrar indiondgay isisisisisisisisisis oway\n",
      "Epoch:  14 | Train loss: 1.405 | Val loss: 1.577 | Gen: ethhhhhhhhhhhhhhhhhh arirarrarrrrarrrrarr indiondiotindgay isisisisisisisisisis oway\n",
      "Epoch:  15 | Train loss: 1.382 | Val loss: 1.559 | Gen: ethhhhhhhhhhhhhhhhhh arirarrarrrrarrrrarr indiondiotinday isisisisisisisisway oway\n",
      "Epoch:  16 | Train loss: 1.361 | Val loss: 1.546 | Gen: ethhhhhhhhhhhhhhhhhh arirarrarrrarrrrarrr indiotindgnday isisisisisway oway\n",
      "Epoch:  17 | Train loss: 1.342 | Val loss: 1.533 | Gen: ethhhhhhhhhhhhhhhhhh arirarirarrrarrrarrr indiondiotinday isisisisway oway\n",
      "Epoch:  18 | Train loss: 1.324 | Val loss: 1.523 | Gen: ethhhhhhhhhhhhhhhhhh arirarirarrarrrarrrw indiotinday isisisway oway\n",
      "Epoch:  19 | Train loss: 1.307 | Val loss: 1.512 | Gen: ethhhhhhhhhhhhhhhhhh arirarirarrarrrarway indiotinday isisisway oway\n",
      "Epoch:  20 | Train loss: 1.291 | Val loss: 1.505 | Gen: ethhhhhhhhhhhhhhhhhh arirararrarrway indiotinday isisisway oway\n",
      "Epoch:  21 | Train loss: 1.276 | Val loss: 1.493 | Gen: ethhhhhhhhhhhhhhhhhh arirarirarway indiotingray isisisway oway\n",
      "Epoch:  22 | Train loss: 1.261 | Val loss: 1.489 | Gen: ethhhhhhhhhhhhhhhhhh arirarirararway indiotinday isisisway oway\n",
      "Epoch:  23 | Train loss: 1.248 | Val loss: 1.478 | Gen: ethhhhhhhhhhhhhhhhhh arirarirarway indiotingray isisisway oway\n",
      "Epoch:  24 | Train loss: 1.235 | Val loss: 1.472 | Gen: ethhhhhhhhhhhhhhhthh arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  25 | Train loss: 1.222 | Val loss: 1.461 | Gen: ethhhhhhhhhhthhthhth arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  26 | Train loss: 1.210 | Val loss: 1.455 | Gen: ethhhhhhththhthhthth arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  27 | Train loss: 1.198 | Val loss: 1.446 | Gen: ethhhhththhththhthth arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  28 | Train loss: 1.187 | Val loss: 1.439 | Gen: ethhththhththththhth arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  29 | Train loss: 1.177 | Val loss: 1.430 | Gen: ethththththhthhhhhhh arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  30 | Train loss: 1.166 | Val loss: 1.422 | Gen: ethththththththththt arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  31 | Train loss: 1.156 | Val loss: 1.414 | Gen: ethththththththththa arirarirararway ontindindindinday isisisisway oway\n",
      "Epoch:  32 | Train loss: 1.146 | Val loss: 1.406 | Gen: ethththththththththt arirariraray ontindindindinday isisisisway oway\n",
      "Epoch:  33 | Train loss: 1.136 | Val loss: 1.398 | Gen: ethththththththththa arirariray ontindindindinday isisisisay oway\n",
      "Epoch:  34 | Train loss: 1.126 | Val loss: 1.390 | Gen: ethththththththththa arirariray ontindindindinday isisisay oway\n",
      "Epoch:  35 | Train loss: 1.118 | Val loss: 1.383 | Gen: eththththththththay arirariray ontindindindinday isisisay oway\n",
      "Epoch:  36 | Train loss: 1.108 | Val loss: 1.376 | Gen: eththththththththay arirariray ontindindindinday isisisay oway\n",
      "Epoch:  37 | Train loss: 1.100 | Val loss: 1.370 | Gen: eththththththay arirariray ontindindindioinday isisisay oway\n",
      "Epoch:  38 | Train loss: 1.092 | Val loss: 1.362 | Gen: ethththththththay arirariray ontindindindioinday isisisay oway\n",
      "Epoch:  39 | Train loss: 1.084 | Val loss: 1.355 | Gen: eththththththay arirariray ontindindindioinday isisisay oway\n",
      "Epoch:  40 | Train loss: 1.076 | Val loss: 1.348 | Gen: eththththththay arirariarway ontindindindioinday isisisay oway\n",
      "Epoch:  41 | Train loss: 1.068 | Val loss: 1.342 | Gen: ethththththay arirariay ontindindindioinday isisisay oway\n",
      "Epoch:  42 | Train loss: 1.060 | Val loss: 1.336 | Gen: ethththththay arirairay ontindindindioinday isisisay oway\n",
      "Epoch:  43 | Train loss: 1.054 | Val loss: 1.333 | Gen: ethththththay ariray ontindindioinday isisway oway\n",
      "Epoch:  44 | Train loss: 1.047 | Val loss: 1.327 | Gen: ethththththay arirairay ontindindiooinday isisway oway\n",
      "Epoch:  45 | Train loss: 1.040 | Val loss: 1.324 | Gen: ethththththay ariray ontindindiooinday isisway oway\n",
      "Epoch:  46 | Train loss: 1.034 | Val loss: 1.320 | Gen: ethththththay ariray ontindindiooinday isisway oway\n",
      "Epoch:  47 | Train loss: 1.028 | Val loss: 1.316 | Gen: eththththay ariray ondioiodingdingay isisway oway\n",
      "Epoch:  48 | Train loss: 1.022 | Val loss: 1.313 | Gen: eththththay ariray ondioiodingdingay isisway oway\n",
      "Epoch:  49 | Train loss: 1.017 | Val loss: 1.310 | Gen: eththththay ariray ondiotingdingay isisway oway\n",
      "Epoch:  50 | Train loss: 1.011 | Val loss: 1.307 | Gen: eththththay ariray ongiotindingay isisway oway\n",
      "Epoch:  51 | Train loss: 1.005 | Val loss: 1.304 | Gen: eththththay iraray ongiotindingay isisway oway\n",
      "Epoch:  52 | Train loss: 1.000 | Val loss: 1.302 | Gen: eththththay iraray ongiotindingay isisway oway\n",
      "Epoch:  53 | Train loss: 0.995 | Val loss: 1.300 | Gen: ethththay iraray ongiotindingay isisway oway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  54 | Train loss: 0.990 | Val loss: 1.296 | Gen: ethththay iraray ongiotindingay isisay oway\n",
      "Epoch:  55 | Train loss: 0.986 | Val loss: 1.295 | Gen: ethththay iraray ongiotindingay isisway oway\n",
      "Epoch:  56 | Train loss: 0.981 | Val loss: 1.292 | Gen: ethththay iraray ongiotingay isisay oway\n",
      "Epoch:  57 | Train loss: 0.977 | Val loss: 1.291 | Gen: ethththay iraray ongiotingay isisay oway\n",
      "Epoch:  58 | Train loss: 0.972 | Val loss: 1.288 | Gen: ethththay iraray ongiotingay isisay oway\n",
      "Epoch:  59 | Train loss: 0.969 | Val loss: 1.287 | Gen: eththay irayway ongiotingay isisay oway\n",
      "Epoch:  60 | Train loss: 0.965 | Val loss: 1.285 | Gen: ethththay iraray ongiotingay isisay oway\n",
      "Epoch:  61 | Train loss: 0.961 | Val loss: 1.283 | Gen: eththay irayway ongiotingay isisay oway\n",
      "Epoch:  62 | Train loss: 0.957 | Val loss: 1.282 | Gen: eththay iraray ongiotingay isisay oway\n",
      "Epoch:  63 | Train loss: 0.953 | Val loss: 1.279 | Gen: eththay irayway ongiotingay isisay oway\n",
      "Epoch:  64 | Train loss: 0.949 | Val loss: 1.279 | Gen: eththay iraray ongiotingay isisay oway\n",
      "Epoch:  65 | Train loss: 0.945 | Val loss: 1.277 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  66 | Train loss: 0.941 | Val loss: 1.276 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  67 | Train loss: 0.937 | Val loss: 1.274 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  68 | Train loss: 0.934 | Val loss: 1.273 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  69 | Train loss: 0.931 | Val loss: 1.271 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  70 | Train loss: 0.927 | Val loss: 1.270 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  71 | Train loss: 0.924 | Val loss: 1.268 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  72 | Train loss: 0.921 | Val loss: 1.267 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  73 | Train loss: 0.918 | Val loss: 1.265 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  74 | Train loss: 0.914 | Val loss: 1.264 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  75 | Train loss: 0.912 | Val loss: 1.263 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  76 | Train loss: 0.909 | Val loss: 1.261 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  77 | Train loss: 0.906 | Val loss: 1.260 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  78 | Train loss: 0.903 | Val loss: 1.259 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  79 | Train loss: 0.901 | Val loss: 1.257 | Gen: eththay irayway ongicingay isisay oway\n",
      "Epoch:  80 | Train loss: 0.898 | Val loss: 1.255 | Gen: eththay irayway ongicingay isway oway\n",
      "Epoch:  81 | Train loss: 0.895 | Val loss: 1.254 | Gen: ethay irayway ongicingay isisay oway\n",
      "Epoch:  82 | Train loss: 0.893 | Val loss: 1.253 | Gen: eththay irayway ongicingay isway oway\n",
      "Epoch:  83 | Train loss: 0.890 | Val loss: 1.252 | Gen: ethay irayway ongicingay isisay oway\n",
      "Epoch:  84 | Train loss: 0.888 | Val loss: 1.250 | Gen: eththay irayway ongicingay isway oway\n",
      "Epoch:  85 | Train loss: 0.885 | Val loss: 1.249 | Gen: ethay irayway ongicingay isway oway\n",
      "Epoch:  86 | Train loss: 0.882 | Val loss: 1.247 | Gen: ethay irayway ongicingay isway oway\n",
      "Epoch:  87 | Train loss: 0.880 | Val loss: 1.246 | Gen: ethay irayway ongicingay isway oway\n",
      "Epoch:  88 | Train loss: 0.878 | Val loss: 1.245 | Gen: ethay irayway ongicingay isway oway\n",
      "Epoch:  89 | Train loss: 0.875 | Val loss: 1.243 | Gen: ethay irayway ongicingay isway oway\n",
      "Epoch:  90 | Train loss: 0.873 | Val loss: 1.242 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  91 | Train loss: 0.871 | Val loss: 1.240 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  92 | Train loss: 0.869 | Val loss: 1.239 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  93 | Train loss: 0.866 | Val loss: 1.238 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  94 | Train loss: 0.864 | Val loss: 1.237 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  95 | Train loss: 0.862 | Val loss: 1.235 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  96 | Train loss: 0.860 | Val loss: 1.235 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  97 | Train loss: 0.858 | Val loss: 1.234 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  98 | Train loss: 0.856 | Val loss: 1.233 | Gen: ethay irayway ongicingay isway owingray\n",
      "Epoch:  99 | Train loss: 0.854 | Val loss: 1.231 | Gen: ethay irayway ongicingay isway owingray\n",
      "Obtained lowest validation loss of: 1.2309618010753538\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay irayway ongicingay isway owingray\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "attention_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 100,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 100,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"attention\",\n",
    "    \"decoder_type\": \"attention\",  # options: rnn / rnn_attention / attention / transformer\n",
    "}\n",
    "attention_args_s.update(args_dict)\n",
    "print_opts(attention_args_s)\n",
    "\n",
    "attention_encoder_s, attention_decoder_s, attention_losses_s = train(attention_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, attention_encoder_s, attention_decoder_s, None, attention_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tcpUFKqo2Oi"
   },
   "source": [
    "## Step 8: Transformer encoder and decoder\n",
    "\n",
    "The following cells provide an implementation of the transformer encoder and decoder that use your `ScaledDotAttention` and `CausalScaledDotAttention`. Please read through them to understand what they are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "N3B-fWsarlVk"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.self_attentions = nn.ModuleList(\n",
    "            [\n",
    "                ScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_mlps = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            None: Used to conform to standard encoder return signature.\n",
    "            None: Used to conform to standard encoder return signature.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.size()\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
    "        encoded = encoded + self.positional_encodings[:seq_len]\n",
    "\n",
    "        annotations = encoded\n",
    "        for i in range(self.num_layers):\n",
    "            new_annotations, self_attention_weights = self.self_attentions[i](\n",
    "                annotations, annotations, annotations\n",
    "            )  # batch_size x seq_len x hidden_size\n",
    "            residual_annotations = annotations + new_annotations\n",
    "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
    "            annotations = residual_annotations + new_annotations\n",
    "\n",
    "        # Transformer encoder does not have a last hidden or cell layer.\n",
    "        return annotations, None\n",
    "        # return annotations, None, None\n",
    "    def create_positional_encodings(self, max_seq_len=1000):\n",
    "        \"\"\"Creates positional encodings for the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
    "\n",
    "        Returns:\n",
    "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
    "        \"\"\"\n",
    "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
    "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
    "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
    "        trig_args = pos_indices / (10000**exponents)\n",
    "        sin_terms = torch.sin(trig_args)\n",
    "        cos_terms = torch.cos(trig_args)\n",
    "\n",
    "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
    "        pos_encodings[:, 0::2] = sin_terms\n",
    "        pos_encodings[:, 1::2] = cos_terms\n",
    "\n",
    "        if self.opts.cuda:\n",
    "            pos_encodings = pos_encodings.cuda()\n",
    "\n",
    "        return pos_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nyvTZFxtrvc6"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.self_attentions = nn.ModuleList(\n",
    "            [\n",
    "                CausalScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.encoder_attentions = nn.ModuleList(\n",
    "            [\n",
    "                ScaledDotAttention(\n",
    "                    hidden_size=hidden_size,\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.attention_mlps = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                )\n",
    "                for i in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: Not used in the transformer decoder\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        embed = embed + self.positional_encodings[:seq_len]\n",
    "\n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        for i in range(self.num_layers):\n",
    "            new_contexts, self_attention_weights = self.self_attentions[i](\n",
    "                contexts, contexts, contexts\n",
    "            )  # batch_size x seq_len x hidden_size\n",
    "            residual_contexts = contexts + new_contexts\n",
    "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](\n",
    "                residual_contexts, annotations, annotations\n",
    "            )  # batch_size x seq_len x hidden_size\n",
    "            residual_contexts = residual_contexts + new_contexts\n",
    "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
    "            contexts = residual_contexts + new_contexts\n",
    "\n",
    "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "            self_attention_weights_list.append(self_attention_weights)\n",
    "\n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "\n",
    "        return output, (encoder_attention_weights, self_attention_weights)\n",
    "\n",
    "    def create_positional_encodings(self, max_seq_len=1000):\n",
    "        \"\"\"Creates positional encodings for the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
    "\n",
    "        Returns:\n",
    "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len.\n",
    "        \"\"\"\n",
    "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
    "        dim_indices = torch.arange(self.hidden_size // 2)[None, ...]\n",
    "        exponents = (2 * dim_indices).float() / (self.hidden_size)\n",
    "        trig_args = pos_indices / (10000**exponents)\n",
    "        sin_terms = torch.sin(trig_args)\n",
    "        cos_terms = torch.cos(trig_args)\n",
    "\n",
    "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
    "        pos_encodings[:, 0::2] = sin_terms\n",
    "        pos_encodings[:, 1::2] = cos_terms\n",
    "\n",
    "        pos_encodings = pos_encodings.cuda()\n",
    "\n",
    "        return pos_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29ZjkXTNrUKb"
   },
   "source": [
    "\n",
    "## Step 9: Training and analysis (with scaled dot-product attention)\n",
    "\n",
    "Now we will train a (simplified) transformer encoder-decoder model.\n",
    "\n",
    "First, we train our smaller model on the small dataset. Use this model to answer Question 5 in the handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "mk8e4KSnuZ8N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 100                                    \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 4                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('destination', 'estinationday')\n",
      "('surveying', 'urveyingsay')\n",
      "('declaring', 'eclaringday')\n",
      "('conditioned', 'onditionedcay')\n",
      "('vindication', 'indicationvay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 3.130 | Val loss: 2.456 | Gen: ayay ayay iayeay ayay ay-ay-ay-ay\n",
      "Epoch:   1 | Train loss: 2.183 | Val loss: 2.124 | Gen: ay ay-ay-ay-ay ininingnnnnnngay ity iay-way-ay-ay-ay-ay-\n",
      "Epoch:   2 | Train loss: 1.915 | Val loss: 1.941 | Gen: ay ayay ingingingay iay iay-ay-ay-ay-ay-ay-a\n",
      "Epoch:   3 | Train loss: 1.744 | Val loss: 1.879 | Gen: ay ayway ingingingay itway ingay-way-ay\n",
      "Epoch:   4 | Train loss: 1.633 | Val loss: 1.758 | Gen: ay iay-ay-ay inoningincay iiiiway inay-way-ay-ay-ay-wa\n",
      "Epoch:   5 | Train loss: 1.524 | Val loss: 1.713 | Gen: eay ay-ay inoningay isiiiiay inray-oway\n",
      "Epoch:   6 | Train loss: 1.461 | Val loss: 1.665 | Gen: ay away inongay isway angway-oray\n",
      "Epoch:   7 | Train loss: 1.363 | Val loss: 1.626 | Gen: etay away inongintingay iway ay-ogway\n",
      "Epoch:   8 | Train loss: 1.290 | Val loss: 1.628 | Gen: etay aaiay inoitiongay issiiiiiay ay-oginay\n",
      "Epoch:   9 | Train loss: 1.249 | Val loss: 1.498 | Gen: etay aray inoingingtiongtingwa iay oringway\n",
      "Epoch:  10 | Train loss: 1.181 | Val loss: 1.535 | Gen: eteway iaray inoingicintintiongay isiiiiiiiiiiiiay onriray-ingway\n",
      "Epoch:  11 | Train loss: 1.140 | Val loss: 1.506 | Gen: eway ariay oninoinganay isay onray-oway\n",
      "Epoch:  12 | Train loss: 1.093 | Val loss: 1.547 | Gen: eay aiay oninoindingay isay onay-ingway\n",
      "Epoch:  13 | Train loss: 1.054 | Val loss: 1.737 | Gen: eateway ararway oninondingay isay onangray\n",
      "Epoch:  14 | Train loss: 1.050 | Val loss: 1.477 | Gen: eatetay ariay oninoinganotingay isway onangingway\n",
      "Epoch:  15 | Train loss: 0.956 | Val loss: 1.399 | Gen: etetay ariay onitioninongtinditit issay onay-oway-y\n",
      "Epoch:  16 | Train loss: 0.907 | Val loss: 1.386 | Gen: eateway ariay onditingtingway isway onay-ingway\n",
      "Epoch:  17 | Train loss: 0.871 | Val loss: 1.371 | Gen: etay arirway ondiningay issay onaringway\n",
      "Epoch:  18 | Train loss: 0.967 | Val loss: 1.805 | Gen: eay araywayawaaaaaaaaaaa oticiongay ayatay oray\n",
      "Epoch:  19 | Train loss: 1.104 | Val loss: 1.290 | Gen: ethay arway onitingtinontay iay onay-ingway\n",
      "Epoch:  20 | Train loss: 0.933 | Val loss: 1.233 | Gen: ethay ariray onditingtingway iay owray\n",
      "Epoch:  21 | Train loss: 0.841 | Val loss: 1.212 | Gen: ethay ariray onditingtingway isway owray\n",
      "Epoch:  22 | Train loss: 0.793 | Val loss: 1.244 | Gen: ethay ariay onditingtingway isway owray\n",
      "Epoch:  23 | Train loss: 0.768 | Val loss: 1.232 | Gen: ethay ariay onditingtingway isway ondway\n",
      "Epoch:  24 | Train loss: 0.738 | Val loss: 1.209 | Gen: ethay ariay ondititingway isway ondway\n",
      "Epoch:  25 | Train loss: 0.710 | Val loss: 1.232 | Gen: etay ariay onditioway isway owray\n",
      "Epoch:  26 | Train loss: 0.691 | Val loss: 1.234 | Gen: etay ariay ondititiongway isway owray\n",
      "Epoch:  27 | Train loss: 0.671 | Val loss: 1.226 | Gen: ethay ariay ondititingway isway ondway\n",
      "Epoch:  28 | Train loss: 0.652 | Val loss: 1.225 | Gen: etaawwaaw ariay ondititiongway isway ondgay\n",
      "Epoch:  29 | Train loss: 0.647 | Val loss: 1.216 | Gen: etay ariay ondititingway isway owray-ingway\n",
      "Epoch:  30 | Train loss: 0.629 | Val loss: 1.264 | Gen: ethay ariway onditioway isway owray-ingway\n",
      "Epoch:  31 | Train loss: 0.625 | Val loss: 1.191 | Gen: etay ariay onditiongway isway ondiray\n",
      "Epoch:  32 | Train loss: 0.587 | Val loss: 1.159 | Gen: etay ariway onditiongcay isway owrkingway\n",
      "Epoch:  33 | Train loss: 0.573 | Val loss: 1.229 | Gen: etay ariway onditiongway isway owray-ingway\n",
      "Epoch:  34 | Train loss: 0.572 | Val loss: 1.223 | Gen: ethay ariway oniodititinggtititit isway owrdway\n",
      "Epoch:  35 | Train loss: 0.594 | Val loss: 1.279 | Gen: ethay ariay ondititiongcay isway owray-oway\n",
      "Epoch:  36 | Train loss: 0.591 | Val loss: 1.210 | Gen: ethay ariway oninionditinggtitggg isway owrkinggway\n",
      "Epoch:  37 | Train loss: 0.558 | Val loss: 1.139 | Gen: ethhay ariway onditiongay isway owrkingway\n",
      "Epoch:  38 | Train loss: 0.525 | Val loss: 1.289 | Gen: ethay ariay ondititingtitgdgditg isway owrkinggway\n",
      "Epoch:  39 | Train loss: 0.539 | Val loss: 1.114 | Gen: ethay ariway onitioningway isway orkingway\n",
      "Epoch:  40 | Train loss: 0.495 | Val loss: 1.124 | Gen: etheway ariway onditiongay isway owrkingway\n",
      "Epoch:  41 | Train loss: 0.472 | Val loss: 1.093 | Gen: ethay ariway onitiongfingtay isway orkingway\n",
      "Epoch:  42 | Train loss: 0.455 | Val loss: 1.063 | Gen: ethhay ariway ondititingotay isway owrkingway\n",
      "Epoch:  43 | Train loss: 0.434 | Val loss: 1.098 | Gen: ethay ariway onditiongcay isway orkingway\n",
      "Epoch:  44 | Train loss: 0.418 | Val loss: 1.073 | Gen: ethay ariway ondiitingotingway isway orkingway\n",
      "Epoch:  45 | Train loss: 0.404 | Val loss: 1.109 | Gen: ethay ariway onditiongway isway orkingway\n",
      "Epoch:  46 | Train loss: 0.397 | Val loss: 1.046 | Gen: ethay ariway onitioningcay isway orkingway\n",
      "Epoch:  47 | Train loss: 0.389 | Val loss: 1.095 | Gen: ethhay ariway onditiongotingway isway orkingway\n",
      "Epoch:  48 | Train loss: 0.430 | Val loss: 1.267 | Gen: eteway ariray ondiditingoway isway ordrngyway\n",
      "Epoch:  49 | Train loss: 0.525 | Val loss: 1.216 | Gen: ethay ariway oniditiongway isway owray-oway\n",
      "Epoch:  50 | Train loss: 0.453 | Val loss: 1.216 | Gen: etheway ariway onditioningcingtitin isway orkinggway\n",
      "Epoch:  51 | Train loss: 0.471 | Val loss: 1.101 | Gen: ethay ariway onitionitingtitititg isway orkingway\n",
      "Epoch:  52 | Train loss: 0.415 | Val loss: 1.005 | Gen: ethay ariway onditiningoway isway orkingway\n",
      "Epoch:  53 | Train loss: 0.372 | Val loss: 1.031 | Gen: ethay ariway onditiongfingtingggd isway orkinggway\n",
      "Epoch:  54 | Train loss: 0.356 | Val loss: 0.989 | Gen: ethay ariway ondiitiningway isway orkingway\n",
      "Epoch:  55 | Train loss: 0.341 | Val loss: 1.002 | Gen: ethay ariway onditiongcingway isway orkingway\n",
      "Epoch:  56 | Train loss: 0.327 | Val loss: 0.958 | Gen: ethay ariway ondiitingotingway isway orkingway\n",
      "Epoch:  57 | Train loss: 0.310 | Val loss: 0.981 | Gen: ethay ariway ondiitinnnnnEOSEOSEOSEOSEOSnnn isway orkingway\n",
      "Epoch:  58 | Train loss: 0.303 | Val loss: 0.985 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  59 | Train loss: 0.293 | Val loss: 1.001 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  60 | Train loss: 0.286 | Val loss: 0.994 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  61 | Train loss: 0.277 | Val loss: 1.020 | Gen: ethay ariway onditiongay isway orkingway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  62 | Train loss: 0.271 | Val loss: 0.999 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  63 | Train loss: 0.263 | Val loss: 1.030 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  64 | Train loss: 0.256 | Val loss: 1.009 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  65 | Train loss: 0.248 | Val loss: 1.023 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  66 | Train loss: 0.242 | Val loss: 1.024 | Gen: ethay ariway onditiongay isway orkingway\n",
      "Epoch:  67 | Train loss: 0.235 | Val loss: 1.007 | Gen: ethay ariway onditiongcay isway orkingway\n",
      "Epoch:  68 | Train loss: 0.230 | Val loss: 1.020 | Gen: ethay ariway onditiongnay isway orkingway\n",
      "Epoch:  69 | Train loss: 0.226 | Val loss: 1.025 | Gen: ethay ariway ondiitingotay isway orkingway\n",
      "Epoch:  70 | Train loss: 0.248 | Val loss: 1.411 | Gen: ethay away ondiitingoway iwwwwwwwwwwwwwwwwwww owrdgy\n",
      "Epoch:  71 | Train loss: 0.492 | Val loss: 1.576 | Gen: eothhthchhhhhhhhhhhh arthay-away ondiiidingotingngngn ysqiystschjayhjayhja orkingway\n",
      "Epoch:  72 | Train loss: 0.588 | Val loss: 1.163 | Gen: ethewchay ariay onditiongconingngway issay orkingway\n",
      "Epoch:  73 | Train loss: 0.419 | Val loss: 1.123 | Gen: ehhhay aray onditiongay isay orkingway\n",
      "Epoch:  74 | Train loss: 0.321 | Val loss: 0.946 | Gen: ethhay ariay onditiongcay isway orkingway\n",
      "Epoch:  75 | Train loss: 0.268 | Val loss: 0.923 | Gen: ethhay ariay onditiongay isay orkingway\n",
      "Epoch:  76 | Train loss: 0.247 | Val loss: 0.922 | Gen: ethhay ariay onditiongay isay orkingway\n",
      "Epoch:  77 | Train loss: 0.235 | Val loss: 0.923 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  78 | Train loss: 0.224 | Val loss: 0.925 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  79 | Train loss: 0.216 | Val loss: 0.929 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  80 | Train loss: 0.209 | Val loss: 0.933 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  81 | Train loss: 0.202 | Val loss: 0.937 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  82 | Train loss: 0.196 | Val loss: 0.942 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  83 | Train loss: 0.191 | Val loss: 0.946 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  84 | Train loss: 0.186 | Val loss: 0.951 | Gen: ethay ariay onditiongay isay orkingway\n",
      "Epoch:  85 | Train loss: 0.182 | Val loss: 0.954 | Gen: ethay ariay onditiongay isaaw orkingway\n",
      "Epoch:  86 | Train loss: 0.177 | Val loss: 0.958 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  87 | Train loss: 0.173 | Val loss: 0.962 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  88 | Train loss: 0.169 | Val loss: 0.968 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  89 | Train loss: 0.165 | Val loss: 0.972 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  90 | Train loss: 0.161 | Val loss: 0.979 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  91 | Train loss: 0.158 | Val loss: 0.984 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  92 | Train loss: 0.154 | Val loss: 0.990 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  93 | Train loss: 0.151 | Val loss: 0.994 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  94 | Train loss: 0.147 | Val loss: 1.004 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  95 | Train loss: 0.144 | Val loss: 1.007 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  96 | Train loss: 0.141 | Val loss: 1.017 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  97 | Train loss: 0.138 | Val loss: 1.020 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  98 | Train loss: 0.134 | Val loss: 1.032 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Epoch:  99 | Train loss: 0.131 | Val loss: 1.036 | Gen: ethay ariay onditiongay isway orkingway\n",
      "Obtained lowest validation loss of: 0.9215109746267156\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay ariay onditiongay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans32_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 100,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 100,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 4,\n",
    "}\n",
    "trans32_args_s.update(args_dict)\n",
    "print_opts(trans32_args_s)\n",
    "\n",
    "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l28mKuZxvaRT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay ariay onditiongay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L8EqLYFu48H"
   },
   "source": [
    "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FdZO69DozuUu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 10                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('exotic', 'exoticway')\n",
      "('wrung', 'ungwray')\n",
      "('fiat', 'iatfay')\n",
      "('declaring', 'eclaringday')\n",
      "('development', 'evelopmentday')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.762 | Val loss: 2.350 | Gen: ettetay ay nnnnnnnnnnnnnnnnneng iday innnnnay\n",
      "Epoch:   1 | Train loss: 2.087 | Val loss: 2.122 | Gen: etay-ay ay innninnnnininininina is-i-i-iay onday-ay-ay\n",
      "Epoch:   2 | Train loss: 1.863 | Val loss: 1.967 | Gen: etay ay indntinay-inay isay ongtay-ay\n",
      "Epoch:   3 | Train loss: 1.704 | Val loss: 1.841 | Gen: etay ay ontionay-ionay isay ongtay-ay\n",
      "Epoch:   4 | Train loss: 1.585 | Val loss: 1.720 | Gen: etay ay ondontiondionday isay ongray-indway\n",
      "Epoch:   5 | Train loss: 1.501 | Val loss: 1.696 | Gen: etay-ay aray-iay ondnay-inday isay ongray-indway\n",
      "Epoch:   6 | Train loss: 1.415 | Val loss: 1.608 | Gen: etay araway ontiondionday isay oway-y-y-y\n",
      "Epoch:   7 | Train loss: 1.332 | Val loss: 1.605 | Gen: etay araway ondatinay isay ongray-ingway\n",
      "Epoch:   8 | Train loss: 1.260 | Val loss: 1.550 | Gen: etay araway ondongtionayionday isay ongray-ingway\n",
      "Epoch:   9 | Train loss: 1.223 | Val loss: 1.514 | Gen: etay ariway ondinay-inay isay owngray\n",
      "Epoch:  10 | Train loss: 1.157 | Val loss: 1.431 | Gen: etay aray onditingay isay owngray-ingway\n",
      "Epoch:  11 | Train loss: 1.090 | Val loss: 1.376 | Gen: etay ariway onditinay isay owngray\n",
      "Epoch:  12 | Train loss: 1.037 | Val loss: 1.377 | Gen: etay aray onditingnay isay owngray\n",
      "Epoch:  13 | Train loss: 1.012 | Val loss: 1.317 | Gen: etay ariway onditionay isway owngringway\n",
      "Epoch:  14 | Train loss: 0.963 | Val loss: 1.330 | Gen: etay aray ondinatinay isay owngray\n",
      "Epoch:  15 | Train loss: 0.927 | Val loss: 1.249 | Gen: ethay ariway onditinay isay owngray\n",
      "Epoch:  16 | Train loss: 0.883 | Val loss: 1.250 | Gen: ethay aray onditingngingnway isay owngringway\n",
      "Epoch:  17 | Train loss: 0.894 | Val loss: 1.320 | Gen: ethay aray ondinay-ay isay orgngray\n",
      "Epoch:  18 | Train loss: 0.875 | Val loss: 1.185 | Gen: ethay aray ondingngtiongay isay orngeway\n",
      "Epoch:  19 | Train loss: 0.809 | Val loss: 1.164 | Gen: ethay aray onditingay-ay isay owngray-y\n",
      "Epoch:  20 | Train loss: 0.769 | Val loss: 1.194 | Gen: ethay aray ondcitiongay isay orngeway\n",
      "Epoch:  21 | Train loss: 0.748 | Val loss: 1.109 | Gen: ethay aray onditingngay isay oringray\n",
      "Epoch:  22 | Train loss: 0.720 | Val loss: 1.097 | Gen: ethay aray onditingiongengayy isay owngringway\n",
      "Epoch:  23 | Train loss: 0.695 | Val loss: 1.073 | Gen: ethay aray onditingngay isay owngray\n",
      "Epoch:  24 | Train loss: 0.670 | Val loss: 1.061 | Gen: ethay aray onditingay isay owngringway\n",
      "Epoch:  25 | Train loss: 0.656 | Val loss: 1.021 | Gen: ethay ariway onditingngay isway oringray\n",
      "Epoch:  26 | Train loss: 0.628 | Val loss: 1.007 | Gen: ethay arway onditigngiongway isay owngringway\n",
      "Epoch:  27 | Train loss: 0.605 | Val loss: 0.986 | Gen: ethay ariway onditigngy isway owvongray\n",
      "Epoch:  28 | Train loss: 0.585 | Val loss: 0.995 | Gen: ethay arway onditigiongway isay owngringway\n",
      "Epoch:  29 | Train loss: 0.582 | Val loss: 0.988 | Gen: ethay ariway ondingiongingway isway owvongray\n",
      "Epoch:  30 | Train loss: 0.599 | Val loss: 1.045 | Gen: ethay arway onditiongiongengway isway owkingray\n",
      "Epoch:  31 | Train loss: 0.595 | Val loss: 0.971 | Gen: ethay ariway onditigngy isay owvorgay\n",
      "Epoch:  32 | Train loss: 0.556 | Val loss: 0.946 | Gen: ethay ariway onditigigy isay owkingray\n",
      "Epoch:  33 | Train loss: 0.530 | Val loss: 0.890 | Gen: ethay ariway onditigngiongway isway orkingray\n",
      "Epoch:  34 | Train loss: 0.501 | Val loss: 0.912 | Gen: ethay ariway onditiongiongway isway owkingray\n",
      "Epoch:  35 | Train loss: 0.480 | Val loss: 0.922 | Gen: ehthay airway onditiongiongy isway owkingray\n",
      "Epoch:  36 | Train loss: 0.471 | Val loss: 0.888 | Gen: ethay ariway onditiongiongy isway owkingray\n",
      "Epoch:  37 | Train loss: 0.453 | Val loss: 0.877 | Gen: ethay airway onditigniongway isway owvongray\n",
      "Epoch:  38 | Train loss: 0.450 | Val loss: 0.888 | Gen: ethay airway onditiongiongway isway owkingray\n",
      "Epoch:  39 | Train loss: 0.431 | Val loss: 0.846 | Gen: ehthay airway onditiongiongway isway okingray\n",
      "Epoch:  40 | Train loss: 0.420 | Val loss: 0.861 | Gen: ethay airway onditiongiongway isway owkingray\n",
      "Epoch:  41 | Train loss: 0.404 | Val loss: 0.838 | Gen: ehthay airway onditiongiongway isway okingray\n",
      "Epoch:  42 | Train loss: 0.396 | Val loss: 0.843 | Gen: ethay airway onditiongiongway isway okinghay\n",
      "Epoch:  43 | Train loss: 0.383 | Val loss: 0.828 | Gen: ehthay airway onditiongiongway isway okingray\n",
      "Epoch:  44 | Train loss: 0.379 | Val loss: 0.802 | Gen: ethay airway onditiongiongway isway okinghay\n",
      "Epoch:  45 | Train loss: 0.370 | Val loss: 0.870 | Gen: ehthay airway onditiongiongway isway oowkinghay\n",
      "Epoch:  46 | Train loss: 0.380 | Val loss: 0.806 | Gen: ethay airway onditigingy isway owkingray\n",
      "Epoch:  47 | Train loss: 0.379 | Val loss: 0.854 | Gen: ehthay airway onditiongiongway isway oowkinghay\n",
      "Epoch:  48 | Train loss: 0.360 | Val loss: 0.798 | Gen: ehthay airway onditiongiongway isway okinghay\n",
      "Epoch:  49 | Train loss: 0.336 | Val loss: 0.776 | Gen: ehthay airway onditiongngy isway okinghay\n",
      "Epoch:  50 | Train loss: 0.330 | Val loss: 0.775 | Gen: ehthay airway onditiongiongway isway okinghay\n",
      "Epoch:  51 | Train loss: 0.319 | Val loss: 0.736 | Gen: ethay airway onditiongniongway isway okinghay\n",
      "Epoch:  52 | Train loss: 0.313 | Val loss: 0.776 | Gen: ethay airway onditiongiongway isway orkingway\n",
      "Epoch:  53 | Train loss: 0.304 | Val loss: 0.723 | Gen: ehthay airway onditiongngy isway okinghay\n",
      "Epoch:  54 | Train loss: 0.293 | Val loss: 0.731 | Gen: ethay airway onditiongiongway isway orkingway\n",
      "Epoch:  55 | Train loss: 0.286 | Val loss: 0.717 | Gen: ehthay airway onditiongngy isway otonghay\n",
      "Epoch:  56 | Train loss: 0.278 | Val loss: 0.707 | Gen: ethay airway onditiongngy isway orkingway\n",
      "Epoch:  57 | Train loss: 0.275 | Val loss: 0.718 | Gen: ethay airway onditiongngy isway otonghay\n",
      "Epoch:  58 | Train loss: 0.269 | Val loss: 0.720 | Gen: ethay airway onditiongniongway isway orkngingway\n",
      "Epoch:  59 | Train loss: 0.264 | Val loss: 0.675 | Gen: ethay airway onditiongngway isway orkingway\n",
      "Epoch:  60 | Train loss: 0.253 | Val loss: 0.711 | Gen: ethay airway onditiongngway isway orkingway\n",
      "Epoch:  61 | Train loss: 0.248 | Val loss: 0.692 | Gen: ethay airway onditiongngway isway orkingway\n",
      "Epoch:  62 | Train loss: 0.239 | Val loss: 0.685 | Gen: ethay airway onditiongngway isway orkingway\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  63 | Train loss: 0.237 | Val loss: 0.690 | Gen: ethay airway onditiongngcay isway orkingway\n",
      "Epoch:  64 | Train loss: 0.228 | Val loss: 0.705 | Gen: ethay airway onditiongngway isway orkngingway\n",
      "Epoch:  65 | Train loss: 0.222 | Val loss: 0.679 | Gen: ethay airway onditiongngway isway orkingway\n",
      "Epoch:  66 | Train loss: 0.223 | Val loss: 0.720 | Gen: ethay airway onditiongngway isway orknginway\n",
      "Epoch:  67 | Train loss: 0.256 | Val loss: 0.815 | Gen: ethay airway onditiongngcay isway orkinggway\n",
      "Epoch:  68 | Train loss: 0.346 | Val loss: 1.030 | Gen: ethay irway onnditiongcway isway orknghway\n",
      "Epoch:  69 | Train loss: 0.379 | Val loss: 0.797 | Gen: ehttay airway onditiongtingcay isway owkinggray\n",
      "Epoch:  70 | Train loss: 0.281 | Val loss: 0.650 | Gen: ehthay airway onditiongcinway isway orkingway\n",
      "Epoch:  71 | Train loss: 0.227 | Val loss: 0.585 | Gen: ehtcay airway onditiongcay isway orkingway\n",
      "Epoch:  72 | Train loss: 0.202 | Val loss: 0.590 | Gen: ehtcay airway onditiongcay isway orkingway\n",
      "Epoch:  73 | Train loss: 0.192 | Val loss: 0.592 | Gen: ehtcay airway onditiongcay isway orkingway\n",
      "Epoch:  74 | Train loss: 0.184 | Val loss: 0.597 | Gen: ehtcay airway onditiongcay isway orkingway\n",
      "Epoch:  75 | Train loss: 0.179 | Val loss: 0.601 | Gen: ehtcay airway onditiongcay isway orkingway\n",
      "Epoch:  76 | Train loss: 0.173 | Val loss: 0.604 | Gen: ehtcay airway onditiongcay isway orkingway\n",
      "Epoch:  77 | Train loss: 0.169 | Val loss: 0.606 | Gen: ehtcay airway onditiongcay isway orkingway\n",
      "Epoch:  78 | Train loss: 0.164 | Val loss: 0.610 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  79 | Train loss: 0.160 | Val loss: 0.611 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  80 | Train loss: 0.156 | Val loss: 0.614 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Epoch:  81 | Train loss: 0.153 | Val loss: 0.614 | Gen: ethay airway onditiongcay isway orkingway\n",
      "Validation loss has not improved in 10 epochs, stopping early\n",
      "Obtained lowest validation loss of: 0.5847208234217932\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditiongcay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans32_args_l = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 100,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 512,\n",
    "    \"hidden_size\": 32,\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 3,\n",
    "}\n",
    "trans32_args_l.update(args_dict)\n",
    "print_opts(trans32_args_l)\n",
    "\n",
    "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SmoTgrDcr_dw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 20                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('destination', 'estinationday')\n",
      "('surveying', 'urveyingsay')\n",
      "('declaring', 'eclaringday')\n",
      "('conditioned', 'onditionedcay')\n",
      "('vindication', 'indicationvay')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.371 | Val loss: 2.010 | Gen: ay-ay-ay-ay-ay-ay-ay aaaaaaaaay ingingingngngnfnfay aaaaaay ongray-ray\n",
      "Epoch:   1 | Train loss: 1.659 | Val loss: 1.603 | Gen: epay-ay aray oingingway ay orgway-oway\n",
      "Epoch:   2 | Train loss: 1.376 | Val loss: 1.452 | Gen: ay-ay-ay aray ongingdgingdgdgdgdgd isay oray-oray\n",
      "Epoch:   3 | Train loss: 1.196 | Val loss: 1.503 | Gen: ay-ay-ay aray ondgingday isay oray-oray\n",
      "Epoch:   4 | Train loss: 1.048 | Val loss: 1.360 | Gen: eay-hay iray ondonioindindinday iswiwiwiwiwiwiwiy oringway\n",
      "Epoch:   5 | Train loss: 0.895 | Val loss: 1.165 | Gen: etay-hay-ay ariway ondoidingingindingdg iway oroway-ingray-inay\n",
      "Epoch:   6 | Train loss: 0.791 | Val loss: 1.123 | Gen: ethay-ay ariway ondinioitinday isay orkoingway\n",
      "Epoch:   7 | Train loss: 0.744 | Val loss: 1.271 | Gen: ehethay arway ondioitingway ay orkoway-iy-ingray\n",
      "Epoch:   8 | Train loss: 0.689 | Val loss: 1.095 | Gen: ethay-ay arway onditiongcay isay orkigway-ay-ay\n",
      "Epoch:   9 | Train loss: 0.571 | Val loss: 0.905 | Gen: ethtay arway onditiongcay isway orkway-ingway\n",
      "Epoch:  10 | Train loss: 0.500 | Val loss: 0.865 | Gen: ethay arway ondiditingcay isway orkigggway\n",
      "Epoch:  11 | Train loss: 0.430 | Val loss: 0.916 | Gen: ethay-away arway ondiditingcay isway orkwagingway\n",
      "Epoch:  12 | Train loss: 0.419 | Val loss: 0.928 | Gen: ethay-ay arway ondioitingway isway orkway-ingway\n",
      "Epoch:  13 | Train loss: 0.366 | Val loss: 0.730 | Gen: ethay arway onditingcay isway orkiggway\n",
      "Epoch:  14 | Train loss: 0.291 | Val loss: 0.733 | Gen: ethay arway onditionionway isway orkingway\n",
      "Epoch:  15 | Train loss: 0.247 | Val loss: 0.694 | Gen: ethay ariway onditionionway isway orkingway\n",
      "Epoch:  16 | Train loss: 0.217 | Val loss: 0.773 | Gen: ethay airway onditingcay isway orkingway\n",
      "Epoch:  17 | Train loss: 0.226 | Val loss: 0.722 | Gen: ethay ariway onditiongcay isway orkingway\n",
      "Epoch:  18 | Train loss: 0.233 | Val loss: 0.829 | Gen: ethay away onditionay isway owaingway\n",
      "Epoch:  19 | Train loss: 0.214 | Val loss: 0.642 | Gen: ethay arirway onditionioncay isway orkingway\n",
      "Epoch:  20 | Train loss: 0.159 | Val loss: 0.670 | Gen: eheay airway onditioniniongcay isay orkingway\n",
      "Epoch:  21 | Train loss: 0.149 | Val loss: 0.676 | Gen: ethay airway onditionionioncay isiway orkinggway\n",
      "Epoch:  22 | Train loss: 0.143 | Val loss: 0.653 | Gen: ethay airway onditioniongcay isway orkingway\n",
      "Epoch:  23 | Train loss: 0.135 | Val loss: 0.706 | Gen: ethay arirway onditioniningcay issway orkinggray\n",
      "Epoch:  24 | Train loss: 0.124 | Val loss: 0.651 | Gen: eththay ariway onditioningcay isway orkingway\n",
      "Epoch:  25 | Train loss: 0.100 | Val loss: 0.653 | Gen: eththay arirway onditioningcay isway orkingway\n",
      "Epoch:  26 | Train loss: 0.083 | Val loss: 0.539 | Gen: ethay arirway onditioningcay isway orkingway\n",
      "Epoch:  27 | Train loss: 0.055 | Val loss: 0.538 | Gen: ethay ariway onditioningcay isay orkingway\n",
      "Epoch:  28 | Train loss: 0.046 | Val loss: 0.563 | Gen: ethay arirway onditioningcay isway orkingway\n",
      "Epoch:  29 | Train loss: 0.038 | Val loss: 0.556 | Gen: ethay airway onditioningcay isay orkingway\n",
      "Epoch:  30 | Train loss: 0.035 | Val loss: 0.619 | Gen: ethay arirway onditioniongcay issway orkingway\n",
      "Epoch:  31 | Train loss: 0.035 | Val loss: 0.601 | Gen: ethay arirway onditioningcay isay orkingway\n",
      "Epoch:  32 | Train loss: 0.031 | Val loss: 0.598 | Gen: ethay arirway onditioningcay isway orkingway\n",
      "Epoch:  33 | Train loss: 0.040 | Val loss: 1.247 | Gen: ay ariway ondiaionininincay isay orkingway\n",
      "Epoch:  34 | Train loss: 0.271 | Val loss: 0.718 | Gen: ethay ariway onditingnay iwsway orkingway\n",
      "Epoch:  35 | Train loss: 0.262 | Val loss: 1.750 | Gen: ethckway arrway ndititioningcay sasway orkingway\n",
      "Epoch:  36 | Train loss: 0.409 | Val loss: 0.774 | Gen: eay irway onditioningcay isay orkingway\n",
      "Epoch:  37 | Train loss: 0.232 | Val loss: 0.595 | Gen: ethay airway onditioningcay issway orkingray\n",
      "Epoch:  38 | Train loss: 0.141 | Val loss: 0.559 | Gen: ethay airway onditioningcay isay orkingway\n",
      "Epoch:  39 | Train loss: 0.165 | Val loss: 0.508 | Gen: ethay airway onditioningcay isay orkingway\n",
      "Epoch:  40 | Train loss: 0.091 | Val loss: 0.549 | Gen: ethsay airway onditioningcay isway orkingway\n",
      "Epoch:  41 | Train loss: 0.084 | Val loss: 0.500 | Gen: ethay airway onditioniningcay isway orkingway\n",
      "Epoch:  42 | Train loss: 0.133 | Val loss: 0.644 | Gen: ethay airway onditioningcay issway orkingway\n",
      "Epoch:  43 | Train loss: 0.144 | Val loss: 0.531 | Gen: ethay airway onditioniniongcay isay orkingway\n",
      "Epoch:  44 | Train loss: 0.061 | Val loss: 0.490 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  45 | Train loss: 0.037 | Val loss: 0.508 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  46 | Train loss: 0.032 | Val loss: 0.492 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  47 | Train loss: 0.034 | Val loss: 0.533 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  48 | Train loss: 0.029 | Val loss: 0.489 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  49 | Train loss: 0.021 | Val loss: 0.495 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Obtained lowest validation loss of: 0.4886014160768288\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditioningcay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans64_args_s = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_small\",\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 64,\n",
    "    \"hidden_size\": 64,  # Increased model size\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 3,\n",
    "}\n",
    "trans64_args_s.update(args_dict)\n",
    "print_opts(trans64_args_s)\n",
    "\n",
    "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dardK4RWvUWV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 1                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 20                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('exotic', 'exoticway')\n",
      "('wrung', 'ungwray')\n",
      "('fiat', 'iatfay')\n",
      "('declaring', 'eclaringday')\n",
      "('development', 'evelopmentday')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Moved models to GPU!\n",
      "Epoch:   0 | Train loss: 2.402 | Val loss: 2.060 | Gen: et-w-ay way iootioiotioooootinti isdisd-w-way onay-ay\n",
      "Epoch:   1 | Train loss: 1.692 | Val loss: 1.849 | Gen: etay away-arwaway ontintintintintintin isway ongway-ingway\n",
      "Epoch:   2 | Train loss: 1.478 | Val loss: 1.728 | Gen: etetethay iway ongingintintintintin isdisay ongay-ingway\n",
      "Epoch:   3 | Train loss: 1.330 | Val loss: 1.575 | Gen: ethay arwayiway ongingingwingwingngn isay ongay-ingway\n",
      "Epoch:   4 | Train loss: 1.192 | Val loss: 1.474 | Gen: etay-ay ariway-awawawaway onontingatingway isay-iay ooringway\n",
      "Epoch:   5 | Train loss: 1.049 | Val loss: 1.419 | Gen: ehay arway-araway ontiongonay-ingating isay orway-ingngray\n",
      "Epoch:   6 | Train loss: 0.919 | Val loss: 1.348 | Gen: etay ariway ononciongincay issay orwingray\n",
      "Epoch:   7 | Train loss: 0.844 | Val loss: 1.183 | Gen: etehay aiway onoincay-incay isay okray-oingway\n",
      "Epoch:   8 | Train loss: 0.792 | Val loss: 1.210 | Gen: ehay rray ontingcay isway okrkngray\n",
      "Epoch:   9 | Train loss: 0.761 | Val loss: 1.065 | Gen: ethay arway onindiongcay-ingway isay oray-ingway\n",
      "Epoch:  10 | Train loss: 0.645 | Val loss: 1.008 | Gen: ehay aray ondiningnay-ingway isay okray-ongway\n",
      "Epoch:  11 | Train loss: 0.581 | Val loss: 0.869 | Gen: ethay arway ondiningcay-ingway isway orkningway\n",
      "Epoch:  12 | Train loss: 0.514 | Val loss: 0.852 | Gen: ethay aiway ondintiongcay isway okrkingway\n",
      "Epoch:  13 | Train loss: 0.455 | Val loss: 0.816 | Gen: ethay arway ondintingntingcay isway okrkingway\n",
      "Epoch:  14 | Train loss: 0.419 | Val loss: 0.823 | Gen: ethay arway onditiongntingway isway owrkngngway\n",
      "Epoch:  15 | Train loss: 0.394 | Val loss: 0.811 | Gen: ethay arway onditiongntingway isway owrkingway\n",
      "Epoch:  16 | Train loss: 0.370 | Val loss: 0.836 | Gen: ehethay aiway ondimintingway isay ookringway\n",
      "Epoch:  17 | Train loss: 0.346 | Val loss: 0.785 | Gen: ethay ariwway onditiongntingcay isway orkingngway\n",
      "Epoch:  18 | Train loss: 0.340 | Val loss: 0.942 | Gen: ethay ariwway onditingntingway isway orkingngway\n",
      "Epoch:  19 | Train loss: 0.297 | Val loss: 0.644 | Gen: ethay airwwway onditiongingingway isway okingray\n",
      "Epoch:  20 | Train loss: 0.242 | Val loss: 0.638 | Gen: ehthay airwway onditiongntingcay isway okringngway\n",
      "Epoch:  21 | Train loss: 0.220 | Val loss: 0.608 | Gen: ethay airwwway onditiongntigway isway okingngway\n",
      "Epoch:  22 | Train loss: 0.198 | Val loss: 0.509 | Gen: ehthay airwway onditingntay-ingway issway orkingway\n",
      "Epoch:  23 | Train loss: 0.175 | Val loss: 0.505 | Gen: ethay airwway onditiongnay-ingway isway orkingway\n",
      "Epoch:  24 | Train loss: 0.150 | Val loss: 0.494 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  25 | Train loss: 0.135 | Val loss: 0.485 | Gen: ethay airwway onditiongnigcay isway orkingway\n",
      "Epoch:  26 | Train loss: 0.115 | Val loss: 0.470 | Gen: ethay airwway onditiongingcay issway orkingway\n",
      "Epoch:  27 | Train loss: 0.116 | Val loss: 0.483 | Gen: ethay airwway onditiongay isway orkingway\n",
      "Epoch:  28 | Train loss: 0.116 | Val loss: 1.107 | Gen: hethay airway onditioningcay issway orkingway\n",
      "Epoch:  29 | Train loss: 0.208 | Val loss: 1.198 | Gen: ehtay irway onditiotingngay isway orkingway\n",
      "Epoch:  30 | Train loss: 0.355 | Val loss: 1.031 | Gen: hettay airway onditioninginingcay- isway orkingwrway\n",
      "Epoch:  31 | Train loss: 0.270 | Val loss: 0.624 | Gen: ehtay airway onditingoningcay isway orkingway\n",
      "Epoch:  32 | Train loss: 0.218 | Val loss: 0.605 | Gen: hettay airway onditingnay-ingcay isway owkingrway\n",
      "Epoch:  33 | Train loss: 0.167 | Val loss: 0.468 | Gen: ehtay airway onditiongcay isway orkingway\n",
      "Epoch:  34 | Train loss: 0.120 | Val loss: 0.491 | Gen: ehtay airway onditiongcay isway orkingway\n",
      "Epoch:  35 | Train loss: 0.121 | Val loss: 0.425 | Gen: ethay airway onditiongincay isway orkingway\n",
      "Epoch:  36 | Train loss: 0.111 | Val loss: 0.389 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  37 | Train loss: 0.069 | Val loss: 0.371 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  38 | Train loss: 0.057 | Val loss: 0.343 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  39 | Train loss: 0.052 | Val loss: 0.361 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  40 | Train loss: 0.046 | Val loss: 0.347 | Gen: ethay airway onditioningcay issway orkingway\n",
      "Epoch:  41 | Train loss: 0.046 | Val loss: 0.363 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  42 | Train loss: 0.046 | Val loss: 0.366 | Gen: ethay airway onditioningcay issway orkingway\n",
      "Epoch:  43 | Train loss: 0.041 | Val loss: 0.348 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  44 | Train loss: 0.034 | Val loss: 0.344 | Gen: ethay airway onditioningcay issway orkingway\n",
      "Epoch:  45 | Train loss: 0.030 | Val loss: 0.374 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  46 | Train loss: 0.031 | Val loss: 0.369 | Gen: ethay airway onditioningcay issway orkingway\n",
      "Epoch:  47 | Train loss: 0.034 | Val loss: 0.398 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  48 | Train loss: 0.031 | Val loss: 0.364 | Gen: ethay airway onditioningcay issway orkingway\n",
      "Epoch:  49 | Train loss: 0.023 | Val loss: 0.373 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Obtained lowest validation loss of: 0.3430892709150629\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditioningcay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = \"the air conditioning is working\"\n",
    "\n",
    "trans64_args_l = AttrDict()\n",
    "args_dict = {\n",
    "    \"data_file_name\": \"pig_latin_large\",  # Increased data set size\n",
    "    \"cuda\": True,\n",
    "    \"nepochs\": 50,\n",
    "    \"checkpoint_dir\": \"checkpoints\",\n",
    "    \"learning_rate\": 5e-4,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"batch_size\": 512,\n",
    "    \"hidden_size\": 64,  # Increased model size\n",
    "    \"encoder_type\": \"transformer\",\n",
    "    \"decoder_type\": \"transformer\",  # options: rnn / rnn_attention / transformer\n",
    "    \"num_transformer_layers\": 3,\n",
    "}\n",
    "trans64_args_l.update(args_dict)\n",
    "print_opts(trans64_args_l)\n",
    "\n",
    "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
    "\n",
    "translated = translate_sentence(\n",
    "    TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l\n",
    ")\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSSyiG39vVlN"
   },
   "source": [
    "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-Ql0pxrEvVP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_loss_comparison_by_dataset(\n",
    "    trans32_losses_s,\n",
    "    trans32_losses_l,\n",
    "    trans64_losses_s,\n",
    "    trans64_losses_l,\n",
    "    trans32_args_s,\n",
    "    trans32_args_l,\n",
    "    trans64_args_s,\n",
    "    trans64_args_l,\n",
    "    \"trans_by_dataset\",\n",
    ")\n",
    "save_loss_comparison_by_hidden(\n",
    "    trans32_losses_s,\n",
    "    trans32_losses_l,\n",
    "    trans64_losses_s,\n",
    "    trans64_losses_l,\n",
    "    trans32_args_s,\n",
    "    trans32_args_l,\n",
    "    trans64_args_s,\n",
    "    trans64_args_l,\n",
    "    \"trans_by_hidden\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "TjPTaRB4mpCd",
    "s9IS9B9-yUU5",
    "9DaTdRNuUra7",
    "4BIpGwANoQOg",
    "pbvpn4MaV0I1",
    "bRWfRdmVVjUl",
    "0yh08KhgnA30",
    "YDYMr7NclZdw",
    "dCae1mOUlZrC",
    "ecEq4TP2lZ4Z",
    "TSDTbsydlaGI",
    "RWwA6OGqlaTq",
    "AJSafHSAmu_w",
    "73_p8d5EmvOJ",
    "vYPae08Io1Fi",
    "xq7nhsEio1w-",
    "unReAOrjo113",
    "ZkjHbtvT6Qxs",
    "B7gJLw5t_rnW",
    "9tcpUFKqo2Oi",
    "29ZjkXTNrUKb"
   ],
   "name": "nmt.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
